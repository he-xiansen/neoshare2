# ç¬¬02ç« ï¼šLangChainä½¿ç”¨ä¹‹Model I/O

è®²å¸ˆï¼šå°šç¡…è°·-å®‹çº¢åº·

å®˜ç½‘ï¼š[å°šç¡…è°·][https://www.atguigu.com/]
123

***

## 1ã€Model I/Oä»‹ç»

Model I/O æ¨¡å—æ˜¯ä¸è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œäº¤äº’çš„`æ ¸å¿ƒç»„ä»¶`ï¼Œåœ¨æ•´ä¸ªæ¡†æ¶ä¸­æœ‰ç€å¾ˆé‡è¦çš„åœ°ä½ã€‚

æ‰€è°“çš„Model I/Oï¼ŒåŒ…æ‹¬è¾“å…¥æç¤º(Format)ã€è°ƒç”¨æ¨¡å‹(Predict)ã€è¾“å‡ºè§£æ(Parse)ã€‚åˆ†åˆ«å¯¹åº”ç€`Prompt Template`ï¼Œ `Model` å’Œ`Output Parser`ã€‚

> ç®€å•æ¥è¯´ï¼Œå°±æ˜¯è¾“å…¥ã€å¤„ç†ã€è¾“å‡ºè¿™ä¸‰ä¸ªæ­¥éª¤ã€‚

![image-20250530093017666](images/image-20250530093017666-1749717157186.png)

é’ˆå¯¹æ¯ä¸ªç¯èŠ‚ï¼ŒLangChainéƒ½æä¾›äº†æ¨¡æ¿å’Œå·¥å…·ï¼Œå¯ä»¥å¸®åŠ©å¿«æ·çš„è°ƒç”¨å„ç§è¯­è¨€æ¨¡å‹çš„æ¥å£ã€‚



## 2ã€Model I/Oä¹‹è°ƒç”¨æ¨¡å‹

LangChainä½œä¸ºä¸€ä¸ªâ€œå·¥å…·â€ï¼Œä¸æä¾›ä»»ä½• LLMsï¼Œè€Œæ˜¯ä¾èµ–äºç¬¬ä¸‰æ–¹é›†æˆå„ç§å¤§æ¨¡å‹ã€‚æ¯”å¦‚ï¼Œå°†OpenAIã€Anthropicã€LlaMAã€é˜¿é‡ŒQwenã€ChatGLMã€Hugging Face ç­‰å¹³å°çš„æ¨¡å‹æ— ç¼æ¥å…¥åˆ°ä½ çš„åº”ç”¨ã€‚

**èƒŒæ™¯ï¼š**

OpenAIçš„GPTç³»åˆ—æ¨¡å‹å½±å“äº†å¤§æ¨¡å‹æŠ€æœ¯å‘å±•çš„å¼€å‘èŒƒå¼å’Œæ ‡å‡†ã€‚æ‰€ä»¥æ— è®ºæ˜¯Qwenã€ChatGLMç­‰æ¨¡å‹ï¼Œå®ƒä»¬çš„ä½¿ç”¨æ–¹æ³•å’Œå‡½æ•°è°ƒç”¨é€»è¾‘åŸºæœ¬`éµå¾ªOpenAIå®šä¹‰çš„è§„èŒƒ`ï¼Œæ²¡æœ‰å¤ªå¤§å·®å¼‚ã€‚è¿™å°±ä½¿å¾—å¤§éƒ¨åˆ†çš„å¼€æºé¡¹ç›®æ‰èƒ½å¤Ÿé€šè¿‡ä¸€ä¸ªè¾ƒä¸ºé€šç”¨çš„æ¥å£æ¥æ¥å…¥å’Œä½¿ç”¨ä¸åŒçš„æ¨¡å‹ã€‚

### 2.1 æ¨¡å‹åˆ†ç±»

**LangChainæ”¯æŒçš„æ¨¡å‹æœ‰ä¸‰å¤§ç±»**

#### ç±»å‹1ï¼šLLMs(éå¯¹è¯æ¨¡å‹)

LLMsï¼Œä¹Ÿå«Text Modelã€éå¯¹è¯æ¨¡å‹ï¼Œæ˜¯è®¸å¤šè¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºçš„æ”¯æŸ±ã€‚ä¸»è¦ç‰¹ç‚¹å¦‚ä¸‹ï¼š

- **è¾“å…¥**ï¼šæ¥å—`æ–‡æœ¬å­—ç¬¦ä¸²`æˆ–`PromptValue`å¯¹è±¡
- **è¾“å‡º**ï¼šæ€»æ˜¯è¿”å›`æ–‡æœ¬å­—ç¬¦ä¸²`

![image-20250726152118382](images/image-20250726152118382.png)

- **ä¸æ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡**ã€‚æ¯æ¬¡è°ƒç”¨ç‹¬ç«‹å¤„ç†è¾“å…¥ï¼Œæ— æ³•è‡ªåŠ¨å…³è”å†å²å¯¹è¯ï¼ˆéœ€æ‰‹åŠ¨æ‹¼æ¥å†å²æ–‡æœ¬ï¼‰ã€‚
- **åº•å±‚æœºåˆ¶**ï¼šç›´æ¥è°ƒç”¨è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬è¡¥å…¨APIï¼ˆå¦‚OpenAIçš„`text-davinci-003`ï¼‰ï¼Œæ›´æ¥è¿‘åº•å±‚æ¨¡å‹çš„åŸç”Ÿæ¥å£ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šä»…éœ€å•æ¬¡æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦ç”Ÿæˆã€ç¿»è¯‘ã€ä»£ç ç”Ÿæˆã€å•æ¬¡é—®ç­”ï¼‰æˆ–å¯¹æ¥ä¸æ”¯æŒæ¶ˆæ¯ç»“æ„çš„æ—§æ¨¡å‹ï¼ˆå¦‚éƒ¨åˆ†æœ¬åœ°éƒ¨ç½²æ¨¡å‹ï¼‰ï¼ˆ`è¨€å¤–ä¹‹æ„ï¼Œä¼˜å…ˆæ¨èChatModel`ï¼‰
- **å±€é™æ€§**ï¼šæ— æ³•å¤„ç†è§’è‰²åˆ†å·¥æˆ–å¤æ‚å¯¹è¯é€»è¾‘ã€‚

ä¸¾ä¾‹ï¼š

```python
import os
import dotenv
from langchain_openai import OpenAI
dotenv.load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY1")
os.environ["OPENAI_BASE_URL"] = os.getenv("OPENAI_BASE_URL")



llm = OpenAI()
str = llm.invoke("å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—")  # ç›´æ¥è¾“å…¥å­—ç¬¦ä¸²
print(str)
```

#### ç±»å‹2ï¼šChat Models(å¯¹è¯æ¨¡å‹)

ChatModelsï¼Œä¹Ÿå«èŠå¤©æ¨¡å‹ã€å¯¹è¯æ¨¡å‹ï¼Œåº•å±‚ä½¿ç”¨LLMsã€‚

**å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨ï¼Œä»¥ ChatModel ä¸ºä¸»ï¼**

ä¸»è¦ç‰¹ç‚¹å¦‚ä¸‹ï¼š

- **è¾“å…¥**ï¼šæ¥å—`PromptValue`æˆ–æ¶ˆæ¯åˆ—è¡¨`List[BaseMessage]`ï¼Œæ¯æ¡æ¶ˆæ¯éœ€æŒ‡å®šè§’è‰²ï¼ˆå¦‚SystemMessageã€HumanMessageã€AIMessageï¼‰
- **è¾“å‡º**ï¼šæ€»æ˜¯è¿”å›å¸¦è§’è‰²çš„`æ¶ˆæ¯å¯¹è±¡`ï¼ˆ`BaseMessage`å­ç±»ï¼‰ï¼Œé€šå¸¸æ˜¯`AIMessage`

![image-20250726153227958](images/image-20250726153227958.png)

- **åŸç”Ÿæ”¯æŒå¤šè½®å¯¹è¯**ã€‚é€šè¿‡æ¶ˆæ¯åˆ—è¡¨ç»´æŠ¤ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼š`[SystemMessage, HumanMessage, AIMessage, ...]`ï¼‰ï¼Œæ¨¡å‹å¯åŸºäºå®Œæ•´å¯¹è¯å†å²ç”Ÿæˆå›å¤ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šå¯¹è¯ç³»ç»Ÿï¼ˆå¦‚å®¢æœæœºå™¨äººã€é•¿æœŸäº¤äº’çš„AIåŠ©æ‰‹ï¼‰

ä¸¾ä¾‹ï¼š

```python
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.create(
	model="gpt-3.5-turbo",
	messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„AIæ™ºèƒ½å°åŠ©æ‰‹"},
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
  	]
)

print(type(response.choices[0].message))
```

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

chat_model = ChatOpenAI()
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªè¯—äºº"),
    HumanMessage(content="å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—")
]
response = chat_model.invoke(messages)  # è¾“å…¥æ¶ˆæ¯åˆ—è¡¨

print(type(response))  # <class 'langchain_core.messages.ai.AIMessage'>
```

#### ç±»å‹3ï¼šEmbedding Model(åµŒå…¥æ¨¡å‹)

**Embedding Modelï¼š**ä¹Ÿå«æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å°†`æ–‡æœ¬`ä½œä¸ºè¾“å…¥å¹¶è¿”å›`æµ®ç‚¹æ•°åˆ—è¡¨`ï¼Œä¹Ÿå°±æ˜¯Embeddingã€‚ï¼ˆåé¢ç« èŠ‚ã€Š07-LangChainä½¿ç”¨ä¹‹Retrievalã€‹é‡ç‚¹è®²ï¼‰

### 2.2 æ¨¡å‹å‚æ•°

æ¨¡å‹è°ƒç”¨å‡½æ•°ä½¿ç”¨æ—¶éœ€åˆå§‹åŒ–æ¨¡å‹ï¼Œå¹¶è®¾ç½®å¿…è¦çš„å‚æ•°ï¼Œä¾‹å¦‚APIå¯†é’¥å’Œæ¨¡å‹åç§°ã€‚

#### 2.2.1 å‚æ•°åˆ—è¡¨

| æ­¥éª¤                            | è¯´æ˜                                                         |
| ------------------------------- | ------------------------------------------------------------ |
| `OpenAI(...) / ChatOpenAI(...)` | åˆ›å»ºä¸€ä¸ªæ¨¡å‹å¯¹è±¡                                             |
| `model / model_name`            | æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹åç§°ã€‚å¦‚ `qwen2.5-turbo`ã€`deepseek-v3`ã€`gpt-4o-mini` |
| `temperature=0.7`               | æ¸©åº¦ï¼Œæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„â€œéšæœºæ€§â€ï¼Œå–å€¼èŒƒå›´ä¸º0ï½1ï¼Œæ•°å­—è¶Šå¤§ï¼Œéšæœºæ€§è¶Šé«˜ã€‚è¾ƒé«˜çš„å€¼ï¼ˆå¦‚0.9ï¼‰ä¼šç”Ÿæˆæ›´å¤šæ ·åŒ–çš„å›ç­”ï¼Œè¾ƒä½çš„å€¼ï¼ˆå¦‚0.3ï¼‰åˆ™ç”Ÿæˆæ›´ç¡®å®šçš„å›ç­”ã€‚ |
| `max_tokens`                    | é™åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼Œé˜²æ­¢è¾“å‡ºè¿‡é•¿ã€‚tokensæ˜¯APIä¸­çš„æ–‡æœ¬å•ä½ã€‚ |
| `model.invoke(xxx)`             | æ‰§è¡Œè°ƒç”¨ï¼Œå°†ç”¨æˆ·è¾“å…¥å‘é€ç»™æ¨¡å‹                               |
| `.content`                      | æå–æ¨¡å‹è¿”å›çš„å®é™…æ–‡æœ¬å†…å®¹                                   |

> temperatureï¼šé€‚å½“é™ä½è¯¥å€¼ï¼ˆå¦‚0.3è‡³0.5ï¼‰ï¼Œæé«˜å›ç­”çš„ç¨³å®šæ€§ï¼Œå‡å°‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„éšæœºæ€§ã€‚

#### 2.2.2 é‡ç‚¹è¯´æ˜ï¼šToken

**Tokenæ˜¯ä»€ä¹ˆï¼Ÿ**

`åŸºæœ¬å•ä½`: å¤§æ¨¡å‹å¤„ç†çš„æœ€å°å•ä½æ˜¯tokenï¼ˆç›¸å½“äºè‡ªç„¶è¯­è¨€ä¸­çš„è¯æˆ–å­—ï¼‰ï¼Œè¾“å‡ºæ—¶é€ä¸ªtokenä¾æ¬¡ç”Ÿæˆã€‚

`æ”¶è´¹ä¾æ®`ï¼šå¤§è¯­è¨€æ¨¡å‹(LLM)é€šå¸¸ä¹Ÿæ˜¯ä»¥tokençš„æ•°é‡ä½œä¸ºå…¶è®¡é‡(æˆ–æ”¶è´¹)çš„ä¾æ®ã€‚

- 1ä¸ªä¸­æ–‡Tokenâ‰ˆ1-1.8ä¸ªæ±‰å­—ï¼Œ1ä¸ªè‹±æ–‡Tokenâ‰ˆ3-4å­—æ¯
- Tokenä¸å­—ç¬¦è½¬åŒ–çš„å¯è§†åŒ–å·¥å…·ï¼š
  - OpenAIæä¾›ï¼šhttps://platform.openai.com/tokenizer
  - ç™¾åº¦æ™ºèƒ½äº‘æä¾›ï¼šhttps://console.bce.baidu.com/support/#/tokenizer


**Tokençš„ç”Ÿæˆæœºåˆ¶&äº¤äº’è¡¨ç°**

- `ç”Ÿæˆæœºåˆ¶`ï¼šæ¯ä¸ªæ–°tokençš„ç”Ÿæˆéƒ½åŸºäºä¹‹å‰æ‰€æœ‰tokençš„ä¸Šä¸‹æ–‡ï¼Œå½¢æˆé“¾å¼é¢„æµ‹ç»“æ„ï¼ˆæ ¹æ®å‰ä¸‰ä¸ªé¢„æµ‹ç¬¬å››ä¸ªï¼Œå†æ ¹æ®å‰å››ä¸ªé¢„æµ‹ç¬¬äº”ä¸ªï¼‰ã€‚
- `äº¤äº’è¡¨ç°`ï¼šå¯¹è¯æ¡†ä¸­çš„æ–‡å­—æ˜¯ä¸€ä¸ªä¸ªè¿ç»­å¼¹å‡ºçš„ï¼Œç›´è§‚å±•ç¤ºtokençš„ç”Ÿæˆè¿‡ç¨‹ã€‚

ä¸¾ä¾‹ï¼š

```python
prompt = "LangChainæ˜¯ä»€ä¹ˆï¼Ÿ"

model = ChatOpenAI(
        model= "gpt-4",
        temperature=0.7,
        max_tokens=20
)

print(model.invoke(prompt).content)
```

> LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ OpenAI çš„ GPT-4ï¼‰çš„

### 2.3 OpenAIçš„ä½¿ç”¨

é€šè¿‡OpenAIçš„å¤§æ¨¡å‹çš„è°ƒç”¨ï¼Œæ˜ç¡®å‡ ä¸ªäº‹ã€‚

#### 2.3.1 æµ‹è¯•å‰çš„å‡†å¤‡å·¥ä½œ

è€ƒè™‘åˆ°OpenAIåœ¨å›½å†…è®¿é—®åŠå……å€¼çš„ä¸ä¾¿ï¼Œå¤§å®¶å¯ä»¥ä½¿ç”¨CloseAIç½‘ç«™æ³¨å†Œå’Œå……å€¼ï¼Œ`å…·ä½“è´¹ç”¨è‡ªç†`ã€‚

https://www.closeai-asia.com

#### 2.3.2 OpenAIå®˜æ–¹å’Œlangchain APIçš„è°ƒç”¨

**æ–¹å¼1ï¼šOpenAI å®˜æ–¹ SDK è°ƒç”¨æ–¹å¼ï¼ˆäº†è§£å³å¯ï¼‰**

`è°ƒç”¨éå¯¹è¯æ¨¡å‹`ï¼š

```python
from openai import OpenAI

# ä»ç¯å¢ƒå˜é‡è¯»å–APIå¯†é’¥ï¼ˆæ¨èå®‰å…¨å­˜å‚¨ï¼‰
client = OpenAI(api_key="sk-cvUm8OddQblyIsxJ...VNNAGHTm9kMH7Bf226G2",  #å¡«å†™è‡ªå·±çš„api-key
                base_url="https://api.openai-proxy.org/v1") #é€šè¿‡ä»£ç ç¤ºä¾‹è·å–

# è°ƒç”¨Completionæ¥å£
response = client.completions.create(
    model="gpt-3.5-turbo-instruct",  # éå¯¹è¯æ¨¡å‹
    prompt="è¯·å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼š\n'Artificial intelligence will reshape the future.'",
    max_tokens=100,  # ç”Ÿæˆæ–‡æœ¬æœ€å¤§é•¿åº¦
    temperature=0.7,  # æ§åˆ¶éšæœºæ€§ï¼ˆ0-2ï¼‰
)
# æå–ç»“æœ
print(response.choices[0].text.strip())
```

`è°ƒç”¨å¯¹è¯æ¨¡å‹`ï¼š

ä¸‹è¿°ä»£ç éœ€è¦å¡«å†™æ­£ç¡®çš„keyï¼Œè¿›è¡Œæµ‹è¯•

```python
from openai import OpenAI

client = OpenAI(api_key="sk-cvUm8OddQblyIsxJ25f....gIHTm9kMH7Bf226G2", #å¡«å†™è‡ªå·±çš„api-key
                base_url="https://api.openai-proxy.org/v1")

completion = client.chat.completions.create(
    model="gpt-3.5-turbo", # å¯¹è¯æ¨¡å‹
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„æ™ºèƒ½AIå°åŠ©æ‰‹"},
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ],
    max_tokens=150,
    temperature=0.5
)

print(completion.choices[0].message)
```

> ChatCompletionMessage(content='ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œå»ºè®®ã€‚æ— è®ºæ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„ç–‘é—®ï¼Œè¿˜æ˜¯å­¦ä¹ å·¥', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)

> è™½ç„¶ä¸Šé¢è®²çš„æ˜¯OpenAI å®˜æ–¹ SDK è°ƒç”¨æ–¹å¼ï¼Œä½†æ˜¯OpenAIçš„GPTç³»åˆ—æ¨¡å‹å½±å“äº†å¤§æ¨¡å‹æŠ€æœ¯å‘å±•çš„å¼€å‘èŒƒå¼å’Œæ ‡å‡†ï¼Œæ‰€ä»¥æ— è®ºæ˜¯Qwenã€ChatGLMç­‰æ¨¡å‹ï¼Œå®ƒä»¬çš„ä½¿ç”¨æ–¹æ³•å’Œå‡½æ•°è°ƒç”¨é€»è¾‘åŸºæœ¬éµå¾ªOpenAIå®šä¹‰çš„è§„èŒƒï¼Œæ²¡æœ‰å¤ªå¤§å·®å¼‚ã€‚æ‰€ä»¥ï¼Œä¸Šé¢çš„å†™æ³•ï¼Œå¦‚æœæ›¿æ¢ä¸ºå…¶ä»–æ¨¡å‹çš„base_urlã€api_keyã€model_nameä¹‹åï¼Œä¹Ÿå¯ä»¥è·å–å¤§æ¨¡å‹ã€‚

**æ–¹å¼2ï¼šlangchain APIçš„è°ƒç”¨**

è§ä¸‹

#### 2.3.2 3ç§æ–¹å¼è·å–å¤§æ¨¡å‹(ä»¥å¯¹è¯æ¨¡å‹ä¸ºä¾‹)

**æ–¹å¼1ï¼šç¡¬ç¼–ç **ï¼ˆç”Ÿäº§ç¯å¢ƒä¸æ¨èï¼‰

ç›´æ¥å°† API Key å’Œæ¨¡å‹å‚æ•°å†™å…¥ä»£ç ï¼Œ**ä»…é€‚ç”¨äºä¸´æ—¶æµ‹è¯•**ï¼Œå­˜åœ¨å¯†é’¥æ³„éœ²é£é™©ã€‚

```python
from langchain_openai import ChatOpenAI

# ç¡¬ç¼–ç  API Key å’Œæ¨¡å‹å‚æ•°
llm = ChatOpenAI(
    api_key="sk-xxxxxxxxx",  # æ˜æ–‡æš´éœ²å¯†é’¥
    base_url="https://api.openai-proxy.org/v1",
    model="gpt-3.5-turbo",
    temperature=0.7
)

# è°ƒç”¨ç¤ºä¾‹
response = llm.invoke("è§£é‡Šç¥ç»ç½‘ç»œåŸç†")
print(response.content)
```

> è¾“å‡ºï¼šç•¥

**æ–¹å¼2ï¼šé…ç½®ç¯å¢ƒå˜é‡**ï¼ˆåŸºç¡€å®‰å…¨ï¼‰

é€šè¿‡ç³»ç»Ÿç¯å¢ƒå˜é‡å­˜å‚¨å¯†é’¥ï¼Œé¿å…ä»£ç æ˜æ–‡æš´éœ²ã€‚

ç»ˆç«¯è®¾ç½®å˜é‡ï¼ˆä¸´æ—¶ç”Ÿæ•ˆï¼‰ï¼š

```bash
export OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxxxxxx"  # Linux/Mac
set OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxxxxxx"    # Windows
```

æˆ–è€…ä»PyCharmè®¾ç½®

![image-20250626111625471](images/image-20250626111625471.png)

![image-20250626111825003](images/image-20250626111825003.png)

ä»£ç ç¤ºä¾‹ï¼š

```python
import os
from langchain_openai import ChatOpenAI

# ä»ç¯å¢ƒå˜é‡è¯»å–å¯†é’¥
llm = ChatOpenAI(
    api_key=os.environ["OPENAI_API_KEY"],  # åŠ¨æ€è·å–
    base_url=os.environ["OPENAI_BASE_URL"],
    model="gpt-4o-mini",
    max_tokens=100
)

response = llm.invoke("LangChain æ˜¯ä»€ä¹ˆï¼Ÿ")
print(response.content)
```

> è¾“å‡ºï¼šç•¥

**ä¼˜ç‚¹**ï¼šå¯†é’¥ä¸ä»£ç åˆ†ç¦»ï¼Œé€‚åˆå•æœºå¼€å‘

**â€‹ç¼ºç‚¹â€‹**â€‹ï¼šé‡å¯ç»ˆç«¯åå˜é‡å¤±æ•ˆï¼Œéœ€é‡æ–°è®¾ç½®ã€‚



**æ–¹å¼3ï¼šä½¿ç”¨.envé…ç½®æ–‡ä»¶**ï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰

ä½¿ç”¨ `python-dotenv` åŠ è½½æœ¬åœ°é…ç½®æ–‡ä»¶ï¼Œæ”¯æŒå¤šç¯å¢ƒç®¡ç†ï¼ˆå¼€å‘/ç”Ÿäº§ï¼‰ã€‚

1ï¼‰å®‰è£…ä¾èµ–

```python
pip install python-dotenv
```

2ï¼‰åˆ›å»º `.env` æ–‡ä»¶ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼š

```bash
OPENAI_API_KEY="sk-xxxxxxxxx"  # éœ€å¡«å†™è‡ªå·±çš„API KEY
OPENAI_BASE_URL="https://api.openai-proxy.org/v1"
```

3ï¼‰ä»£ç ç¤ºä¾‹

æ–¹å¼1

```python
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
import os

load_dotenv()  # è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶

# print(os.getenv("OPENAI_API_KEY"))
# print(os.getenv("OPENAI_BASE_URL"))

llm = ChatOpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),  # å®‰å…¨è¯»å–
    base_url=os.getenv("OPENAI_BASE_URL"),
    model="gpt-4o-mini",
    temperature=0.7,
)

response = llm.invoke("RAG æŠ€æœ¯çš„æ ¸å¿ƒæµç¨‹")
print(response.content)
```

> è¾“å‡ºï¼šç•¥

æ–¹å¼2ï¼šç»™oså†…éƒ¨çš„ç¯å¢ƒå˜é‡èµ‹å€¼

```python
from langchain_openai import ChatOpenAI
import dotenv
dotenv.load_dotenv()

import os

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY1")
os.environ["OPENAI_API_BASE"] = os.getenv("OPENAI_BASE_URL")

text = "çŒ«ç‹æ˜¯çŒ«å—ï¼Ÿ"

chat_model = ChatOpenAI(
	model="gpt-4o-mini",
    temperature=0.7,
)
response = chat_model.invoke(text)
print(type(response))
print(response.content)
```

> <class 'langchain_core.messages.ai.AIMessage'>
> â€œçŒ«ç‹â€æ˜¯å¯¹ç¾å›½è‘—åæ­Œæ‰‹åŸƒå°”ç»´æ–¯Â·æ™®é›·æ–¯åˆ©ï¼ˆElvis Presleyï¼‰çš„æ˜µç§°ï¼Œä»–è¢«ç§°ä¸ºâ€œçŒ«ç‹â€æ˜¯å› ä¸ºä»–çš„éŸ³ä¹é£æ ¼å’Œç‹¬ç‰¹çš„é­…åŠ›ï¼Œè€Œä¸æ˜¯å› ä¸ºä»–æ˜¯ä¸€åªçŒ«ã€‚åŸƒå°”ç»´æ–¯æ˜¯æ‘‡æ»šä¹çš„ä¼ å¥‡äººç‰©ï¼Œå¯¹éŸ³ä¹å’Œæ–‡åŒ–äº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š

- é…ç½®æ–‡ä»¶å¯åŠ å…¥ .gitignore é¿å…æ³„éœ²
- æ”¯æŒå¤šç¯å¢ƒé…ç½®ï¼ˆå¦‚ `.env.prod` å’Œ `.env.dev`ï¼‰
- ç»“åˆ LangChain å¯æ‰©å±•å…¶å®ƒæ¨¡å‹ï¼ˆå¦‚ DeepSeekã€é˜¿é‡Œäº‘ï¼‰

å°ç»“ï¼š

|    **æ–¹å¼**     | **å®‰å…¨æ€§** | **æŒä¹…æ€§** |    **é€‚ç”¨åœºæ™¯**    |
| :-------------: | :--------: | :--------: | :----------------: |
|     ç¡¬ç¼–ç       |    âš ï¸ ä½    |   âŒ ä¸´æ—¶   |    æœ¬åœ°å¿«é€Ÿæµ‹è¯•    |
|    ç¯å¢ƒå˜é‡     |    âœ… ä¸­    |  âš ï¸ ä¼šè¯çº§  |    çŸ­æœŸå¼€å‘è°ƒè¯•    |
| `.env` é…ç½®æ–‡ä»¶ |   âœ…âœ… é«˜    |   âœ… æ°¸ä¹…   | ç”Ÿäº§ç¯å¢ƒã€å›¢é˜Ÿåä½œ |

> ä»¥ä¸Š3ç§æ–¹å¼ï¼Œé€‚åˆäºæ‰€æœ‰çš„LLMçš„è·å–ã€‚

### 2.4 ç™¾åº¦åƒå¸†å¹³å°

#### 2.4.1 å¼€å‘å‚è€ƒæ–‡æ¡£

https://cloud.baidu.com/doc/qianfan-docs/s/Mm8r1mejk

å…¶ä¸­ï¼Œæ–‡æœ¬ç”Ÿæˆå‚è€ƒä»£ç å¦‚ä¸‹ï¼š

```python
from openai import OpenAI

client = OpenAI(
    api_key="bce-v3/ALTAK-KZke********/f1d6ee*************",  # åƒå¸†bearer token
    base_url="https://qianfan.baidubce.com/v2",  # åƒå¸†åŸŸå
    default_headers={"appid": "app-MuYR79q6"}   # ç”¨æˆ·åœ¨åƒå¸†ä¸Šçš„appidï¼Œéå¿…ä¼ 
)

completion = client.chat.completions.create(
    model="ernie-4.0-turbo-8k", # é¢„ç½®æœåŠ¡è¯·æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨ï¼Œå®šåˆ¶æœåŠ¡è¯·å¡«å…¥APIåœ°å€
    messages=[{'role': 'system', 'content': 'You are a helpful assistant.'},
              {'role': 'user', 'content': 'Helloï¼'}]
)

print(completion.choices[0].message)
```

#### 2.4.2 è·å–API Keyå’ŒID

åˆ›å»ºAPI Keyï¼šhttps://console.bce.baidu.com/qianfan/ais/console/apiKey

åˆ›å»ºApp IDï¼šhttps://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application/v2



### 2.5 é˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°

#### 2.5.1 æ³¨å†Œä¸keyçš„è·å–

**æ³¨å†Œï¼š**

æå‰å¼€é€šç™¾ç‚¼å¹³å°è´¦å·å¹¶ç”³è¯·API KEYï¼šhttps://bailian.console.aliyun.com/#/home

![image-20250727183339144](images/image-20250727183339144.png)

**å¯¹åº”çš„é…ç½®æ–‡ä»¶ï¼š**

```bash
DASHSCOPE_API_KEY="sk-f1a87324#####e6a819a482"  #ä½¿ç”¨è‡ªå·±çš„api key
DASHSCOPE_BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
```

#### 2.5.2 æ¨¡å‹çš„è°ƒç”¨

å‚è€ƒå…·ä½“æ¨¡å‹çš„ä»£ç ç¤ºä¾‹ã€‚è¿™é‡Œä»¥DeepSeekä¸ºä¾‹ï¼š

![image-20250727183416356](images/image-20250727183416356.png)

ä¸¾ä¾‹1ï¼šé€šè¿‡OpenAI SDK

```
pip install openai
```

```python
import os
from openai import OpenAI

client = OpenAI(
    # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key="sk-xxx",
    api_key=os.getenv("DASHSCOPE_API_KEY"),  # å¦‚ä½•è·å–API Keyï¼šhttps://help.aliyun.com/zh/model-studio/developer-reference/get-api-key
    base_url=os.getenv("DASHSCOPE_BASE_URL")
)

completion = client.chat.completions.create(
    model="deepseek-r1",  # æ­¤å¤„ä»¥ deepseek-r1 ä¸ºä¾‹ï¼Œå¯æŒ‰éœ€æ›´æ¢æ¨¡å‹åç§°ã€‚
    messages=[
        {'role': 'user', 'content': '9.9å’Œ9.11è°å¤§'}
    ]
)

# é€šè¿‡reasoning_contentå­—æ®µæ‰“å°æ€è€ƒè¿‡ç¨‹
print("æ€è€ƒè¿‡ç¨‹ï¼š")
print(completion.choices[0].message.reasoning_content)

# é€šè¿‡contentå­—æ®µæ‰“å°æœ€ç»ˆç­”æ¡ˆ
print("æœ€ç»ˆç­”æ¡ˆï¼š")
print(completion.choices[0].message.content)
```

ä¸¾ä¾‹2ï¼šé€šè¿‡DashScope SDK

```
pip install dashscope
```

```python
import os
import dashscope

messages = [
    {'role': 'user', 'content': 'ä½ æ˜¯è°ï¼Ÿ'}
]

response = dashscope.Generation.call(
    # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key="sk-xxx",
    api_key=os.getenv('DASHSCOPE_API_KEY'),
    model="deepseek-r1",  # æ­¤å¤„ä»¥ deepseek-r1 ä¸ºä¾‹ï¼Œå¯æŒ‰éœ€æ›´æ¢æ¨¡å‹åç§°ã€‚
    messages=messages,
    # result_formatå‚æ•°ä¸å¯ä»¥è®¾ç½®ä¸º"text"ã€‚
    result_format='message'
)

print("=" * 20 + "æ€è€ƒè¿‡ç¨‹" + "=" * 20)
print(response.output.choices[0].message.reasoning_content)
print("=" * 20 + "æœ€ç»ˆç­”æ¡ˆ" + "=" * 20)
print(response.output.choices[0].message.content)
```



### 2.6 å…¶å®ƒå¤§æ¨¡å‹çš„ä½¿ç”¨

#### 2.6.1 æ™ºè°±çš„GLM

**æ³¨å†Œæ™ºè°±æ¨¡å‹å¹¶è·å–API Keyï¼š**

https://www.bigmodel.cn/usercenter/proj-mgmt/apikeys

![image-20250727183806745](images/image-20250727183806745.png)

![image-20250626155732485](images/image-20250626155732485.png)

```bash
#è®°å½•è‡ªå·±çš„api keyï¼Œå£°æ˜åœ¨.envæ–‡ä»¶ä¸­
ZHIPUAI_API_KEY="63a0f275b3a9###############rA4Y8daGaLydxQ"  
```

æ¥ç€é€‰æ‹©æŸ¥çœ‹ã€Šå¼€å‘æ–‡æ¡£ã€‹

![image-20250731235556948](images/image-20250731235556948.png)

 ![image-20250731235649560](images/image-20250731235649560.png)

æˆ–è€…é€‰æ‹©å¦‚ä¸‹ã€Šå‚è€ƒæ–‡æ¡£ã€‹çš†å¯ï¼š

https://www.bigmodel.cn/dev/api/normal-model/glm-4

![image-20250727185147403](images/image-20250727185147403.png)



ä¸¾ä¾‹1ï¼šä½¿ç”¨OpenAI SDK

```python
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv("ZHIPUAI_API_KEY"),
    base_url=os.getenv("ZHIPUAI_URL")
)

completion = client.chat.completions.create(
    model="glm-4-air-250414",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªèªæ˜ä¸”å¯Œæœ‰åˆ›é€ åŠ›çš„å°è¯´ä½œå®¶"},
        {"role": "user", "content": "è¯·ä½ ä½œä¸ºç«¥è¯æ•…äº‹å¤§ç‹ï¼Œå†™ä¸€ç¯‡çŸ­ç¯‡ç«¥è¯æ•…äº‹ï¼Œæ•…äº‹çš„ä¸»é¢˜æ˜¯è¦æ°¸è¿œä¿æŒä¸€é¢—å–„è‰¯çš„å¿ƒï¼Œè¦èƒ½å¤Ÿæ¿€å‘å„¿ç«¥çš„å­¦ä¹ å…´è¶£å’Œæƒ³è±¡åŠ›ï¼ŒåŒæ—¶ä¹Ÿèƒ½å¤Ÿå¸®åŠ©å„¿ç«¥æ›´å¥½åœ°ç†è§£å’Œæ¥å—æ•…äº‹ä¸­æ‰€è•´å«çš„é“ç†å’Œä»·å€¼è§‚ã€‚"}
    ],
    top_p=0.7,
    temperature=0.9
 )

print(completion.choices[0].message)
```

ä¸¾ä¾‹2ï¼šä½¿ç”¨Langchain SDK

```python
import os
from langchain_openai import ChatOpenAI
from langchain.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory

llm = ChatOpenAI(
    temperature=0.95,
    model="glm-4-air-250414",
    openai_api_key= os.getenv("ZHIPUAI_API_KEY"),
    openai_api_base=os.getenv("ZHIPUAI_URL"),
)
prompt = ChatPromptTemplate(
    messages=[
        SystemMessagePromptTemplate.from_template(
            "You are a nice chatbot having a conversation with a human."
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessagePromptTemplate.from_template("{question}")
    ]
)

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
conversation = LLMChain(
    llm=llm,
    prompt=prompt,
    verbose=True,
    memory=memory
)
conversation.invoke({"question": "ç»™æˆ‘è®²ä¸ªå†·ç¬‘è¯"})
```



ä¸¾ä¾‹3ï¼šå‚è€ƒlangchainçš„æ–‡æ¡£

https://imooc-langchain.shortvar.com/docs/integrations/chat/zhipuai/

**å®‰è£…åŒ…ï¼š**

```bash
pip install  langchain-community

pip install pyjwt
```

ä»£ç ç¤ºä¾‹ï¼š

```python
import dotenv
from langchain_community.chat_models import ChatZhipuAI
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage

#æ™ºè°±å¤§æ¨¡å‹ï¼šå‚è€ƒlangchainçš„å¤§æ¨¡å‹

dotenv.load_dotenv()

import os
os.environ["ZHIPUAI_API_KEY"] = os.getenv("ZHIPUAI_API_KEY")

chat = ChatZhipuAI(
    model="glm-4",
    temperature=0.5,
)

messages = [
    AIMessage(content="å“ˆç½—~"),
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªè¯—äºº"),
    HumanMessage(content="å†™ä¸€é¦–å…³äºAIçš„ä¸ƒè¨€ç»å¥"),
]

response = chat.invoke(messages)
print(response.content)  # æ˜¾ç¤º AI ç”Ÿæˆçš„è¯—
```

> æ™ºèƒ½åŠ©æ‰‹æ˜¾ç¥é€šï¼Œ
> ä¸‡ç‰©äº’è”æ…§çœ¼ä¸­ã€‚
> ç¼–ç ä¸–ç•Œè—è¯—æ„ï¼Œ
> å…±èæœªæ¥è·¯æ— ç©·ã€‚



#### 2.6.2 ç¡…åŸºæµåŠ¨å¹³å°

å®˜ç½‘ï¼šhttps://www.siliconflow.cn/

**ç”³è¯·API Keyï¼š**

![image-20250727192019966](images/image-20250727192019966.png)

**å‚è€ƒæ–‡æ¡£ï¼š**https://docs.siliconflow.cn/cn/userguide/quickstart

```python
from openai import OpenAI

client = OpenAI(api_key=os.getenv("SILICON_API_KEY"),
                base_url="https://api.siliconflow.cn/v1")
response = client.chat.completions.create(
    model='Pro/deepseek-ai/DeepSeek-R1',
    # model="Qwen/Qwen2.5-72B-Instruct",
    messages=[
        {'role': 'user',
        'content': "æ¨ç†æ¨¡å‹ä¼šç»™å¸‚åœºå¸¦æ¥å“ªäº›æ–°çš„æœºä¼š"}
    ],
    stream=True
)

for chunk in response:
    if not chunk.choices:
        continue
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
    if chunk.choices[0].delta.reasoning_content:
        print(chunk.choices[0].delta.reasoning_content, end="", flush=True)
```

æˆ–è€…ï¼š

![image-20250801003033228](images/image-20250801003033228.png)

![image-20250801003050941](images/image-20250801003050941.png)

```python
import requests

url = "https://api.siliconflow.cn/v1/chat/completions"

payload = {
    "model": "deepseek-ai/DeepSeek-R1", #å¡«å†™ä½ é€‰æ‹©çš„å¤§æ¨¡å‹
    "messages": [
        {
            "role": "user",
            "content": "1 +2 * 3 = ï¼Ÿ"
        }
    ]
}
headers = {
    "Authorization": "Bearer sk-auciaxqpz.....zepozralhwleyrdoyjani", #å¡«å†™ä½ çš„api-key
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```



### 2.7 å¦‚ä½•é€‰æ‹©åˆé€‚çš„å¤§æ¨¡å‹

#### 2.7.1 æœ‰æ²¡æœ‰æœ€å¥½çš„å¤§æ¨¡å‹

å‡¡æ˜¯é—®ã€Œå“ªä¸ªå¤§æ¨¡å‹æœ€å¥½ï¼Ÿã€çš„ï¼Œéƒ½æ˜¯ä¸æ‡‚çš„ã€‚

ä¸å¦¨åé—®ï¼šã€Œæ— è®ºåšä»€ä¹ˆï¼Œæœ‰éƒ½è¡¨ç°æœ€å¥½çš„å‘˜å·¥å—ï¼Ÿã€

åˆ’é‡ç‚¹ï¼š**æ²¡æœ‰æœ€å¥½çš„å¤§æ¨¡å‹ï¼Œåªæœ‰æœ€é€‚åˆçš„å¤§æ¨¡å‹**

åŸºç¡€æ¨¡å‹é€‰å‹ï¼Œåˆè§„å’Œå®‰å…¨æ˜¯é¦–è¦è€ƒé‡å› ç´ ï¼



**ä¸ºä»€ä¹ˆä¸è¦ä¾èµ–æ¦œå•?**

- æ¦œå•å·²è¢«åº”è¯•æ•™è‚²æ±¡æŸ“ï¼Œè¿˜ç®—å€¼å¾—ç›¸ä¿¡çš„æ¦œå•ï¼š[LMSYS Chatbot Arena Leaderboard][https://lmarena.ai/leaderboard]
- æ¦œå•ä½“ç°çš„æ˜¯æ•´ä½“èƒ½åŠ›ï¼Œæ”¾åˆ°ä¸€ä»¶å…·ä½“äº‹æƒ…ä¸Šï¼Œæ’åä½çš„å¯èƒ½åå€’æ›´å¥½
- æ¦œå•ä½“ç°ä¸å‡ºæˆæœ¬å·®å¼‚



**æœ¬è¯¾ç¨‹ä¸»è¦ä»¥OpenAIä¸ºä¾‹å±•å¼€åç»­çš„è¯¾ç¨‹ã€‚**å› ä¸ºï¼š

1ã€OpenAl æœ€æµè¡Œï¼Œå³ä¾¿å›½å†…ä¹Ÿæ˜¯å¦‚æ­¤

2ã€OpenAl æœ€å…ˆè¿›ã€‚åˆ«çš„æ¨¡å‹æœ‰çš„èƒ½åŠ›ï¼ŒOpenAIä¸€å®šéƒ½æœ‰ã€‚OpenAIæœ‰çš„ï¼Œåˆ«çš„æ¨¡å‹ä¸ä¸€å®šæœ‰ã€‚

3ã€å…¶å®ƒæ¨¡å‹éƒ½åœ¨è¿½èµ¶å’Œæ¨¡ä»¿OpenAl

> å­¦ä¼šOpenAlï¼Œå…¶å®ƒæ¨¡å‹è§¦ç±»æ—é€šã€‚åä¹‹ï¼Œä¸ä¸€å®š



#### 2.7.2 åç»­è·å–å¤§æ¨¡å‹çš„æ ‡å‡†æ–¹å¼

åç»­çš„å„ç§æ¨¡å‹æµ‹è¯•ï¼Œéƒ½åŸºäºå¦‚ä¸‹çš„æ¨¡å‹å±•å¼€ï¼š

éå¯¹è¯æ¨¡å‹ï¼š

```python
import os
import dotenv
from langchain_openai import OpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

llm = OpenAI(   #éå¯¹è¯æ¨¡å‹
	#max_tokens=512,
    #temperature=0.7,
)  
```

å¯¹è¯æ¨¡å‹ï¼š

```python
import os
import dotenv
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

chat_model = ChatOpenAI(   #å¯¹è¯æ¨¡å‹
    model="gpt-4o-mini",
    #max_tokens=512,
    #temperature=0.7,
)  
```

å¯¹åº”çš„é…ç½®æ–‡ä»¶ï¼š

```bash
OPENAI_API_KEY="sk-xxxxxx"   #ä»CloseAIå¹³å°ï¼Œæ³¨å†Œè‡ªå·±çš„è´¦å·ï¼Œå¹¶è·å–API KEY
OPENAI_BASE_URL="https://api.openai-proxy.org/v1"
```



### 2.8 å…³äºå¯¹è¯æ¨¡å‹çš„Message(æ¶ˆæ¯)

èŠå¤©æ¨¡å‹ï¼Œä½¿ç”¨`èŠå¤©æ¶ˆæ¯`ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›`èŠå¤©æ¶ˆæ¯`ä½œä¸ºè¾“å‡ºã€‚



**LangChainæœ‰ä¸€äº›å†…ç½®çš„æ¶ˆæ¯ç±»å‹ï¼š**

- ğŸ”¥`SystemMessage`ï¼šè®¾å®šAIè¡Œä¸ºè§„åˆ™æˆ–èƒŒæ™¯ä¿¡æ¯ã€‚æ¯”å¦‚è®¾å®šAIçš„åˆå§‹çŠ¶æ€ã€è¡Œä¸ºæ¨¡å¼æˆ–å¯¹è¯çš„æ€»ä½“ç›®æ ‡ã€‚æ¯”å¦‚â€œä½œä¸ºä¸€ä¸ªä»£ç ä¸“å®¶â€ï¼Œæˆ–è€…â€œè¿”å›jsonæ ¼å¼â€ã€‚é€šå¸¸ä½œä¸ºè¾“å…¥æ¶ˆæ¯åºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªä¼ é€’ã€‚  
- ğŸ”¥`HumanMessage`ï¼šè¡¨ç¤ºæ¥è‡ªç”¨æˆ·è¾“å…¥ã€‚æ¯”å¦‚â€œå®ç° ä¸€ä¸ªå¿«é€Ÿæ’åºæ–¹æ³•â€
- ğŸ”¥`AIMessage`ï¼šå­˜å‚¨AIå›å¤çš„å†…å®¹ã€‚è¿™å¯ä»¥æ˜¯æ–‡æœ¬ï¼Œä¹Ÿå¯ä»¥æ˜¯è°ƒç”¨å·¥å…·çš„è¯·æ±‚
- `ChatMessage`ï¼šå¯ä»¥è‡ªå®šä¹‰è§’è‰²çš„é€šç”¨æ¶ˆæ¯ç±»å‹
- `FunctionMessage/ToolMessage`ï¼šå‡½æ•°è°ƒç”¨/å·¥å…·æ¶ˆæ¯ï¼Œç”¨äºå‡½æ•°è°ƒç”¨ç»“æœçš„æ¶ˆæ¯ç±»å‹

> æ³¨æ„ï¼š
>
> FunctionMessageå’ŒToolMessageåˆ†åˆ«æ˜¯åœ¨å‡½æ•°è°ƒç”¨å’Œå·¥å…·è°ƒç”¨åœºæ™¯ä¸‹æ‰ä¼šä½¿ç”¨çš„ç‰¹æ®Šæ¶ˆæ¯ç±»å‹ï¼ŒHumanMessageã€AIMessageå’ŒSystemMessageæ‰æ˜¯æœ€å¸¸ç”¨çš„æ¶ˆæ¯ç±»å‹ã€‚
>

ä¸¾ä¾‹1ï¼š

``` python
from langchain_core.messages import HumanMessage, SystemMessage

messages = [SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„æ™ºèƒ½å°åŠ©æ‰‹"),
            HumanMessage(content="ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"),]

print(messages)
```

> [SystemMessage(content='ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„æ™ºèƒ½å°åŠ©æ‰‹', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ å¥½ï¼Œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', additional_kwargs={}, response_metadata={})]

ä¸¾ä¾‹2ï¼š

```python
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

messages = [
    SystemMessage(content=["ä½ æ˜¯ä¸€ä¸ªæ•°å­¦å®¶,åªä¼šå›ç­”æ•°å­¦é—®é¢˜","æ¯æ¬¡ä½ éƒ½èƒ½ç»™å‡ºè¯¦ç»†çš„æ–¹æ¡ˆ"]),
    HumanMessage(content="1 + 2 * 3 = ?"),
    AIMessage(content="1 + 2 * 3 çš„ç»“æœæ˜¯7"),
]

print(messages)
```

> [SystemMessage(content=['ä½ æ˜¯ä¸€ä¸ªæ•°å­¦å®¶,åªä¼šå›ç­”æ•°å­¦é—®é¢˜', 'æ¯æ¬¡ä½ éƒ½èƒ½ç»™å‡ºè¯¦ç»†çš„æ–¹æ¡ˆ'], additional_kwargs={}, response_metadata={}), HumanMessage(content='1 + 2 * 3 = ?', additional_kwargs={}, response_metadata={}), AIMessage(content='1 + 2 * 3 çš„ç»“æœæ˜¯7', additional_kwargs={}, response_metadata={})]

ä¸¾ä¾‹3ï¼š

```python
#1.å¯¼å…¥ç›¸å…³åŒ…
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# 2.ç›´æ¥åˆ›å»ºä¸åŒç±»å‹æ¶ˆæ¯
systemMessage = SystemMessage(
  content="ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ",
  additional_kwargs={"ä½ çš„åå­—": "å°è°·AI"}
)
humanMessage = HumanMessage(
  content="ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?"
)
aiMessage = AIMessage(
  content="æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰"
)
# 3.æ‰“å°æ¶ˆæ¯åˆ—è¡¨
messages = [systemMessage,humanMessage,aiMessage]
print(messages)
```

> [SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ', additional_kwargs={'ä½ çš„åå­—': 'å°è°·AI'}, response_metadata={}), HumanMessage(content='ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?', additional_kwargs={}, response_metadata={}), AIMessage(content='æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰', additional_kwargs={}, response_metadata={})]

ä¸¾ä¾‹4ï¼š

```python
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    ChatMessage
)

# åˆ›å»ºä¸åŒç±»å‹çš„æ¶ˆæ¯
system_message = SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®ç§‘å­¦å®¶")
human_message = HumanMessage(content="è§£é‡Šä¸€ä¸‹éšæœºæ£®æ—ç®—æ³•")
ai_message = AIMessage(content="éšæœºæ£®æ—æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•...")
custom_message = ChatMessage(role="analyst", content="è¡¥å……ä¸€ç‚¹å…³äºè¶…å‚æ•°è°ƒä¼˜çš„ä¿¡æ¯")

print(system_message.content)
print(human_message.content)
print(ai_message.content)
print(custom_message.content)
```

> ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®ç§‘å­¦å®¶
> è§£é‡Šä¸€ä¸‹éšæœºæ£®æ—ç®—æ³•
> éšæœºæ£®æ—æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•...
> è¡¥å……ä¸€ç‚¹å…³äºè¶…å‚æ•°è°ƒä¼˜çš„ä¿¡æ¯

ä¸¾ä¾‹5ï¼šç»“åˆå¤§æ¨¡å‹ä½¿ç”¨

```python
import os
from langchain_core.messages import SystemMessage,HumanMessage

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

chat_model = ChatOpenAI(
    model="gpt-4o-mini",
)

# ç»„æˆæ¶ˆæ¯åˆ—è¡¨
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ“…é•¿äººå·¥æ™ºèƒ½ç›¸å…³å­¦ç§‘çš„ä¸“å®¶"),
    HumanMessage(content="è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
]

response = chat_model.invoke(messages)
print(response.content)
print(type(response))  #<class 'langchain_core.messages.ai.AIMessage'>
```

> ```
> æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒæ—¨åœ¨é€šè¿‡ç»éªŒè‡ªåŠ¨æ”¹è¿›ç³»ç»Ÿçš„æ€§èƒ½ã€‚æ¢å¥è¯è¯´ï¼Œæœºå™¨å­¦ä¹ ä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œè¯†åˆ«æ¨¡å¼ï¼Œä»è€Œè¿›è¡Œé¢„æµ‹æˆ–å†³ç­–ï¼Œè€Œä¸éœ€è¦æ˜ç¡®ç¼–ç¨‹ã€‚
> 
> æœºå™¨å­¦ä¹ çš„åŸºæœ¬è¿‡ç¨‹é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š
> 
> 1. **æ•°æ®æ”¶é›†**ï¼šæ”¶é›†ç›¸å…³æ•°æ®ï¼Œè¿™äº›æ•°æ®å¯ä»¥æ˜¯ç»“æ„åŒ–çš„ï¼ˆå¦‚æ•°æ®åº“ä¸­çš„è¡¨æ ¼ï¼‰æˆ–éç»“æ„åŒ–çš„ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒç­‰ï¼‰ã€‚
> 
> 2. **æ•°æ®é¢„å¤„ç†**ï¼šå¯¹æ”¶é›†åˆ°çš„æ•°æ®è¿›è¡Œæ¸…æ´—å’Œå¤„ç†ï¼Œä»¥å»é™¤å™ªå£°å’Œä¸ç›¸å…³çš„ä¿¡æ¯ï¼Œå¡«è¡¥ç¼ºå¤±å€¼ï¼Œå¹¶å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–æˆ–å½’ä¸€åŒ–ã€‚
> 
> 3. **ç‰¹å¾é€‰æ‹©ä¸æå–**ï¼šé€‰æ‹©å¯¹æ¨¡å‹æœ‰é¢„æµ‹èƒ½åŠ›çš„ç‰¹å¾ï¼Œæˆ–è€…åˆ›é€ æ–°çš„ç‰¹å¾ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¡¨ç¤ºé—®é¢˜ã€‚
> 
> 4. **æ¨¡å‹é€‰æ‹©**ï¼šé€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ ç®—æ³•ä¸æ¨¡å‹ï¼Œè¿™äº›ç®—æ³•å¯ä»¥åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ç­‰ç±»å‹ã€‚
> 
> 5. **è®­ç»ƒæ¨¡å‹**ï¼šä½¿ç”¨è®­ç»ƒæ•°æ®æ¥è°ƒæ•´æ¨¡å‹çš„å‚æ•°ï¼Œä»¥ä¾¿å…¶èƒ½å¤Ÿæ ¹æ®è¾“å…¥æ•°æ®åšå‡ºå‡†ç¡®çš„é¢„æµ‹æˆ–åˆ†ç±»ã€‚
> 
> 6. **è¯„ä¼°æ¨¡å‹**ï¼šä½¿ç”¨æµ‹è¯•æ•°æ®æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œå¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°ç­‰ã€‚
> 
> 7. **ä¼˜åŒ–ä¸è¿­ä»£**ï¼šæ ¹æ®è¯„ä¼°ç»“æœå¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œå¯èƒ½éœ€è¦è¿”å›å‰é¢çš„æ­¥éª¤è¿›è¡Œè°ƒæ•´ï¼Œç›´åˆ°æ»¡æ„ä¸ºæ­¢ã€‚
> 
> 8. **éƒ¨ç½²ä¸ç›‘æ§**ï¼šå°†è®­ç»ƒå¥½çš„æ¨¡å‹æŠ•å…¥å®é™…åº”ç”¨ä¸­ï¼Œå¹¶æŒç»­ç›‘æ§å…¶è¡¨ç°ï¼Œä»¥ä¾¿åœ¨é‡åˆ°æ–°æ•°æ®æ—¶è¿›è¡Œè°ƒä¼˜ã€‚
> 
> æœºå™¨å­¦ä¹ å·²ç»å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿã€é‡‘èé¢„æµ‹ã€åŒ»ç–—è¯Šæ–­ç­‰ï¼Œå…¶ç›®æ ‡æ˜¯é€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹å¼æ¥æé«˜å†³ç­–çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚
> <class 'langchain_core.messages.ai.AIMessage'>
> ```



### 2.9 è°ƒç”¨æ–¹æ³•

ä¸ºäº†å°½å¯èƒ½ç®€åŒ–è‡ªå®šä¹‰é“¾çš„åˆ›å»ºï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ª["Runnable"](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)åè®®ã€‚è®¸å¤šLangChainç»„ä»¶å®ç°äº† Runnable åè®®ï¼ŒåŒ…æ‹¬èŠå¤©æ¨¡å‹ã€æç¤ºè¯æ¨¡æ¿ã€è¾“å‡ºè§£æå™¨ã€æ£€ç´¢å™¨ã€ä»£ç†(æ™ºèƒ½ä½“)ç­‰ã€‚

æ¯ä¸ª LCEL å¯¹è±¡éƒ½å®ç°äº† Runnable æ¥å£ï¼Œè¯¥æ¥å£å®šä¹‰äº†ä¸€ç»„å…¬å…±çš„è°ƒç”¨æ–¹æ³•ã€‚è¿™ä½¿å¾— LCEL å¯¹è±¡é“¾ä¹Ÿè‡ªåŠ¨æ”¯æŒè¿™äº›è°ƒç”¨æˆä¸ºå¯èƒ½ã€‚



**Runnable æ¥å£å®šä¹‰çš„å…¬å…±çš„è°ƒç”¨æ–¹æ³•å¦‚ä¸‹ï¼š**

- `invoke`: å¤„ç†å•æ¡è¾“å…¥ï¼Œç­‰å¾…LLMå®Œå…¨æ¨ç†å®Œæˆåå†è¿”å›è°ƒç”¨ç»“æœ
- `stream`: æµå¼å“åº”ï¼Œé€å­—è¾“å‡ºLLMçš„å“åº”ç»“æœ
- `batch`: å¤„ç†æ‰¹é‡è¾“å…¥

è¿™äº›ä¹Ÿæœ‰ç›¸åº”çš„å¼‚æ­¥æ–¹æ³•ï¼Œåº”è¯¥ä¸ asyncio çš„`await`è¯­æ³•ä¸€èµ·ä½¿ç”¨ä»¥å®ç°å¹¶å‘ï¼š

- `astream`: å¼‚æ­¥æµå¼å“åº”
- `ainvoke`: å¼‚æ­¥å¤„ç†å•æ¡è¾“å…¥
- `abatch`: å¼‚æ­¥å¤„ç†æ‰¹é‡è¾“å…¥
- `astream_log`: å¼‚æ­¥æµå¼è¿”å›ä¸­é—´æ­¥éª¤ï¼Œä»¥åŠæœ€ç»ˆå“åº”
- `astream_events`: ï¼ˆæµ‹è¯•ç‰ˆï¼‰å¼‚æ­¥æµå¼è¿”å›é“¾ä¸­å‘ç”Ÿçš„äº‹ä»¶ï¼ˆåœ¨ langchain-core 0.1.14 ä¸­å¼•å…¥ï¼‰



#### 2.9.1 æµå¼è¾“å‡ºä¸éæµå¼è¾“å‡º

åœ¨Langchainä¸­ï¼Œè¯­è¨€æ¨¡å‹çš„è¾“å‡ºåˆ†ä¸ºäº†ä¸¤ç§ä¸»è¦çš„æ¨¡å¼ï¼š**æµå¼è¾“å‡º**ä¸**éæµå¼è¾“å‡º**ã€‚

ä¸‹é¢æ˜¯ä¸¤ä¸ªåœºæ™¯ï¼š

- éæµå¼è¾“å‡ºï¼šç”¨æˆ·æé—®ï¼Œè¯·ç¼–å†™ä¸€é¦–è¯—ï¼Œç³»ç»Ÿåœ¨é™é»˜æ•°ç§’å`çªç„¶å¼¹å‡º`äº†å®Œæ•´çš„è¯—æ­Œã€‚
  - å¦‚åŒä¸€ç§â€œæäº¤è¯·æ±‚ï¼Œç­‰å¾…ç»“æœâ€çš„æµç¨‹ï¼Œå®ç°ç®€å•ï¼Œä½†ä½“éªŒå•è°ƒã€‚
- æµå¼è¾“å‡ºï¼šç”¨æˆ·æé—®ï¼Œè¯·ç¼–å†™ä¸€é¦–è¯—ï¼Œå½“é—®é¢˜åˆšåˆšå‘é€ï¼Œç³»ç»Ÿå°±å¼€å§‹`ä¸€å­—ä¸€å¥`ï¼ˆé€ä¸ªtokenï¼‰è¿›è¡Œå›å¤ï¼Œæ„Ÿè§‰æ˜¯ä¸€è¾¹æ€è€ƒä¸€è¾¹è¾“å‡ºã€‚
  - æ›´åƒæ˜¯â€œå®æ—¶å¯¹è¯â€ï¼Œæ›´ä¸ºè´´è¿‘äººç±»äº¤äº’çš„ä¹ æƒ¯ï¼Œæ›´æœ‰å¸å¼•åŠ›ã€‚

**éæµå¼è¾“å‡ºï¼š**

è¿™æ˜¯Langchainä¸LLMäº¤äº’æ—¶çš„é»˜è®¤è¡Œä¸ºï¼Œæ˜¯æœ€ç®€å•ã€æœ€ç¨³å®šçš„è¯­è¨€æ¨¡å‹è°ƒç”¨æ–¹å¼ã€‚å½“ç”¨æˆ·å‘å‡ºè¯·æ±‚åï¼Œç³»ç»Ÿåœ¨åå°ç­‰å¾…æ¨¡å‹`ç”Ÿæˆå®Œæ•´å“åº”`ï¼Œç„¶å`ä¸€æ¬¡æ€§å°†å…¨éƒ¨ç»“æœè¿”å›`ã€‚åœ¨å¤§å¤šæ•°é—®ç­”ã€æ‘˜è¦ã€ä¿¡æ¯æŠ½å–ç±»ä»»åŠ¡ä¸­ï¼Œéæµå¼è¾“å‡ºæä¾›äº†ç»“æ„æ¸…æ™°ã€é€»è¾‘å®Œæ•´çš„ç»“æœï¼Œé€‚åˆå¿«é€Ÿé›†æˆå’Œéƒ¨ç½²ã€‚

ä¸¾ä¾‹1ï¼š

```python
import os
import dotenv
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

#åˆå§‹åŒ–å¤§æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini")

# åˆ›å»ºæ¶ˆæ¯
messages = [HumanMessage(content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")]

# éæµå¼è°ƒç”¨LLMè·å–å“åº”
response = chat_model.invoke(messages)

# æ‰“å°å“åº”å†…å®¹
print(response)
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼Œæ˜¯ç›´æ¥å…¨éƒ¨è¾“å‡ºçš„ã€‚

> content='ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨è®¾è®¡æ¥æä¾›ä¿¡æ¯å’Œè§£ç­”é—®é¢˜ã€‚æˆ‘å¯ä»¥å¸®åŠ©ä½ è§£ç­”å„ç§é—®é¢˜ï¼Œæ¯”å¦‚å­¦ä¹ ã€ç§‘æŠ€ã€æ–‡åŒ–ã€ç”Ÿæ´»ç­‰æ–¹é¢çš„å†…å®¹ã€‚å¦‚æœä½ æœ‰ä»»ä½•å…·ä½“çš„é—®é¢˜æˆ–è€…éœ€è¦äº†è§£çš„ä¸»é¢˜ï¼Œæ¬¢è¿éšæ—¶é—®æˆ‘ï¼' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 12, 'total_tokens': 69, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-BmdJTYMLA9iiUFDIAJLJREFdJN5Us', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--2b25b74a-12b0-4162-80fc-7d348b3ed3fb-0' usage_metadata={'input_tokens': 12, 'output_tokens': 57, 'total_tokens': 69, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}

ä¸¾ä¾‹2ï¼š


```python
import os
import dotenv
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

# åˆå§‹åŒ–å¤§æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini")

# æ”¯æŒå¤šä¸ªæ¶ˆæ¯ä½œä¸ºè¾“å…¥
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚ä½ å«äºè€å¸ˆ"),
    HumanMessage(content="ä½ æ˜¯è°ï¼Ÿ")
]
response = chat_model.invoke(messages)
print(response.content)
```

> æˆ‘å«äºè€å¸ˆï¼Œæ˜¯ä¸€ä½ä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚åœ¨è¿™é‡Œæˆ‘å¯ä»¥å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯æˆ–æ˜¯è¿›è¡Œäº¤æµã€‚æœ‰éœ€è¦å¸®åŠ©çš„åœ°æ–¹å—ï¼Ÿ

ä¸¾ä¾‹3ï¼š

```python
import os
import dotenv
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

# åˆå§‹åŒ–å¤§æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini")

# æ”¯æŒå¤šä¸ªæ¶ˆæ¯ä½œä¸ºè¾“å…¥
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚ä½ å«äºè€å¸ˆ"),
    HumanMessage(content="ä½ æ˜¯è°ï¼Ÿ")
]
response = chat_model(messages)   #ç‰¹åˆ«çš„å†™æ³•
print(response.content)
```

ç¬¬19è¡Œï¼Œåº•å±‚è°ƒç”¨`BaseChatModel.__call__`ï¼Œå†…éƒ¨è°ƒç”¨çš„è¿˜æ˜¯invoke()ã€‚åç»­è¿˜ä¼šæœ‰è¿™ç§å†™æ³•å‡ºç°ï¼Œäº†è§£å³å¯ã€‚

**æµå¼è¾“å‡º**

ä¸€ç§æ›´å…·äº¤äº’æ„Ÿçš„æ¨¡å‹è¾“å‡ºæ–¹å¼ï¼Œç”¨æˆ·ä¸å†éœ€è¦ç­‰å¾…å®Œæ•´ç­”æ¡ˆï¼Œè€Œæ˜¯èƒ½çœ‹åˆ°æ¨¡å‹**é€ä¸ª token** åœ°å®æ—¶è¿”å›å†…å®¹ã€‚é€‚åˆæ„å»ºå¼ºè°ƒâ€œå®æ—¶åé¦ˆâ€çš„åº”ç”¨ï¼Œå¦‚èŠå¤©æœºå™¨äººã€å†™ä½œåŠ©æ‰‹ç­‰ã€‚

Langchain ä¸­é€šè¿‡è®¾ç½® `stream=True` å¹¶é…åˆ **å›è°ƒæœºåˆ¶** æ¥å¯ç”¨æµå¼è¾“å‡ºã€‚

ä¸¾ä¾‹ï¼š

```python
import os
import dotenv
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

# åˆå§‹åŒ–å¤§æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini",
                        streaming=True  # å¯ç”¨æµå¼è¾“å‡º
                        )

# åˆ›å»ºæ¶ˆæ¯
messages = [HumanMessage(content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")]

# æµå¼è°ƒç”¨LLMè·å–å“åº”
print("å¼€å§‹æµå¼è¾“å‡ºï¼š")
for chunk in chat_model.stream(messages):
    # é€ä¸ªæ‰“å°å†…å®¹å—
    print(chunk.content, end="", flush=True) # åˆ·æ–°ç¼“å†²åŒº (æ— æ¢è¡Œç¬¦ï¼Œç¼“å†²åŒºæœªåˆ·æ–°ï¼Œå†…å®¹å¯èƒ½ä¸ä¼šç«‹å³æ˜¾ç¤º)

print("\næµå¼è¾“å‡ºç»“æŸ")
```

è¾“å‡ºç»“æœå¦‚ä¸‹ï¼ˆä¸€æ®µæ®µæ–‡å­—é€ä¸ªè¾“å‡ºï¼‰

```
å¼€å§‹æµå¼è¾“å‡ºï¼š
ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œè§£å†³é—®é¢˜ã€‚æˆ‘å¯ä»¥å¤„ç†å„ç§ä¸»é¢˜ï¼ŒåŒ…æ‹¬ç§‘æŠ€ã€å†å²ã€æ–‡åŒ–ã€è¯­è¨€å­¦ä¹ ç­‰ã€‚æ— è®ºä½ æœ‰ä»€ä¹ˆé—®é¢˜ï¼Œå°½ç®¡é—®æˆ‘ï¼Œæˆ‘ä¼šå°½åŠ›æä¾›å‡†ç¡®å’Œæœ‰ç”¨çš„å›ç­”ï¼
æµå¼è¾“å‡ºç»“æŸ
```

#### 2.9.2 æ‰¹é‡è°ƒç”¨

ä¸¾ä¾‹ï¼š

``` python
import os
import dotenv
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

# åˆå§‹åŒ–å¤§æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini")

messages1 = [SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„æ™ºèƒ½å°åŠ©æ‰‹"),
             HumanMessage(content="è¯·å¸®æˆ‘ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "), ]

messages2 = [SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„æ™ºèƒ½å°åŠ©æ‰‹"),
             HumanMessage(content="è¯·å¸®æˆ‘ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯AIGC"), ]

messages3 = [SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„æ™ºèƒ½å°åŠ©æ‰‹"),
             HumanMessage(content="è¯·å¸®æˆ‘ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯å¤§æ¨¡å‹æŠ€æœ¯"), ]

messages = [messages1, messages2, messages3]

# è°ƒç”¨batch
response = chat_model.batch(messages)

print(response)

```

> [AIMessage(content='æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„ä¸€ç§åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿé€šè¿‡ç»éªŒè‡ªåŠ¨æ”¹è¿›å…¶æ€§èƒ½ï¼Œè€Œä¸éœ€è¦æ˜ç¡®çš„ç¼–ç¨‹æŒ‡ä»¤ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæœºå™¨å­¦ä¹ å…³æ³¨çš„æ˜¯è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šåšå‡ºå†³ç­–æˆ–é¢„æµ‹ã€‚\n\næœºå™¨å­¦ä¹ çš„åŸºæœ¬æµç¨‹é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š\n\n1. **æ•°æ®æ”¶é›†**ï¼šæ”¶é›†å’Œå‡†å¤‡æ•°æ®ï¼Œè¿™æ˜¯æœºå™¨å­¦ä¹ çš„åŸºç¡€ã€‚æ•°æ®å¯ä»¥æ˜¯ç»“æ„åŒ–çš„ï¼ˆå¦‚è¡¨æ ¼ï¼‰æˆ–éç»“æ„åŒ–çš„ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒï¼‰ã€‚\n\n2. **æ•°æ®é¢„å¤„ç†**ï¼šå¯¹æ•°æ®è¿›è¡Œæ¸…æ´—ã€æ•´ç†å’Œè½¬æ¢ï¼Œä»¥é€‚åˆåç»­çš„åˆ†æå’Œå»ºæ¨¡ã€‚è¿™å¯èƒ½åŒ…æ‹¬å»é™¤å™ªå£°ã€å¡«è¡¥ç¼ºå¤±å€¼å’Œæ ‡å‡†åŒ–æ•°æ®ç­‰ã€‚\n\n3. **é€‰æ‹©æ¨¡å‹**ï¼šæ ¹æ®é—®é¢˜çš„æ€§è´¨é€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ ç®—æ³•æˆ–æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œçº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ã€‚\n\n4. **è®­ç»ƒæ¨¡å‹**ï¼šä½¿ç”¨å‡†å¤‡å¥½çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚æ¨¡å‹é€šè¿‡è°ƒæ•´å†…éƒ¨å‚æ•°æ¥æœ€å°åŒ–é¢„æµ‹ä¸å®é™…ç»“æœä¹‹é—´çš„å·®è·ã€‚\n\n5. **è¯„ä¼°æ¨¡å‹**ï¼šä½¿ç”¨æµ‹è¯•æ•°æ®é›†æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œé€šå¸¸é€šè¿‡å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼ç­‰æŒ‡æ ‡ã€‚\n\n6. **æ¨¡å‹ä¼˜åŒ–**ï¼šæ ¹æ®è¯„ä¼°ç»“æœè°ƒæ•´æ¨¡å‹å‚æ•°æˆ–é€‰æ‹©ä¸åŒçš„ç®—æ³•ï¼Œä»¥æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚\n\n7. **éƒ¨ç½²åº”ç”¨**ï¼šå°†è®­ç»ƒå¥½çš„æ¨¡å‹åº”ç”¨äºå®é™…é—®é¢˜ï¼Œè¿›è¡Œå®æ—¶æˆ–æ‰¹é‡é¢„æµ‹ã€‚\n\næœºå™¨å­¦ä¹ é€šå¸¸è¢«åˆ†ä¸ºä¸‰å¤§ç±»ï¼š\n\n1. **ç›‘ç£å­¦ä¹ **ï¼šæ¨¡å‹åœ¨æ‹¥æœ‰æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹å­¦ä¹ ï¼Œç›®æ ‡æ˜¯é¢„æµ‹æ–°çš„è¾“å…¥æ•°æ®çš„ç»“æœã€‚å¸¸ç”¨ä¾‹å­åŒ…æ‹¬åˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚\n\n2. **æ— ç›‘ç£å­¦ä¹ **ï¼šæ¨¡å‹åœ¨æ²¡æœ‰æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹å­¦ä¹ ï¼Œç›®æ ‡æ˜¯å‘ç°æ•°æ®çš„ç»“æ„å’Œæ¨¡å¼ã€‚å¸¸ç”¨ä¾‹å­åŒ…æ‹¬èšç±»å’Œé™ç»´ã€‚\n\n3. **å¼ºåŒ–å­¦ä¹ **ï¼šæ¨¡å‹é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ ï¼Œç›®æ ‡æ˜¯é€šè¿‡è¯•é”™è¿‡ç¨‹æ¥ä¼˜åŒ–å†³ç­–ï¼Œæœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚\n\næœºå™¨å­¦ä¹ çš„åº”ç”¨éå¸¸å¹¿æ³›ï¼ŒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿã€é‡‘èé¢„æµ‹ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸã€‚éšç€æ•°æ®çš„å¢åŠ å’Œè®¡ç®—èƒ½åŠ›çš„æå‡ï¼Œæœºå™¨å­¦ä¹ æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œè¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨äºå®é™…é—®é¢˜ä¸­ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 490, 'prompt_tokens': 30, 'total_tokens': 520, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_57db37749c', 'id': 'chatcmpl-Bms2AQa5fLnKKMWybQvs0oLm3mc7C', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--6ab9c603-e5bc-4cc7-bba4-706bdc6f28d9-0', usage_metadata={'input_tokens': 30, 'output_tokens': 490, 'total_tokens': 520, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='AIGCï¼ˆäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ŒArtificial Intelligence Generated Contentï¼‰æ˜¯æŒ‡åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯è‡ªåŠ¨ç”Ÿæˆå„ç§å†…å®¹çš„è¿‡ç¨‹ã€‚è¿™äº›å†…å®¹å¯ä»¥åŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰ã€‚éšç€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰æŠ€æœ¯çš„å‘å±•ï¼ŒAIGCåœ¨å„ä¸ªé¢†åŸŸå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›å’Œåº”ç”¨å‰æ™¯ã€‚\n\nä¾‹å¦‚ï¼š\n\n1. **æ–‡æœ¬ç”Ÿæˆ**ï¼šä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼Œå¦‚OpenAIçš„GPTç³»åˆ—ï¼Œç”Ÿæˆæ–‡ç« ã€æ•…äº‹ã€è¯—æ­Œç­‰ã€‚å®ƒå¯ä»¥ç”¨äºå†…å®¹åˆ›ä½œã€æ–°é—»æŠ¥é“çš„è‡ªåŠ¨æ’°å†™ç­‰ã€‚\n\n2. **å›¾åƒç”Ÿæˆ**ï¼šåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç­‰æŠ€æœ¯ï¼Œåˆ›å»ºæ–°çš„å›¾åƒæˆ–è‰ºæœ¯ä½œå“ã€‚ä¾‹å¦‚ï¼ŒDALL-Eå’ŒMidjourneyç­‰å·¥å…·èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬æè¿°ç”Ÿæˆç›¸åº”çš„å›¾ç‰‡ã€‚\n\n3. **éŸ³é¢‘ç”Ÿæˆ**ï¼šé€šè¿‡AIç®—æ³•ç”ŸæˆéŸ³ä¹ã€è¯­éŸ³æˆ–å…¶ä»–éŸ³é¢‘å†…å®¹ï¼Œèƒ½å¤Ÿç”¨äºéŸ³ä¹åˆ›ä½œã€è™šæ‹ŸåŠ©ç†ä¸­çš„è¯­éŸ³è¾“å‡ºç­‰ã€‚\n\n4. **è§†é¢‘ç”Ÿæˆ**ï¼šAIä¹Ÿå¯ä»¥ç”Ÿæˆæˆ–ç¼–è¾‘è§†é¢‘å†…å®¹ï¼Œåº”ç”¨äºå½±è§†åˆ¶ä½œã€å¹¿å‘Šåˆ›æ„ç­‰é¢†åŸŸã€‚\n\nAIGCçš„ä¼˜åŠ¿åœ¨äºå…¶é«˜æ•ˆæ€§å’Œåˆ›æ–°èƒ½åŠ›ï¼Œå®ƒèƒ½å¤§å¹…åº¦æé«˜å†…å®¹ç”Ÿäº§çš„é€Ÿåº¦ï¼Œé™ä½æˆæœ¬ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥å¸®åŠ©åˆ›ä½œè€…æ¿€å‘çµæ„Ÿã€‚ä¸è¿‡ï¼ŒAIGCä¹Ÿé¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚ç‰ˆæƒé—®é¢˜ã€ç”Ÿæˆå†…å®¹çš„è´¨é‡å’ŒçœŸå®æ€§ç­‰ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼ŒAIGCçš„åº”ç”¨é¢†åŸŸå’Œå½±å“åŠ›ä»åœ¨ä¸æ–­æ‰©å¤§ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 318, 'prompt_tokens': 31, 'total_tokens': 349, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-Bms2A0v8agsIGevCNImWTIq7ICZgq', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--7aa515e5-37b2-49e7-91be-b29a231537d1-0', usage_metadata={'input_tokens': 31, 'output_tokens': 318, 'total_tokens': 349, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='å¤§æ¨¡å‹æŠ€æœ¯æ˜¯æŒ‡é€šè¿‡å¤§è§„æ¨¡çš„æ•°æ®é›†å’Œå¼ºå¤§çš„è®¡ç®—èƒ½åŠ›è®­ç»ƒå‡ºçš„å¤æ‚æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šå¸¸å…·æœ‰æ•°äº¿ç”šè‡³æ•°ä¸‡äº¿ä¸ªå‚æ•°ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸä¸­æ‰§è¡Œå„ç§ä»»åŠ¡ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«ç­‰ã€‚\n\nä»¥ä¸‹æ˜¯å¤§æ¨¡å‹æŠ€æœ¯çš„ä¸€äº›å…³é”®ç‰¹ç‚¹å’Œåº”ç”¨ï¼š\n\n1. **è§„æ¨¡ä¸å¤æ‚æ€§**ï¼šå¤§æ¨¡å‹é€šå¸¸ç”±å¤šå±‚ç¥ç»ç½‘ç»œæ„æˆï¼Œå…·å¤‡é«˜ç»´åº¦å’Œæ˜¾è‘—çš„è¡¨è¾¾èƒ½åŠ›ã€‚è¿™ä½¿å¾—å®ƒä»¬å¯ä»¥æ•æ‰åˆ°æ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼ã€‚\n\n2. **é¢„è®­ç»ƒä¸å¾®è°ƒ**ï¼šå¤§æ¨¡å‹é€šå¸¸é‡‡ç”¨é¢„è®­ç»ƒå’Œå¾®è°ƒçš„æ–¹æ³•ã€‚æ¨¡å‹å…ˆåœ¨å¤§è§„æ¨¡é€šç”¨æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå†åœ¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚\n\n3. **è¿ç§»å­¦ä¹ **ï¼šå¤§æ¨¡å‹å¯ä»¥å°†ä»ä¸€ä¸ªä»»åŠ¡ä¸­å­¦åˆ°çš„çŸ¥è¯†è¿ç§»åˆ°å…¶ä»–ç›¸å…³ä»»åŠ¡ä¸Šï¼Œä»è€Œå‡å°‘å„ä¸ªä»»åŠ¡ä¹‹é—´çš„è®­ç»ƒæ—¶é—´å’Œæ•°æ®éœ€æ±‚ã€‚\n\n4. **å¤šæ¨¡æ€å­¦ä¹ **ï¼šä¸€äº›å¤§æ¨¡å‹æŠ€æœ¯æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼Œå³å¯ä»¥åŒæ—¶å¤„ç†æ–‡æœ¬ã€å›¾åƒã€å£°éŸ³ç­‰å¤šç§æ•°æ®ç±»å‹ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€šç”¨æ€§å’Œåº”ç”¨èŒƒå›´ã€‚\n\n5. **åº”ç”¨èŒƒå›´å¹¿æ³›**ï¼šå¤§æ¨¡å‹è¢«å¹¿æ³›åº”ç”¨äºèŠå¤©æœºå™¨äººã€è‡ªåŠ¨ç¿»è¯‘ã€å›¾åƒç”Ÿæˆã€åŒ»å­¦å½±åƒåˆ†æã€å†…å®¹æ¨èç­‰å¤šä¸ªé¢†åŸŸã€‚\n\n6. **æŠ€æœ¯æŒ‘æˆ˜**ï¼šå°½ç®¡å¤§æ¨¡å‹æŠ€æœ¯å…·æœ‰å¾ˆé«˜çš„æ€§èƒ½ï¼Œä½†åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¹Ÿå¸¦æ¥äº†è®¡ç®—èµ„æºéœ€æ±‚å¤§ã€èƒ½è€—é«˜ã€æ¨¡å‹è§£é‡Šæ€§å·®ç­‰æŒ‘æˆ˜ã€‚\n\næ€»çš„æ¥è¯´ï¼Œå¤§æ¨¡å‹æŠ€æœ¯ä»£è¡¨äº†æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ç§è¶‹åŠ¿å’Œå‘å±•æ–¹å‘ï¼Œå…¶å¼ºå¤§çš„èƒ½åŠ›ä½¿å…¶åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 383, 'prompt_tokens': 31, 'total_tokens': 414, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-Bms2Atgkau6s0fmfThAacknwoZNal', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--93b50346-c340-4821-9847-562f80fb8cbc-0', usage_metadata={'input_tokens': 31, 'output_tokens': 383, 'total_tokens': 414, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]



#### 2.9.3 åŒæ­¥è°ƒç”¨ä¸å¼‚æ­¥è°ƒç”¨

**åŒæ­¥è°ƒç”¨**

ä¸¾ä¾‹ï¼š

``` python
import time

def call_model():
    # æ¨¡æ‹ŸåŒæ­¥APIè°ƒç”¨
    print("å¼€å§‹è°ƒç”¨æ¨¡å‹...")
    time.sleep(5)  # æ¨¡æ‹Ÿè°ƒç”¨ç­‰å¾…,å•ä½ï¼šç§’
    print("æ¨¡å‹è°ƒç”¨å®Œæˆã€‚")

def perform_other_tasks():
    # æ¨¡æ‹Ÿæ‰§è¡Œå…¶ä»–ä»»åŠ¡
    for i in range(5):
        print(f"æ‰§è¡Œå…¶ä»–ä»»åŠ¡ {i + 1}")
        time.sleep(1)  # å•ä½ï¼šç§’

def main():
    start_time = time.time()
    call_model()
    perform_other_tasks()
    end_time = time.time()
    total_time = end_time - start_time
    return f"æ€»å…±è€—æ—¶ï¼š{total_time}ç§’"

# è¿è¡ŒåŒæ­¥ä»»åŠ¡å¹¶æ‰“å°å®Œæˆæ—¶é—´
main_time = main()
print(main_time)
```

> å¼€å§‹è°ƒç”¨æ¨¡å‹...
> æ¨¡å‹è°ƒç”¨å®Œæˆã€‚
> æ‰§è¡Œå…¶ä»–ä»»åŠ¡ 1
> æ‰§è¡Œå…¶ä»–ä»»åŠ¡ 2
> æ‰§è¡Œå…¶ä»–ä»»åŠ¡ 3
> æ‰§è¡Œå…¶ä»–ä»»åŠ¡ 4
> æ‰§è¡Œå…¶ä»–ä»»åŠ¡ 5
> æ€»å…±è€—æ—¶ï¼š10.061029434204102ç§’

ä¹‹å‰çš„`llm.invoke(...)`æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŒæ­¥è°ƒç”¨ã€‚æ¯ä¸ªæ“ä½œä¾æ¬¡æ‰§è¡Œï¼Œç›´åˆ°å½“å‰æ“ä½œå®Œæˆåæ‰å¼€å§‹ä¸‹ä¸€ä¸ªæ“ä½œï¼Œä»è€Œå¯¼è‡´æ€»çš„æ‰§è¡Œæ—¶é—´æ˜¯å„ä¸ªæ“ä½œæ—¶é—´çš„æ€»å’Œã€‚



**å¼‚æ­¥è°ƒç”¨**

å¼‚æ­¥è°ƒç”¨ï¼Œå…è®¸ç¨‹åºåœ¨ç­‰å¾…æŸäº›æ“ä½œå®Œæˆæ—¶ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡ï¼Œè€Œä¸æ˜¯é˜»å¡ç­‰å¾…ã€‚è¿™åœ¨å¤„ç†I/Oæ“ä½œï¼ˆå¦‚ç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶è¯»å†™ç­‰ï¼‰æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç¨‹åºçš„æ•ˆç‡å’Œå“åº”æ€§ã€‚

ä¸¾ä¾‹ï¼šï¼ˆå†™æ³•1ï¼šæ­¤å†™æ³•ä¸é€‚åˆJupyter Notebook)

``` python
import asyncio
import time

async def async_call(llm):
    await asyncio.sleep(5)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
    print("å¼‚æ­¥è°ƒç”¨å®Œæˆ")

async def perform_other_tasks():
    await asyncio.sleep(5)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
    print("å…¶ä»–ä»»åŠ¡å®Œæˆ")

async def run_async_tasks():
    start_time = time.time()
    await asyncio.gather(
        async_call(None),  # ç¤ºä¾‹è°ƒç”¨ï¼Œæ›¿æ¢Noneä¸ºæ¨¡æ‹Ÿçš„LLMå¯¹è±¡
        perform_other_tasks()
    )
    end_time = time.time()
    return f"æ€»å…±è€—æ—¶ï¼š{end_time - start_time}ç§’"

# æ­£ç¡®è¿è¡Œå¼‚æ­¥ä»»åŠ¡çš„æ–¹å¼
if __name__ == "__main__":
    # ä½¿ç”¨ asyncio.run() æ¥å¯åŠ¨å¼‚æ­¥ç¨‹åº
    result = asyncio.run(run_async_tasks())
    print(result)

```

å†™æ³•2ï¼šæ­¤å†™æ³•é€‚åˆJupyter Notebook

```python
import asyncio
import time

async def async_call(llm):
    await asyncio.sleep(5)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
    print("å¼‚æ­¥è°ƒç”¨å®Œæˆ")

async def perform_other_tasks():
    await asyncio.sleep(5)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
    print("å…¶ä»–ä»»åŠ¡å®Œæˆ")

async def run_async_tasks():
    start_time = time.time()
    await asyncio.gather(
        async_call(None),  # ç¤ºä¾‹è°ƒç”¨ï¼Œæ›¿æ¢Noneä¸ºæ¨¡æ‹Ÿçš„LLMå¯¹è±¡
        perform_other_tasks()
    )
    end_time = time.time()
    return f"æ€»å…±è€—æ—¶ï¼š{end_time - start_time}ç§’"

# # æ­£ç¡®è¿è¡Œå¼‚æ­¥ä»»åŠ¡çš„æ–¹å¼
# if __name__ == "__main__":
#     # ä½¿ç”¨ asyncio.run() æ¥å¯åŠ¨å¼‚æ­¥ç¨‹åº
#     result = asyncio.run(run_async_tasks())
#     print(result)


# åœ¨ Jupyter å•å…ƒæ ¼ä¸­ç›´æ¥è°ƒç”¨
result = await run_async_tasks()
print(result)
```

> å¼‚æ­¥è°ƒç”¨å®Œæˆ
> å…¶ä»–ä»»åŠ¡å®Œæˆ
> æ€»å…±è€—æ—¶ï¼š5.001038551330566ç§’

ä½¿ç”¨`asyncio.gather()`å¹¶è¡Œæ‰§è¡Œæ—¶ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œå› ä¸ºä¸¤ä¸ªä»»åŠ¡å‡ ä¹åŒæ—¶å¼€å§‹ï¼Œå®ƒä»¬çš„æ‰§è¡Œæ—¶é—´å°†é‡å ã€‚å¦‚æœä¸¤ä¸ªä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´ç›¸åŒï¼ˆè¿™é‡Œéƒ½æ˜¯5ç§’ï¼‰ï¼Œé‚£ä¹ˆæ€»æ‰§è¡Œæ—¶é—´åº”è¯¥æ¥è¿‘å•ä¸ªä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´ï¼Œè€Œä¸æ˜¯ä¸¤è€…æ—¶é—´ä¹‹å’Œã€‚



**å¼‚æ­¥è°ƒç”¨ä¹‹ainvoke**

ä¸¾ä¾‹1ï¼šéªŒè¯ainvokeæ˜¯å¦æ˜¯å¼‚æ­¥

```python
# æ–¹å¼1
import inspect

print("ainvoke æ˜¯åç¨‹å‡½æ•°:", inspect.iscoroutinefunction(chat_model.ainvoke))
print("invoke æ˜¯åç¨‹å‡½æ•°:", inspect.iscoroutinefunction(chat_model.invoke))
```

> ainvoke æ˜¯åç¨‹å‡½æ•°: True
> invoke æ˜¯åç¨‹å‡½æ•°: False

ä¸¾ä¾‹2ï¼šï¼ˆä¸èƒ½åœ¨Jupyter Notebookä¸­æµ‹è¯•ï¼‰

```python
import asyncio
import os
import time

import dotenv
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

# åˆå§‹åŒ–å¤§æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini")

# åŒæ­¥è°ƒç”¨ï¼ˆå¯¹æ¯”ç»„ï¼‰
def sync_test():
    messages1 = [SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„æ™ºèƒ½å°åŠ©æ‰‹"),
                 HumanMessage(content="è¯·å¸®æˆ‘ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "), ]
    start_time = time.time()
    response = chat_model.invoke(messages1)  # åŒæ­¥è°ƒç”¨
    duration = time.time() - start_time
    print(f"åŒæ­¥è°ƒç”¨è€—æ—¶ï¼š{duration:.2f}ç§’")
    return response, duration


# å¼‚æ­¥è°ƒç”¨ï¼ˆå®éªŒç»„ï¼‰
async def async_test():
    messages1 = [SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„æ™ºèƒ½å°åŠ©æ‰‹"),
                 HumanMessage(content="è¯·å¸®æˆ‘ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "), ]
    start_time = time.time()
    response = await chat_model.ainvoke(messages1)  # å¼‚æ­¥è°ƒç”¨
    duration = time.time() - start_time
    print(f"å¼‚æ­¥è°ƒç”¨è€—æ—¶ï¼š{duration:.2f}ç§’")
    return response, duration


# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    # è¿è¡ŒåŒæ­¥æµ‹è¯•
    sync_response, sync_duration = sync_test()
    print(f"åŒæ­¥å“åº”å†…å®¹: {sync_response.content[:100]}...\n")

    # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    async_response, async_duration = asyncio.run(async_test())
    print(f"å¼‚æ­¥å“åº”å†…å®¹: {async_response.content[:100]}...\n")

    # å¹¶å‘æµ‹è¯• - ä¿®å¤ç‰ˆæœ¬
    print("\n=== å¹¶å‘æµ‹è¯• ===")
    start_time = time.time()


    async def run_concurrent_tests():
        # åˆ›å»º3ä¸ªå¼‚æ­¥ä»»åŠ¡
        tasks = [async_test() for _ in range(3)]
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        return await asyncio.gather(*tasks)


    # æ‰§è¡Œå¹¶å‘æµ‹è¯•
    results = asyncio.run(run_concurrent_tests())

    total_time = time.time() - start_time
    print(f"\n3ä¸ªå¹¶å‘å¼‚æ­¥è°ƒç”¨æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"å¹³å‡æ¯ä¸ªè°ƒç”¨è€—æ—¶: {total_time / 3:.2f}ç§’")

```

> ```
> åŒæ­¥è°ƒç”¨è€—æ—¶ï¼š5.73ç§’
> åŒæ­¥å“åº”å†…å®¹: æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡è®©è®¡ç®—æœºç³»ç»Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›å…¶æ€§èƒ½ï¼Œè€Œæ— éœ€æ˜ç¡®çš„ç¼–ç¨‹ã€‚å®ƒçš„æ ¸å¿ƒç†å¿µæ˜¯åˆ©ç”¨ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ï¼Œåˆ†æå’Œè¯†åˆ«æ•°æ®ä¸­çš„æ¨¡å¼ï¼Œä»è€Œåœ¨æ–°æ•°æ®å‡ºç°æ—¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚
> 
> ...
> 
> å¼‚æ­¥è°ƒç”¨è€—æ—¶ï¼š4.68ç§’
> å¼‚æ­¥å“åº”å†…å®¹: æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æŠ€æœ¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿé€šè¿‡ç»éªŒè‡ªæˆ‘å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚æ¢å¥è¯è¯´ï¼Œæœºå™¨å­¦ä¹ ä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­æå–æ¨¡å¼å’Œè§„å¾‹ï¼Œå¹¶æ ¹æ®è¿™äº›ä¿¡æ¯è¿›è¡Œé¢„æµ‹æˆ–å†³ç­–ã€‚
> 
> æœºå™¨å­¦ä¹ çš„åŸºæœ¬è¿‡ç¨‹é€šå¸¸...
> 
> 
> === å¹¶å‘æµ‹è¯• ===
> å¼‚æ­¥è°ƒç”¨è€—æ—¶ï¼š3.07ç§’
> å¼‚æ­¥è°ƒç”¨è€—æ—¶ï¼š3.61ç§’
> å¼‚æ­¥è°ƒç”¨è€—æ—¶ï¼š7.43ç§’
> 
> 3ä¸ªå¹¶å‘å¼‚æ­¥è°ƒç”¨æ€»è€—æ—¶: 7.43ç§’
> å¹³å‡æ¯ä¸ªè°ƒç”¨è€—æ—¶: 2.48ç§’
> ```



### 2.10 Embeddings Modelsä½¿ç”¨ä¸¾ä¾‹

Embeddings Modelsï¼ˆåµŒå…¥æ¨¡å‹ï¼‰ç‰¹ç‚¹ï¼šå°†`å­—ç¬¦ä¸²`ä½œä¸ºè¾“å…¥ï¼Œè¿”å›ä¸€ä¸ª`æµ®ç‚¹æ•°`çš„åˆ—è¡¨ã€‚åœ¨NLPä¸­ï¼ŒEmbeddingçš„ä½œç”¨å°±æ˜¯å°†æ•°æ®è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–ã€‚

![åµŒå…¥æ¨¡å‹](images/åµŒå…¥æ¨¡å‹.png)

```python
import os
import dotenv
from langchain_openai import OpenAIEmbeddings

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

embeddings_model = OpenAIEmbeddings(
    model="text-embedding-ada-002"
)

res1 = embeddings_model.embed_query('è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£')
print(res1)

# æ‰“å°ç»“æœï¼š[-0.004306625574827194, 0.003083756659179926, -0.013916781172156334, ...., ]

res2 = embeddings_model.embed_documents(['è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£', 'è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•æ–‡æ¡£'])
print(res2)
# æ‰“å°ç»“æœï¼š[[-0.004306625574827194, 0.003083756659179926, -0.013916781172156334,...],[...,...,.... ]]
```



## 3ã€Model I/Oä¹‹Prompt Template

Prompts Templateï¼Œé€šè¿‡æ¨¡æ¿ç®¡ç†å¤§æ¨¡å‹çš„è¾“å…¥ã€‚

### 3.1 ä»‹ç»ä¸åˆ†ç±»

Prompt Template æ˜¯LangChainä¸­çš„ä¸€ä¸ªæ¦‚å¿µï¼Œæ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œè¿”å›ä¸€ä¸ªä¼ é€’ç»™LLMçš„ä¿¡æ¯ï¼ˆå³æç¤ºè¯promptï¼‰ã€‚

åœ¨åº”ç”¨å¼€å‘ä¸­ï¼Œå›ºå®šçš„æç¤ºè¯é™åˆ¶äº†æ¨¡å‹çš„çµæ´»æ€§å’Œé€‚ç”¨èŒƒå›´ã€‚æ‰€ä»¥ï¼Œprompt template æ˜¯ä¸€ä¸ªæ¨¡æ¿åŒ–çš„å­—ç¬¦ä¸²ï¼Œä½ å¯ä»¥**å°†å˜é‡æ’å…¥åˆ°æ¨¡æ¿**ä¸­ï¼Œä»è€Œåˆ›å»ºå‡ºä¸åŒçš„æç¤ºã€‚

æç¤ºæ¨¡æ¿ä»¥**å­—å…¸**ä½œä¸ºè¾“å…¥ï¼Œå…¶ä¸­æ¯ä¸ªé”®ä»£è¡¨è¦å¡«å……çš„æç¤ºæ¨¡æ¿ä¸­çš„å˜é‡ã€‚ 

æç¤ºæ¨¡æ¿è¾“å‡ºä¸€ä¸ª **PromptValue**ã€‚è¿™ä¸ª PromptValue å¯ä»¥ä¼ é€’ç»™ LLM æˆ– ChatModelï¼Œå¹¶ä¸”è¿˜å¯ä»¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨ã€‚

>  å­˜åœ¨ PromptValue çš„åŸå› æ˜¯ä¸ºäº†æ–¹ä¾¿åœ¨å­—ç¬¦ä¸²å’Œæ¶ˆæ¯ä¹‹é—´åˆ‡æ¢ï¼›æ¨¡æ¿å’Œå¤§æ¨¡å‹ä¹‹é—´ä¼ é€’ï¼ˆchainä¸­ï¼‰

æœ‰å‡ ç§ä¸åŒç±»å‹çš„æç¤ºæ¨¡æ¿ï¼š

- `PromptTemplate`ï¼šLLMæç¤ºæ¨¡æ¿ï¼Œç”¨äº**ç”Ÿæˆå­—ç¬¦ä¸²æç¤º**ã€‚å®ƒä½¿ç”¨ Python çš„å­—ç¬¦ä¸²æ¥æ¨¡æ¿æç¤ºã€‚
- `ChatPromptTemplate`ï¼šèŠå¤©æç¤ºæ¨¡æ¿ï¼Œç”¨äº**ç»„åˆå„ç§è§’è‰²çš„æ¶ˆæ¯æ¨¡æ¿**ï¼Œä¼ å…¥èŠå¤©æ¨¡å‹ã€‚
  æ¶ˆæ¯æ¨¡æ¿åŒ…æ‹¬ï¼šSystemMessagePromptTemplateã€HumanMessagePromptTemplateã€AIMessagePromptTemplateã€ChatMessagePromptTemplateç­‰
- `FewShotPromptTemplate`ï¼šæ ·æœ¬æç¤ºæ¨¡æ¿ï¼Œé€šè¿‡ç¤ºä¾‹æ¥æ•™æ¨¡å‹å¦‚ä½•å›ç­”
- `PipelinePrompt`ï¼šç®¡é“æç¤ºæ¨¡æ¿ï¼Œç”¨äºæŠŠå‡ ä¸ªæç¤ºç»„åˆåœ¨ä¸€èµ·ä½¿ç”¨ã€‚
- `è‡ªå®šä¹‰æ¨¡æ¿`ï¼šå…è®¸åŸºäºå…¶å®ƒæ¨¡æ¿ç±»æ¥å®šåˆ¶è‡ªå·±çš„æç¤ºæ¨¡æ¿ã€‚

**æ¨¡ç‰ˆå¯¼å…¥**


```python
from langchain.prompts.prompt import PromptTemplate

from langchain.prompts import ChatPromptTemplate

from langchain.prompts import FewShotPromptTemplate

from langchain.prompts.pipeline import PipelinePromptTemplate

from langchain.prompts import (
    ChatMessagePromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
```



### 3.2 å¤ä¹ ï¼šstr.format()

Pythonçš„`str.format()`æ–¹æ³•æ˜¯ä¸€ç§å­—ç¬¦ä¸²æ ¼å¼åŒ–çš„æ‰‹æ®µï¼Œå…è®¸åœ¨`å­—ç¬¦ä¸²ä¸­æ’å…¥å˜é‡`ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œå¯ä»¥åˆ›å»ºåŒ…å«`å ä½ç¬¦`çš„å­—ç¬¦ä¸²æ¨¡æ¿ï¼Œå ä½ç¬¦ç”±èŠ±æ‹¬å·`{}`æ ‡è¯†ã€‚

- è°ƒç”¨format()æ–¹æ³•æ—¶ï¼Œå¯ä»¥ä¼ å…¥ä¸€ä¸ªæˆ–å¤šä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°å°†è¢«é¡ºåºæ›¿æ¢è¿›å ä½ç¬¦ä¸­ã€‚
- str.format()æä¾›äº†çµæ´»çš„æ–¹å¼æ¥æ„é€ å­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç§æ ¼å¼åŒ–é€‰é¡¹ã€‚

åœ¨LangChainçš„é»˜è®¤è®¾ç½®ä¸‹ï¼Œ `PromptTemplate` ä½¿ç”¨ Python çš„`str.format()` æ–¹æ³•è¿›è¡Œæ¨¡æ¿åŒ–ã€‚è¿™æ ·åœ¨æ¨¡å‹æ¥æ”¶è¾“å…¥å‰ï¼Œå¯ä»¥æ ¹æ®éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†å’Œç»“æ„åŒ–ã€‚

**åŸºæœ¬ç”¨æ³•**

``` python
# ç®€å•ç¤ºä¾‹ï¼Œç›´æ¥æ›¿æ¢
greeting = "Hello, {}!".format("Alice")
print(greeting)
# è¾“å‡º: Hello, Alice!
```

> Hello, Alice!

**å¸¦æœ‰ä½ç½®å‚æ•°çš„ç”¨æ³•**

``` python
# ä½¿ç”¨ä½ç½®å‚æ•°
info = "Name: {0}, Age: {1}".format("Jerry", 25)
print(info)
```

> Name: Jerry, Age: 25

**å¸¦æœ‰å…³é”®å­—å‚æ•°çš„ç”¨æ³•**

``` python
# ä½¿ç”¨å…³é”®å­—å‚æ•°
info = "Name: {name}, Age: {age}".format(name="Tom", age=25)
print(info)
```

> Name: Tom, Age: 25

**ä½¿ç”¨å­—å…¸è§£åŒ…çš„æ–¹å¼**

``` python
# ä½¿ç”¨å­—å…¸è§£åŒ…
person = {"name": "David", "age": 40}
info = "Name: {name}, Age: {age}".format(**person)
print(info)
```

> Name: David, Age: 40



### 3.2 å…·ä½“ä½¿ç”¨ï¼šPromptTemplate

#### 3.2.1 ä½¿ç”¨è¯´æ˜

PromptTemplateç±»ï¼Œç”¨äºå¿«é€Ÿæ„å»º`åŒ…å«å˜é‡`çš„æç¤ºè¯æ¨¡æ¿ï¼Œå¹¶é€šè¿‡`ä¼ å…¥ä¸åŒçš„å‚æ•°å€¼`ç”Ÿæˆè‡ªå®šä¹‰çš„æç¤ºè¯ã€‚

**ä¸»è¦å‚æ•°ä»‹ç»ï¼š**

- **templateï¼š**å®šä¹‰æç¤ºè¯æ¨¡æ¿çš„å­—ç¬¦ä¸²ï¼Œå…¶ä¸­åŒ…å«`æ–‡æœ¬`å’Œ`å˜é‡å ä½ç¬¦ï¼ˆå¦‚{name}ï¼‰`ï¼›

- **input_variablesï¼š** åˆ—è¡¨ï¼ŒæŒ‡å®šäº†æ¨¡æ¿ä¸­ä½¿ç”¨çš„å˜é‡åç§°ï¼Œåœ¨è°ƒç”¨æ¨¡æ¿æ—¶è¢«æ›¿æ¢ï¼›
- **partial_variablesï¼š**å­—å…¸ï¼Œç”¨äºå®šä¹‰æ¨¡æ¿ä¸­ä¸€äº›å›ºå®šçš„å˜é‡åã€‚è¿™äº›å€¼ä¸éœ€è¦å†æ¯æ¬¡è°ƒç”¨æ—¶è¢«æ›¿æ¢ã€‚

**å‡½æ•°ä»‹ç»ï¼š**

- format()ï¼šç»™input_variableså˜é‡èµ‹å€¼ï¼Œå¹¶è¿”å›æç¤ºè¯ã€‚åˆ©ç”¨format() è¿›è¡Œæ ¼å¼åŒ–æ—¶å°±ä¸€å®šè¦èµ‹å€¼ï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚å½“åœ¨templateä¸­æœªè®¾ç½®input_variablesï¼Œåˆ™ä¼šè‡ªåŠ¨å¿½ç•¥ã€‚

#### 3.2.2 æ–¹å¼1ï¼šä½¿ç”¨æ„é€ æ–¹æ³•

ä¸¾ä¾‹1ï¼š

```python
from langchain.prompts import PromptTemplate

# å®šä¹‰æ¨¡æ¿ï¼šæè¿°ä¸»é¢˜çš„åº”ç”¨
template = PromptTemplate(template="è¯·ç®€è¦æè¿°{topic}çš„åº”ç”¨ã€‚",
                          input_variables=["topic"])

print(template)

# ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæç¤ºè¯
prompt_1 = template.format(topic="æœºå™¨å­¦ä¹ ")
prompt_2 = template.format(topic="è‡ªç„¶è¯­è¨€å¤„ç†")

print("æç¤ºè¯1:", prompt_1)
print("æç¤ºè¯2:", prompt_2)
```

> input_variables=['topic'] input_types={} partial_variables={} template='è¯·ç®€è¦æè¿°{topic}çš„åº”ç”¨ã€‚'
> æç¤ºè¯1: è¯·ç®€è¦æè¿°æœºå™¨å­¦ä¹ çš„åº”ç”¨ã€‚
> æç¤ºè¯2: è¯·ç®€è¦æè¿°è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨ã€‚

å¯ä»¥ç›´è§‚çš„çœ‹åˆ°PromptTemplateå¯ä»¥å°†templateä¸­å£°æ˜çš„å˜é‡topicå‡†ç¡®æå–å‡ºæ¥ï¼Œä½¿promptæ›´æ¸…æ™°ã€‚

ä¸¾ä¾‹2ï¼šå®šä¹‰å¤šå˜é‡æ¨¡æ¿

```python
from langchain.prompts import PromptTemplate

#å®šä¹‰å¤šå˜é‡æ¨¡æ¿
template = PromptTemplate(
    template="è¯·è¯„ä»·{product}çš„ä¼˜ç¼ºç‚¹ï¼ŒåŒ…æ‹¬{aspect1}å’Œ{aspect2}ã€‚",
    input_variables=["product", "aspect1", "aspect2"])

#ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæç¤ºè¯
prompt_1 = template.format(product="æ™ºèƒ½æ‰‹æœº", aspect1="ç”µæ± ç»­èˆª", aspect2="æ‹ç…§è´¨é‡")
prompt_2 = template.format(product="ç¬”è®°æœ¬ç”µè„‘", aspect1="å¤„ç†é€Ÿåº¦", aspect2="ä¾¿æºæ€§")

print("æç¤ºè¯1:",prompt_1)
print("æç¤ºè¯2:",prompt_2)
```

ä¸¾ä¾‹3ï¼š

```python
# æŠ€æœ¯ä¸»é¢˜åˆ—è¡¨
from langchain_core.prompts import PromptTemplate

topics = ["æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†", "åŒºå—é“¾"]

# å®šä¹‰æ¨¡æ¿
template = PromptTemplate(template="è¯·ç®€è¦æè¿°{topic}çš„åº”ç”¨ã€‚",
                          input_variables=["topic"])
# æ‰¹é‡ç”Ÿæˆæç¤ºè¯å¹¶ç”Ÿæˆå†…å®¹
for topic in topics:
    prompt = template.format(topic=topic)
    print(f"{topic}çš„ç”Ÿæˆç»“æœ:", prompt)
```

#### 3.2.3 æ–¹å¼2ï¼šè°ƒç”¨from_template()

ä¸¾ä¾‹1ï¼š

``` python
from langchain.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    "è¯·ç»™æˆ‘ä¸€ä¸ªå…³äº{topic}çš„{type}è§£é‡Šã€‚"
)

#ä¼ å…¥æ¨¡æ¿ä¸­çš„å˜é‡å
prompt = prompt_template.format(type="è¯¦ç»†", topic="é‡å­åŠ›å­¦")

print(prompt)
```

> è¯·ç»™æˆ‘ä¸€ä¸ªå…³äºé‡å­åŠ›å­¦çš„è¯¦ç»†è§£é‡Šã€‚

ä¸¾ä¾‹2ï¼šæ¨¡æ¿æ”¯æŒä»»æ„æ•°é‡çš„å˜é‡ï¼ŒåŒ…æ‹¬ä¸å«å˜é‡ï¼š

```python
#1.å¯¼å…¥ç›¸å…³çš„åŒ…
from langchain_core.prompts import PromptTemplate

# 2.å®šä¹‰æç¤ºè¯æ¨¡ç‰ˆå¯¹è±¡
text = """
Tell me a joke
"""

prompt_template = PromptTemplate.from_template(text)
# 3.é»˜è®¤ä½¿ç”¨f-stringè¿›è¡Œæ ¼å¼åŒ–ï¼ˆè¿”å›æ ¼å¼å¥½çš„å­—ç¬¦ä¸²ï¼‰
prompt = prompt_template.format()
print(prompt)
```

> Tell me a joke



#### 3.2.4 æ–¹å¼3ï¼šéƒ¨åˆ†æç¤ºè¯æ¨¡ç‰ˆ

åœ¨ç”Ÿæˆpromptå‰å°±å·²ç»æå‰åˆå§‹åŒ–éƒ¨åˆ†çš„æç¤ºè¯ï¼Œå®é™…è¿›ä¸€æ­¥å¯¼å…¥æ¨¡ç‰ˆçš„æ—¶å€™åªå¯¼å…¥é™¤å·²åˆå§‹åŒ–çš„å˜é‡å³å¯ã€‚

ä¸¾ä¾‹1ï¼š

æ–¹å¼1ï¼šå¯ä»¥ä½¿ç”¨ PromptTemplate.partial() æ–¹æ³•åˆ›å»ºéƒ¨åˆ†æç¤ºæ¨¡æ¿ã€‚

```python
from langchain.prompts import PromptTemplate

template1 = PromptTemplate(template="{foo}{bar}", input_variables=["foo", "bar"])

#æ–¹å¼1ï¼š
partial_template1 = template1.partial(foo="hello")

prompt1 = partial_template1.format(bar="world")

print(prompt1)
```

æ–¹å¼2ï¼šå¯ä»¥åªä½¿ç”¨åˆ†éƒ¨å˜é‡åˆå§‹åŒ–æç¤ºã€‚

```python
from langchain.prompts import PromptTemplate

#æ–¹å¼2ï¼š
template2 = PromptTemplate(template="{foo}{bar}", input_variables=["foo","bar"], partial_variables={"foo": "hello"})

prompt2 = template2.format(bar="world")

print(prompt2)
```



ä¸¾ä¾‹2ï¼š

```python
from langchain_core.prompts import PromptTemplate

# å®Œæ•´æ¨¡æ¿
full_template = """ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œè¯·ç”¨{style}é£æ ¼å›ç­”ï¼š
é—®é¢˜ï¼š{question}
ç­”æ¡ˆï¼š"""

# é¢„å¡«å……è§’è‰²å’Œé£æ ¼
partial_template = PromptTemplate.from_template(full_template).partial(
    role="èµ„æ·±å¨å¸ˆ",
    style="ä¸“ä¸šä½†å¹½é»˜"
)

# åªéœ€æä¾›å‰©ä½™å˜é‡
print(partial_template.format(question="å¦‚ä½•ç…ç‰›æ’ï¼Ÿ"))
```

> ä½ æ˜¯ä¸€ä¸ªèµ„æ·±å¨å¸ˆï¼Œè¯·ç”¨ä¸“ä¸šä½†å¹½é»˜é£æ ¼å›ç­”ï¼š
> é—®é¢˜ï¼šå¦‚ä½•ç…ç‰›æ’ï¼Ÿ
> ç­”æ¡ˆï¼š

#### 3.2.5 æ–¹å¼4ï¼šç»„åˆæç¤ºè¯

ä¸¾ä¾‹ï¼š

```python
from langchain_core.prompts import PromptTemplate

template = (
    PromptTemplate.from_template("Tell me a joke about {topic}")
    + ", make it funny"
    + "\n\nand in {language}"
)

prompt = template.format(topic="sports", language="spanish")
print(prompt)
```

> Tell me a joke about sports, make it funny
>
> and in spanish



#### 3.2.5 invoke()æ›¿æ¢format()

åªè¦å¯¹è±¡æ˜¯RunnableSerializableæ¥å£ç±»å‹ï¼Œéƒ½å¯ä»¥ä½¿ç”¨invoke()ï¼Œæ›¿æ¢å‰é¢ä½¿ç”¨format()çš„è°ƒç”¨æ–¹å¼ã€‚

format()ï¼Œè¿”å›å€¼ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼›invoke()ï¼Œè¿”å›å€¼ä¸ºPromptValueç±»å‹ï¼Œæ¥ç€è°ƒç”¨to_string()è¿”å›å­—ç¬¦ä¸²ã€‚

ä¸¾ä¾‹1ï¼š

```python
#1.å¯¼å…¥ç›¸å…³çš„åŒ…
from langchain_core.prompts import PromptTemplate

# 2.å®šä¹‰æç¤ºè¯æ¨¡ç‰ˆå¯¹è±¡
prompt_template = PromptTemplate.from_template(
    "Tell me a {adjective} joke about {content}."
)
# 3.é»˜è®¤ä½¿ç”¨f-stringè¿›è¡Œæ ¼å¼åŒ–ï¼ˆè¿”å›æ ¼å¼å¥½çš„å­—ç¬¦ä¸²ï¼‰
prompt_template.invoke({"adjective":"funny", "content":"chickens"})
```

> StringPromptValue(text='Tell me a funny joke about chickens.')

ä¸¾ä¾‹2ï¼š

```python
#1.å¯¼å…¥ç›¸å…³çš„åŒ…
from langchain_core.prompts import PromptTemplate

# 2.ä½¿ç”¨åˆå§‹åŒ–å™¨è¿›è¡Œå®ä¾‹åŒ–
prompt = PromptTemplate(
    input_variables=["adjective", "content"],
    template="Tell me a {adjective} joke about {content}")

# 3. PromptTemplateåº•å±‚æ˜¯RunnableSerializableæ¥å£ æ‰€ä»¥å¯ä»¥ç›´æ¥ä½¿ç”¨invoke()è°ƒç”¨
prompt.invoke({"adjective": "funny", "content": "chickens"})
```

ä¸¾ä¾‹3ï¼š

```python
from langchain_core.prompts import PromptTemplate

prompt_template = (
        PromptTemplate.from_template("Tell me a joke about {topic}")
        + ", make it funny"
        + " and in {language}"
)

prompt = prompt_template.invoke({"topic":"sports", "language":"spanish"})
print(prompt)
```



#### 3.2.6 promptç»“åˆLLM

Prompt ä¸å¤§æ¨¡å‹ç»“åˆï¼š


```python
import os
import dotenv
from langchain_openai import OpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

llm = OpenAI()

prompt_template = PromptTemplate(
    input_variables=["text"],
    template="æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¨‹åºå‘˜ã€‚\nå¯¹äºä¿¡æ¯ {text} è¿›è¡Œç®€çŸ­æè¿°"
)
#print(prompt_template.format(text="langchain"))

# è¾“å…¥æç¤º
prompt = prompt_template.format(text="langchain")

# å¾—åˆ°æ¨¡å‹çš„è¾“å‡º
output = llm.invoke(prompt)
# output = model.invoke("æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¨‹åºå‘˜ã€‚å¯¹äºä¿¡æ¯ langchain è¿›è¡Œç®€çŸ­æè¿°")

# æ‰“å°è¾“å‡ºå†…å®¹
print(output)
print(type(output))
```

> Langchain æ˜¯ä¸€ç§åŒºå—é“¾æŠ€æœ¯ï¼Œæ—¨åœ¨æä¾›å¤šè¯­è¨€æ™ºèƒ½åˆçº¦ç¼–ç¨‹å’Œæ‰§è¡Œçš„å¹³å°ã€‚å®ƒå…è®¸ç¨‹åºå‘˜ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€æ¥ç¼–å†™æ™ºèƒ½åˆçº¦ï¼Œå¹¶æä¾›ä¸€ä¸ªå¯é çš„æ‰§è¡Œç¯å¢ƒã€‚Langchainçš„ç›®æ ‡æ˜¯ä¿ƒè¿›è·¨è¯­è¨€çš„æ™ºèƒ½åˆçº¦å¼€å‘å’Œè·¨å¹³å°çš„å¯ç§»æ¤æ€§ï¼Œä¸ºåŒºå—é“¾æŠ€æœ¯å¸¦æ¥æ›´é«˜çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚å®ƒä¹Ÿè‡´åŠ›äºæä¾›æ›´ç®€å•ã€æ›´å®‰å…¨å’Œæ›´å®¹æ˜“ç»´æŠ¤çš„æ™ºèƒ½åˆçº¦ç¼–ç¨‹æ–¹å¼ã€‚æœ€ç»ˆï¼ŒLangchainæ—¨åœ¨æ¨åŠ¨åŒºå—é“¾æŠ€æœ¯çš„å‘å±•ï¼Œä¸ºç”¨æˆ·å’Œå¼€å‘è€…å¸¦æ¥æ›´å¥½çš„ä½“éªŒã€‚ 
><class 'str'>

### 3.3 å…·ä½“ä½¿ç”¨ï¼šChatPromptTemplate

#### 3.3.1 ä½¿ç”¨è¯´æ˜

ChatPromptTemplateæ˜¯åˆ›å»º`èŠå¤©æ¶ˆæ¯åˆ—è¡¨`çš„æç¤ºæ¨¡æ¿ã€‚å®ƒæ¯”æ™®é€š PromptTemplate æ›´é€‚åˆå¤„ç†å¤šè§’è‰²ã€å¤šè½®æ¬¡çš„å¯¹è¯åœºæ™¯ã€‚

**ç‰¹ç‚¹**ï¼š

- æ”¯æŒ `System`/`Human`/`AI` ç­‰ä¸åŒè§’è‰²çš„æ¶ˆæ¯æ¨¡æ¿
- å¯¹è¯å†å²ç»´æŠ¤

**å‚æ•°ç±»å‹ï¼š**åˆ—è¡¨å‚æ•°æ ¼å¼æ˜¯tupleç±»å‹ï¼ˆ`role`:str `content`:str ç»„åˆæœ€å¸¸ç”¨ï¼‰

å…ƒç»„çš„æ ¼å¼ä¸ºï¼š
`(role: str | type, content: str | list[dict] | list[object])`

- å…¶ä¸­ `role` æ˜¯ï¼š
  - å­—ç¬¦ä¸²ï¼ˆå¦‚ `"system"`ã€`"human"`ã€`"ai"`ï¼‰
  - æ¶ˆæ¯ç±»ï¼ˆå¦‚ `SystemMessage`ã€`HumanMessage` çš„ç±»å¯¹è±¡ï¼‰<strong style='color:red'>ä¸æ”¯æŒ</strong>

#### 3.3.2 æ–¹å¼1ï¼šä½¿ç”¨æ„é€ æ–¹æ³•

ä¸¾ä¾‹ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate

#å‚æ•°ç±»å‹è¿™é‡Œä½¿ç”¨çš„æ˜¯tupleæ„æˆçš„list
prompt_template = ChatPromptTemplate([
    # å­—ç¬¦ä¸² role + å­—ç¬¦ä¸² content
    ("system", "ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ {name}."),
    ("human", "ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?"),
    ("ai", "æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰."),
    ("human", "{user_input}")
])

#æ–¹å¼1ï¼šè°ƒç”¨format()æ–¹æ³•ï¼Œè¿”å›å­—ç¬¦ä¸²
prompt = prompt_template.format(name="å°è°·AI", user_input="ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?")
print(type(prompt))
print(prompt)

```

> <class 'str'>
>
> System: ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ å°è°·AI.
> Human: ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?
> AI: æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰.
> Human: ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?

å¯¹æ¯”ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate([
    ("system", "ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ {name}."),
    ("human", "ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?"),
    ("ai", "æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰."),
    ("human", "{user_input}")
])

#æ–¹å¼2ï¼šè°ƒç”¨format_messages()æ–¹æ³•ï¼Œè¿”å›æ¶ˆæ¯åˆ—è¡¨
prompt2 = prompt_template.format_messages(name="å°è°·AI", user_input="ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?")
print(type(prompt2))
print(prompt2)
```

> <class 'list'>
> [SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ å°è°·AI.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?', additional_kwargs={}, response_metadata={}), AIMessage(content='æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?', additional_kwargs={}, response_metadata={})]

**ç»“è®ºï¼š**ç»™å ä½ç¬¦èµ‹å€¼ï¼Œé’ˆå¯¹äºChatPromptTemplateï¼Œæ¨èä½¿ç”¨`format_messages()`æ–¹æ³•ï¼Œè¿”å›æ¶ˆæ¯åˆ—è¡¨ã€‚

#### 3.3.3 æ–¹å¼2ï¼šè°ƒç”¨from_messages()

ä¸¾ä¾‹1ï¼š

``` python
# å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain_core.prompts import ChatPromptTemplate

# å®šä¹‰èŠå¤©æç¤ºè¯æ¨¡ç‰ˆ
chat_template = ChatPromptTemplate.from_messages(
    [
        ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIæœºå™¨äººï¼Œä½ çš„åå­—æ˜¯{name}ã€‚"),
        ("human", "ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ"),
        ("ai", "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼"),
        ("human", "{user_input}"),
    ]
)

# æ ¼å¼åŒ–èŠå¤©æç¤ºè¯æ¨¡ç‰ˆä¸­çš„å˜é‡
messages = chat_template.format_messages(name="å°æ˜", user_input="ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ")

# æ‰“å°æ ¼å¼åŒ–åçš„èŠå¤©æç¤ºè¯æ¨¡ç‰ˆå†…å®¹
print(messages)
```

> [SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIæœºå™¨äººï¼Œä½ çš„åå­—æ˜¯å°æ˜ã€‚', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ', additional_kwargs={}, response_metadata={})]

ä¸¾ä¾‹2ï¼š

```python
# ç¤ºä¾‹ 1: role ä¸ºå­—ç¬¦ä¸²
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ª{role}."),  
    ("human", "{user_input}"),
])

# ç¤ºä¾‹ 2: role ä¸ºæ¶ˆæ¯ç±» ä¸æ”¯æŒ
from langchain_core.messages import SystemMessage, HumanMessage

# prompt = ChatPromptTemplate.from_messages([
#     (SystemMessage, "ä½ æ˜¯ä¸€ä¸ª{role}."),  # ç±»å¯¹è±¡ role + å­—ç¬¦ä¸² content
#     (HumanMessage, ["ä½ å¥½ï¼", {"type": "text"}]),  # ç±»å¯¹è±¡ role + list[dict] content
# ])
# ä¿®æ”¹
prompt = ChatPromptTemplate.from_messages([
    ("system", ["ä½ å¥½ï¼", {"type": "text"}]),  # å­—ç¬¦ä¸² role + list[dict] content
])
```

#### 3.3.4 æ›´ä¸°å¯Œçš„å‚æ•°ç±»å‹

å‰ä¸¤ç§åˆ›å»ºæ–¹å¼ï¼Œæˆ‘ä»¬çœ‹åˆ°ä¸ç®¡ä½¿ç”¨æ„é€ æ–¹æ³•ï¼Œè¿˜æ˜¯ä½¿ç”¨from_messages()ï¼Œå‚æ•°ç±»å‹éƒ½æ˜¯åˆ—è¡¨ç±»å‹ã€‚åˆ—è¡¨ä¸­çš„å…ƒç´ å¯ä»¥æ˜¯å¤šç§ç±»å‹ï¼Œå‰é¢æˆ‘ä»¬ä¸»è¦æµ‹è¯•äº†å…ƒç»„ç±»å‹ã€‚

æºç ï¼š

```python
def __init__(self,
             messages: Sequence[BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str | dict[str, Any]],
             *,
             template_format: Literal["f-string", "mustache", "jinja2"] = "f-string",
             **kwargs: Any) -> None
```

æºç ï¼š

```python
@classmethod def from_messages(cls,
                  messages: Sequence[BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str | dict[str, Any]],
                  template_format: Literal["f-string", "mustache", "jinja2"] = "f-string")
  -> ChatPromptTemplate
```

ä¸¾ä¾‹1ï¼šåˆ—è¡¨å‚æ•°æ ¼å¼æ˜¯strç±»å‹ï¼ˆä¸æ¨èï¼‰ï¼Œ**å› ä¸ºé»˜è®¤è§’è‰²éƒ½æ˜¯human**

```python
#1.å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage,HumanMessage, AIMessage
# 2.å®šä¹‰èŠå¤©æç¤ºè¯æ¨¡ç‰ˆ
chat_template = ChatPromptTemplate.from_messages(
  [
      
    "Hello, {name}!"  # ç­‰ä»·äº ("human", "Hello, {name}!")
    ]
)

# 3.1æ ¼å¼åŒ–èŠå¤©æç¤ºè¯æ¨¡ç‰ˆä¸­çš„å˜é‡(è‡ªå·±æä¾›çš„)
messages = chat_template.format_messages(name="å°è°·AI")
# 3.2 ä½¿ç”¨invokeæ‰§è¡Œ
# messages=chat_template.invoke({"name":"å°è°·AI"})

# 4.æ‰“å°æ ¼å¼åŒ–åçš„èŠå¤©æç¤ºè¯æ¨¡ç‰ˆå†…å®¹
print(messages)
```

ä¸¾ä¾‹2ï¼šåˆ—è¡¨å‚æ•°æ ¼å¼æ˜¯dictç±»å‹

```python
# ç¤ºä¾‹: å­—å…¸å½¢å¼çš„æ¶ˆæ¯
prompt = ChatPromptTemplate.from_messages([
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ª{role}."},
    {"role": "human", "content": ["å¤æ‚å†…å®¹", {"type": "text"}]},
])

print(prompt.format_messages(role="æ•™å¸ˆ"))
```

ä¸¾ä¾‹3ï¼šMessageç±»å‹

```python
from langchain_core.messages import SystemMessage,HumanMessage


chat_prompt_template = ChatPromptTemplate.from_messages([
    SystemMessage(content="æˆ‘æ˜¯ä¸€ä¸ªè´´å¿ƒçš„æ™ºèƒ½åŠ©æ‰‹"),
    HumanMessage(content="æˆ‘çš„é—®é¢˜æ˜¯:äººå·¥æ™ºèƒ½è‹±æ–‡æ€ä¹ˆè¯´ï¼Ÿ")

])

messages = chat_prompt_template.format_messages()
print(messages)
print(type(messages))
```

> [SystemMessage(content='æˆ‘æ˜¯ä¸€ä¸ªè´´å¿ƒçš„æ™ºèƒ½åŠ©æ‰‹', additional_kwargs={}, response_metadata={}), HumanMessage(content='æˆ‘çš„é—®é¢˜æ˜¯:äººå·¥æ™ºèƒ½è‹±æ–‡æ€ä¹ˆè¯´ï¼Ÿ', additional_kwargs={}, response_metadata={})]
> <class 'list'>

ä¸¾ä¾‹4ï¼šä½¿ç”¨ BaseChatPromptTemplateï¼ˆåµŒå¥—çš„ ChatPromptTemplateï¼‰

```python
from langchain_core.prompts import ChatPromptTemplate

# ä½¿ç”¨ BaseChatPromptTemplateï¼ˆåµŒå¥—çš„ ChatPromptTemplateï¼‰
nested_prompt_template1 = ChatPromptTemplate.from_messages([("system", "åµŒå¥—æç¤ºè¯ä¹‹System")])
nested_prompt_template2 = ChatPromptTemplate.from_messages([("human", "åµŒå¥—æç¤ºè¯ä¹‹Human")])

prompt_template = ChatPromptTemplate.from_messages([
    nested_prompt_template1,nested_prompt_template2
])

prompt_template.format_messages()
```

> [SystemMessage(content='åµŒå¥—æç¤ºè¯ä¹‹System', additional_kwargs={}, response_metadata={}),
>  HumanMessage(content='åµŒå¥—æç¤ºè¯ä¹‹Human', additional_kwargs={}, response_metadata={})]

ä¸¾ä¾‹5ï¼šä½¿ç”¨XxxMessagePromptTemplateï¼Œè§ä¸‹ä¸€èŠ‚

#### 3.3.5 å…³äºXxxMessagePromptTemplateçš„ä½¿ç”¨

LangChainæä¾›ä¸åŒç±»å‹çš„MessagePromptTemplateã€‚æœ€å¸¸ç”¨çš„æ˜¯`SystemMessagePromptTemplate`ã€`HumanMessagePromptTemplate`å’Œ`AIMessagePromptTemplate`ï¼Œåˆ†åˆ«åˆ›å»ºç³»ç»Ÿæ¶ˆæ¯ã€äººå·¥æ¶ˆæ¯å’ŒAIæ¶ˆæ¯ï¼Œå®ƒä»¬æ˜¯ChatMessagePromptTemplateçš„ç‰¹å®šè§’è‰²å­ç±»ã€‚



**åŸºæœ¬æ¦‚å¿µï¼š**

**ChatMessagePromptTemplate**ï¼Œç”¨äºæ„å»ºèŠå¤©æ¶ˆæ¯çš„æ¨¡æ¿ã€‚å®ƒå…è®¸ä½ åˆ›å»ºå¯é‡ç”¨çš„æ¶ˆæ¯æ¨¡æ¿ï¼Œè¿™äº›æ¨¡æ¿å¯ä»¥åŠ¨æ€åœ°æ’å…¥å˜é‡å€¼æ¥ç”Ÿæˆæœ€ç»ˆçš„èŠå¤©æ¶ˆæ¯

- `è§’è‰²æŒ‡å®š`ï¼šå¯ä»¥ä¸ºæ¯æ¡æ¶ˆæ¯æŒ‡å®šè§’è‰²ï¼ˆå¦‚ "system"ã€"human"ã€"ai"ï¼‰ ç­‰ï¼Œè§’è‰²çµæ´»ã€‚
- `æ¨¡æ¿åŒ–`ï¼šæ”¯æŒä½¿ç”¨å˜é‡å ä½ç¬¦ï¼Œå¯ä»¥åœ¨è¿è¡Œæ—¶å¡«å……å…·ä½“å€¼
- `æ ¼å¼åŒ–`ï¼šèƒ½å¤Ÿå°†æ¨¡æ¿ä¸è¾“å…¥å˜é‡ç»“åˆç”Ÿæˆæœ€ç»ˆçš„èŠå¤©æ¶ˆæ¯

**HumanMessagePromptTemplate**ï¼Œä¸“ç”¨äºç”Ÿæˆ`ç”¨æˆ·æ¶ˆæ¯ï¼ˆHumanMessageï¼‰`çš„æ¨¡æ¿ç±»ï¼Œæ˜¯ ChatMessagePromptTemplateçš„ç‰¹å®šè§’è‰²å­ç±»ã€‚

- `æœ¬è´¨`ï¼šé¢„å®šä¹‰äº† `role="human"` çš„ ChatMessagePromptTemplate å¿«æ·æ–¹å¼ï¼Œä¸”æ— éœ€æ— éœ€æ‰‹åŠ¨æŒ‡å®šè§’è‰²
- `æ¨¡æ¿åŒ–`ï¼šæ”¯æŒä½¿ç”¨å˜é‡å ä½ç¬¦ï¼Œå¯ä»¥åœ¨è¿è¡Œæ—¶å¡«å……å…·ä½“å€¼
- `æ ¼å¼åŒ–`ï¼šèƒ½å¤Ÿå°†æ¨¡æ¿ä¸è¾“å…¥å˜é‡ç»“åˆç”Ÿæˆæœ€ç»ˆçš„èŠå¤©æ¶ˆæ¯
- `è¾“å‡ºç±»å‹`ï¼šç”Ÿæˆ `HumanMessage` å¯¹è±¡ï¼ˆ`content` + `role="human"`ï¼‰
- `è®¾è®¡ç›®çš„` ï¼šç®€åŒ–ç”¨æˆ·è¾“å…¥æ¶ˆæ¯çš„æ¨¡æ¿åŒ–æ„é€ ï¼Œé¿å…é‡å¤å®šä¹‰è§’è‰²



ä¸¾ä¾‹1ï¼šChatMessagePromptTemplateçš„ç†è§£

```python
# 1.å¯¼å…¥å…ˆå…³åŒ…
from langchain_core.prompts import ChatMessagePromptTemplate

# 2.å®šä¹‰æ¨¡ç‰ˆ
prompt = "æ„¿{subject}ä¸ä½ åŒåœ¨ "

# 3.åˆ›å»ºè‡ªå®šä¹‰è§’è‰²èŠå¤©æ¶ˆæ¯æç¤ºè¯æ¨¡ç‰ˆ
chat_message_prompt = ChatMessagePromptTemplate.from_template(
    role="ç»åœ°æ­¦å£«", template=prompt
)
# 4.æ ¼å¼èŠå¤©æ¶ˆæ¯æç¤ºè¯
resp = chat_message_prompt.format(subject="åŠ›")
print(type(resp))
print(resp)
```

> <class 'langchain_core.messages.chat.ChatMessage'>
>content='æ„¿åŠ›ä¸ä½ åŒåœ¨ ' additional_kwargs={} response_metadata={} role='ç»åœ°æ­¦å£«'



ä¸¾ä¾‹2ï¼š

```python
# å¯¼å…¥èŠå¤©æ¶ˆæ¯ç±»æ¨¡æ¿
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate

# åˆ›å»ºæ¶ˆæ¯æ¨¡æ¿
system_template = "ä½ æ˜¯ä¸€ä¸ªä¸“å®¶{role}"
system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)

human_template = "ç»™æˆ‘è§£é‡Š{concept}ï¼Œç”¨æµ…æ˜¾æ˜“æ‡‚çš„è¯­è¨€"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

# ç»„åˆæˆèŠå¤©æç¤ºæ¨¡æ¿
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# æ ¼å¼åŒ–æç¤º
formatted_messages = chat_prompt.format_messages(
    role="ç‰©ç†å­¦å®¶",
    concept="ç›¸å¯¹è®º"
)
print(formatted_messages)
```

> [SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªä¸“å®¶ç‰©ç†å­¦å®¶', additional_kwargs={}, response_metadata={}), HumanMessage(content='ç»™æˆ‘è§£é‡Šç›¸å¯¹è®ºï¼Œç”¨æµ…æ˜¾æ˜“æ‡‚çš„è¯­è¨€', additional_kwargs={}, response_metadata={})]
> [SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªä¸“å®¶ç‰©ç†å­¦å®¶', additional_kwargs={}, response_metadata={}), HumanMessage(content='ç»™æˆ‘è§£é‡Šç›¸å¯¹è®ºï¼Œç”¨æµ…æ˜¾æ˜“æ‡‚çš„è¯­è¨€', additional_kwargs={}, response_metadata={})]

ä¸¾ä¾‹3ï¼šç»¼åˆä½¿ç”¨

```python
from langchain_core.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_core.messages import SystemMessage, HumanMessage

# ç¤ºä¾‹ 1: ä½¿ç”¨ BaseMessagePromptTemplate
system_prompt = SystemMessagePromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ª{role}.")
human_prompt = HumanMessagePromptTemplate.from_template("{user_input}")

# ç¤ºä¾‹ 2: ä½¿ç”¨ BaseMessageï¼ˆå·²å®ä¾‹åŒ–çš„æ¶ˆæ¯ï¼‰
system_msg = SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªAIå·¥ç¨‹å¸ˆã€‚")
human_msg = HumanMessage(content="ä½ å¥½ï¼")

# ç¤ºä¾‹ 3: ä½¿ç”¨ BaseChatPromptTemplateï¼ˆåµŒå¥—çš„ ChatPromptTemplateï¼‰
nested_prompt = ChatPromptTemplate.from_messages([("system", "åµŒå¥—æç¤ºè¯")])

prompt = ChatPromptTemplate.from_messages([
    system_prompt,  # MessageLike (BaseMessagePromptTemplate)
    human_msg,      # MessageLike (BaseMessage)
    nested_prompt,  # MessageLike (BaseChatPromptTemplate)
])
```

ç±»ä¼¼çš„ï¼š

```python
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate
from langchain_core.prompts import ChatPromptTemplate

chat_template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ {name}."),
        HumanMessage(content=("ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?")),
        AIMessage(content=("æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰.")),
        HumanMessagePromptTemplate.from_template("{input}")
    ]
)
messages = chat_template.format_messages(input="ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?", name="å°è°·AI")
print(messages)
```

> [SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ å°è°·AI.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?', additional_kwargs={}, response_metadata={}), AIMessage(content='æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?', additional_kwargs={}, response_metadata={})]



#### 3.3.6 format_messages()çš„å‡ ç§æ›¿ä»£æ–¹å¼

å‰æï¼šä½¿ç”¨format_messages()

```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate([
    ("system", "ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ {name}."),
    ("human", "ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?"),
    ("ai", "æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰."),
    ("human", "{user_input}")
])

#æ–¹å¼2ï¼šè°ƒç”¨format_messages()æ–¹æ³•ï¼Œè¿”å›æ¶ˆæ¯åˆ—è¡¨
prompt2 = prompt_template.format_messages(name="å°è°·AI", user_input="ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?")
print(type(prompt2))
print(prompt2)
```

> <class 'list'>
> [SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ å°è°·AI.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?', additional_kwargs={}, response_metadata={}), AIMessage(content='æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?', additional_kwargs={}, response_metadata={})]

æ–¹å¼1ï¼šä½¿ç”¨format()ï¼šå‰é¢å·²ç»è®²è¿‡

```python
from langchain_core.prompts import ChatPromptTemplate

#å‚æ•°ç±»å‹è¿™é‡Œä½¿ç”¨çš„æ˜¯tupleæ„æˆçš„list
prompt_template = ChatPromptTemplate([
    # å­—ç¬¦ä¸² role + å­—ç¬¦ä¸² content
    ("system", "ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ {name}."),
    ("human", "ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?"),
    ("ai", "æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰."),
    ("human", "{user_input}")
])

#æ–¹å¼1ï¼šè°ƒç”¨format()æ–¹æ³•ï¼Œè¿”å›å­—ç¬¦ä¸²
prompt = prompt_template.format(name="å°è°·AI", user_input="ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?")
print(type(prompt))
print(prompt)
```

> <class 'str'>
>
> System: ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ å°è°·AI.
> Human: ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?
> AI: æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰.
> Human: ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?

æ–¹å¼2ï¼šä½¿ç”¨ invoke()

```python
from langchain_core.prompts import ChatPromptTemplate

#å‚æ•°ç±»å‹è¿™é‡Œä½¿ç”¨çš„æ˜¯tupleæ„æˆçš„list
prompt_template = ChatPromptTemplate([
    # å­—ç¬¦ä¸² role + å­—ç¬¦ä¸² content
    ("system", "ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ {name}."),
    ("human", "ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?"),
    ("ai", "æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰."),
    ("human", "{user_input}")
])

prompt = prompt_template.invoke({"name":"å°è°·AI", "user_input":"ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?"})
print(type(prompt))
print(prompt)
print(len(prompt.messages))
```

> <class 'langchain_core.prompt_values.ChatPromptValue'>
> messages=[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ å°è°·AI.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?', additional_kwargs={}, response_metadata={}), AIMessage(content='æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?', additional_kwargs={}, response_metadata={})]
> 4

æ–¹å¼3ï¼šä½¿ç”¨format_prompt()

```python
from langchain_core.prompts import ChatPromptTemplate

#å‚æ•°ç±»å‹è¿™é‡Œä½¿ç”¨çš„æ˜¯tupleæ„æˆçš„list
prompt_template = ChatPromptTemplate([
    # å­—ç¬¦ä¸² role + å­—ç¬¦ä¸² content
    ("system", "ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ {name}."),
    ("human", "ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?"),
    ("ai", "æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰."),
    ("human", "{user_input}")
])

prompt = prompt_template.format_prompt(name="å°è°·AI", user_input="ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?")
print(prompt.to_messages())
print(type(prompt.to_messages()))

```

> [SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªAIå¼€å‘å·¥ç¨‹å¸ˆ. ä½ çš„åå­—æ˜¯ å°è°·AI.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ èƒ½å¼€å‘å“ªäº›AIåº”ç”¨?', additional_kwargs={}, response_metadata={}), AIMessage(content='æˆ‘èƒ½å¼€å‘å¾ˆå¤šAIåº”ç”¨, æ¯”å¦‚èŠå¤©æœºå™¨äºº, å›¾åƒè¯†åˆ«, è‡ªç„¶è¯­è¨€å¤„ç†ç­‰.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆ?', additional_kwargs={}, response_metadata={})]
> <class 'list'>

#### 3.3.7 ç»“åˆLLM

ä¸¾ä¾‹1ï¼š


```python
from langchain.prompts.chat import ChatPromptTemplate

######1ã€æä¾›æç¤ºè¯#########
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæ•°å­¦å®¶ï¼Œä½ å¯ä»¥è®¡ç®—ä»»ä½•ç®—å¼"),
    ("human", "{text}"),
])


# è¾“å…¥æç¤º
messages = chat_prompt.format_messages(text="æˆ‘ä»Šå¹´18å²ï¼Œæˆ‘çš„èˆ…èˆ…ä»Šå¹´38å²ï¼Œæˆ‘çš„çˆ·çˆ·ä»Šå¹´72å²ï¼Œæˆ‘å’Œèˆ…èˆ…ä¸€å…±å¤šå°‘å²äº†ï¼Ÿ")
#print(messages)

######2ã€æä¾›å¤§æ¨¡å‹#########
import os
import dotenv
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

chat_model = ChatOpenAI(model="gpt-4o-mini")

######3ã€ç»“åˆæç¤ºè¯ï¼Œè°ƒç”¨å¤§æ¨¡å‹#########

# å¾—åˆ°æ¨¡å‹çš„è¾“å‡º
output = chat_model.invoke(messages)
# æ‰“å°è¾“å‡ºå†…å®¹
print(output.content)
```

> ä½ ä»Šå¹´18å²ï¼Œä½ çš„èˆ…èˆ…ä»Šå¹´38å²ã€‚é‚£ä¹ˆä½ å’Œèˆ…èˆ…çš„å¹´é¾„æ€»å’Œæ˜¯ï¼š
>
> 18 + 38 = 56
>
> æ‰€ä»¥ä½ å’Œèˆ…èˆ…ä¸€å…±56å²ã€‚

ä¸¾ä¾‹2ï¼š

```python
from dotenv import load_dotenv
from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

load_dotenv()
llm = ChatOpenAI()

template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template("ä½ æ˜¯{product}çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«{name}"),
        HumanMessagePromptTemplate.from_template("hello ä½ å¥½å—ï¼Ÿ"),
        AIMessagePromptTemplate.from_template("æˆ‘å¾ˆå¥½ è°¢è°¢!"),
        HumanMessagePromptTemplate.from_template("{query}"),
    ]
)

prompt = template.format_messages(
    product="AGIè¯¾å ‚",
    name="Bob",
    query="ä½ æ˜¯è°"
)

# æä¾›èŠå¤©æ¨¡å‹
import os
import dotenv
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

chat_model = ChatOpenAI(model="gpt-4o-mini")

# è°ƒç”¨èŠå¤©æ¨¡å‹
response = chat_model.invoke(prompt)
print(response.content)

```

> æˆ‘æ˜¯Bobï¼ŒAGIè¯¾å ‚çš„å®¢æœåŠ©æ‰‹ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ



####  3.3.8 æ’å…¥æ¶ˆæ¯åˆ—è¡¨ï¼šMessagesPlaceholder 

å½“ä½ ä¸ç¡®å®šæ¶ˆæ¯æç¤ºæ¨¡æ¿ä½¿ç”¨ä»€ä¹ˆè§’è‰²ï¼Œæˆ–è€…å¸Œæœ›åœ¨æ ¼å¼åŒ–è¿‡ç¨‹ä¸­`æ’å…¥æ¶ˆæ¯åˆ—è¡¨`æ—¶ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ è¿™å°±éœ€è¦ä½¿ç”¨ MessagesPlaceholderï¼Œè´Ÿè´£åœ¨ç‰¹å®šä½ç½®æ·»åŠ æ¶ˆæ¯åˆ—è¡¨ã€‚

ä½¿ç”¨åœºæ™¯ï¼šå¤šè½®å¯¹è¯ç³»ç»Ÿå­˜å‚¨å†å²æ¶ˆæ¯ä»¥åŠAgent çš„ä¸­é—´æ­¥éª¤å¤„ç†æ­¤åŠŸèƒ½éå¸¸æœ‰ç”¨ã€‚

ä¸¾ä¾‹1ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder("msgs")
])
# prompt_template.invoke({"msgs": [HumanMessage(content="hi!")]})

prompt_template.format_messages(msgs=[HumanMessage(content="hi!")])
```

> [SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}),
>  HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]

è¿™å°†ç”Ÿæˆä¸¤æ¡æ¶ˆæ¯ï¼Œç¬¬ä¸€æ¡æ˜¯ç³»ç»Ÿæ¶ˆæ¯ï¼Œç¬¬äºŒæ¡æ˜¯æˆ‘ä»¬ä¼ å…¥çš„ HumanMessageã€‚ å¦‚æœæˆ‘ä»¬ä¼ å…¥äº† 5 æ¡æ¶ˆæ¯ï¼Œé‚£ä¹ˆæ€»å…±ä¼šç”Ÿæˆ 6 æ¡æ¶ˆæ¯ï¼ˆç³»ç»Ÿæ¶ˆæ¯åŠ ä¸Šä¼ å…¥çš„ 5 æ¡æ¶ˆæ¯ï¼‰ã€‚ è¿™å¯¹äºå°†ä¸€ç³»åˆ—æ¶ˆæ¯æ’å…¥åˆ°ç‰¹å®šä½ç½®éå¸¸æœ‰ç”¨ã€‚ 

ä¸¾ä¾‹2ï¼šå­˜å‚¨å¯¹è¯å†å²å†…å®¹

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        MessagesPlaceholder("history"),
        ("human", "{question}")
    ]
)
prompt.invoke(
    {
        "history": [("human", "what's 5 + 2"), ("ai", "5 + 2 is 7")],
        "question": "now multiply that by 4"
    }
)
```

ä¸¾ä¾‹3ï¼š

```python
#1.å¯¼å…¥ç›¸å…³åŒ…
from langchain_core.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder)

# 2.å®šä¹‰æ¶ˆæ¯æ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template("ä½ æ˜¯{role}"),
    MessagesPlaceholder(variable_name="intermediate_steps"),
    HumanMessagePromptTemplate.from_template("{query}")
])

# 3.å®šä¹‰æ¶ˆæ¯å¯¹è±¡ï¼ˆè¿è¡Œæ—¶å¡«å……ä¸­é—´æ­¥éª¤çš„ç»“æœï¼‰
intermediate = [
    SystemMessage(name="search", content="åŒ—äº¬: æ™´, 25â„ƒ")
]
# 4.æ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯æç¤ºè¯æ¨¡ç‰ˆ
prompt.format_messages(
    role="å¤©æ°”é¢„æŠ¥å‘˜",
    intermediate_steps=intermediate,
    query="åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
)
```

> [SystemMessage(content='ä½ æ˜¯å¤©æ°”é¢„æŠ¥å‘˜', additional_kwargs={}, response_metadata={}),
> SystemMessage(content='åŒ—äº¬: æ™´, 25â„ƒ', additional_kwargs={}, response_metadata={}, name='search'),
> HumanMessage(content='åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ', additional_kwargs={}, response_metadata={})]

ä¸¾ä¾‹4ï¼š

```python
# 1.å¯¼å…¥ç›¸å…³åŒ…
from langchain_core.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder)
from langchain_core.messages import AIMessage, HumanMessage

# 2ï¼Œå®šä¹‰HumanMessageå¯¹è±¡
human_message = HumanMessage(content="å­¦ä¹ ç¼–ç¨‹çš„æœ€å¥½æ–¹æ³•æ˜¯ä»€ä¹ˆ?")
# 3.å®šä¹‰AIMessageå¯¹è±¡
ai_message = AIMessage(
    content="""\
1. é€‰æ‹©ä¸€é—¨ç¼–ç¨‹è¯­è¨€ï¼šé€‰æ‹©ä¸€é—¨ä½ æƒ³å­¦ä¹ çš„ç¼–ç¨‹è¯­è¨€.

2.ä»åŸºç¡€å¼€å§‹ï¼šç†Ÿæ‚‰åŸºæœ¬çš„ç¼–ç¨‹æ¦‚å¿µï¼Œå¦‚å˜é‡ã€æ•°æ®ç±»å‹å’Œæ§åˆ¶ç»“æ„.

3. ç»ƒä¹ ï¼Œç»ƒä¹ ï¼Œå†ç»ƒä¹ ï¼šå­¦ä¹ ç¼–ç¨‹çš„æœ€å¥½æ–¹æ³•æ˜¯é€šè¿‡å®è·µç»éªŒ\
"""
)

# 4. å®šä¹‰æç¤ºè¯
human_prompt = "ç”¨{word_count}ä¸ªè¯æ€»ç»“æˆ‘ä»¬åˆ°ç›®å‰ä¸ºæ­¢çš„å¯¹è¯"

# 5. å®šä¹‰æç¤ºè¯æ¨¡ç‰ˆ
human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)

chat_prompt = ChatPromptTemplate.from_messages(
    [
        MessagesPlaceholder(variable_name="conversation"),
        human_message_template
    ]
)
# 6.æ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯æç¤ºè¯æ¨¡ç‰ˆ
messages1 = chat_prompt.format_messages(
    conversation=[human_message, ai_message], word_count="10"
)
print(messages1)
```

> [HumanMessage(content='å­¦ä¹ ç¼–ç¨‹çš„æœ€å¥½æ–¹æ³•æ˜¯ä»€ä¹ˆ?', additional_kwargs={}, response_metadata={}), AIMessage(content='1. é€‰æ‹©ä¸€é—¨ç¼–ç¨‹è¯­è¨€ï¼šé€‰æ‹©ä¸€é—¨ä½ æƒ³å­¦ä¹ çš„ç¼–ç¨‹è¯­è¨€.\n\n2.ä»åŸºç¡€å¼€å§‹ï¼šç†Ÿæ‚‰åŸºæœ¬çš„ç¼–ç¨‹æ¦‚å¿µï¼Œå¦‚å˜é‡ã€æ•°æ®ç±»å‹å’Œæ§åˆ¶ç»“æ„.\n\n3. ç»ƒä¹ ï¼Œç»ƒä¹ ï¼Œå†ç»ƒä¹ ï¼šå­¦ä¹ ç¼–ç¨‹çš„æœ€å¥½æ–¹æ³•æ˜¯é€šè¿‡å®è·µç»éªŒ', additional_kwargs={}, response_metadata={}), HumanMessage(content='ç”¨10ä¸ªè¯æ€»ç»“æˆ‘ä»¬åˆ°ç›®å‰ä¸ºæ­¢çš„å¯¹è¯', additional_kwargs={}, response_metadata={})]



### 3.4 å…·ä½“ä½¿ç”¨ï¼šå°‘é‡æ ·æœ¬ç¤ºä¾‹çš„æç¤ºæ¨¡æ¿

#### 3.4.1 ä½¿ç”¨è¯´æ˜

åœ¨æ„å»ºpromptæ—¶ï¼Œå¯ä»¥é€šè¿‡æ„å»ºä¸€ä¸ª`å°‘é‡ç¤ºä¾‹åˆ—è¡¨`å»è¿›ä¸€æ­¥æ ¼å¼åŒ–promptï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä½†å¼ºå¤§çš„æŒ‡å¯¼ç”Ÿæˆçš„æ–¹å¼ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚

æ¯ä¸ªç¤ºä¾‹çš„ç»“æ„éƒ½æ˜¯ä¸€ä¸ª`å­—å…¸`ï¼Œå…¶ä¸­`é”®`æ˜¯è¾“å…¥å˜é‡ï¼Œ`å€¼`æ˜¯è¾“å…¥å˜é‡çš„å€¼ã€‚

åŸºäºLLMæ¨¡å‹ä¸èŠå¤©æ¨¡å‹ï¼Œå¯åˆ†åˆ«ä½¿ç”¨`FewShotPromptTemplate`æˆ–`FewShotChatMessagePromptTemplate`ï¼Œä¸¤è€…ä½¿ç”¨åŸºæœ¬ä¸€è‡´ã€‚

å°‘é‡ç¤ºä¾‹æç¤ºæ¨¡æ¿å¯ä»¥ç”±`ä¸€ç»„ç¤ºä¾‹`æˆ–ä¸€ä¸ªè´Ÿè´£ä»å®šä¹‰çš„é›†åˆä¸­é€‰æ‹©ä¸€éƒ¨åˆ†ç¤ºä¾‹çš„`ç¤ºä¾‹é€‰æ‹©å™¨`æ„å»ºã€‚



**ä½“ä¼šï¼š**zeroshotä¼šå¯¼è‡´ä½è´¨é‡å›ç­”

```python

from langchain_openai import ChatOpenAI

import os
import dotenv
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

chat_model = ChatOpenAI(model="gpt-4o-mini",
                        temperature=0.4)

res = chat_model.invoke("2 ğŸ¦œ 9æ˜¯å¤šå°‘?")
print(res.content)
```

> 2 ğŸ¦œ 9 çš„è®¡ç®—æ–¹å¼å–å†³äºä½ æ‰€ç”¨çš„ç¬¦å·â€œğŸ¦œâ€çš„å«ä¹‰ã€‚è¯·æä¾›æ›´å¤šä¿¡æ¯æˆ–è€…è¯´æ˜è¿™ä¸ªç¬¦å·ä»£è¡¨ä»€ä¹ˆè¿ç®—ã€‚

#### 3.4.2 FewShotPromptTemplateçš„ä½¿ç”¨

**ä¸¾ä¾‹1ï¼š**è¿™é‡Œä»¥FewShotPromptTemplateä½œä¸ºæç¤ºæ¨¡æ¿è®²è§£

ç¬¬1æ­¥ï¼šåˆ›å»ºPromptTemplateå¯¹è±¡

é…ç½®ä¸€ä¸ªæ ¼å¼åŒ–ç¨‹åºï¼Œå°†å°‘é‡ç¤ºä¾‹æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ã€‚è¿™ä¸ªæ ¼å¼åŒ–ç¨‹åºåº”è¯¥æ˜¯ä¸€ä¸ªPromptTemplateå¯¹è±¡ã€‚

```python
from langchain_core.prompts import PromptTemplate

#æ–¹å¼1
#example_prompt = PromptTemplate.from_template("é—®é¢˜ï¼š{question}\n{answer}")

#æ–¹å¼2
example_prompt = PromptTemplate(
    template="é—®é¢˜:{question}\n{answer}",
    input_variables=["question", "answer"], 
)
```

ç¬¬2æ­¥ï¼šåˆ›å»ºç¤ºä¾‹é›†åˆ

```python
examples = [
    {
        "question": "è°æ´»å¾—æ›´é•¿ï¼Œç©†ç½•é»˜å¾·Â·é˜¿é‡Œè¿˜æ˜¯è‰¾ä¼¦Â·å›¾çµï¼Ÿ",
        "answer": """
æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯çš„ã€‚
åç»­é—®é¢˜ï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œå»ä¸–æ—¶å¤šå¤§å¹´çºªï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œå»ä¸–æ—¶74å²ã€‚
åç»­é—®é¢˜ï¼šè‰¾ä¼¦Â·å›¾çµå»ä¸–æ—¶å¤šå¤§å¹´çºªï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šè‰¾ä¼¦Â·å›¾çµå»ä¸–æ—¶41å²ã€‚
æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œ
""",
    },
    {
        "question": "å…‹é›·æ ¼æ–¯åˆ—è¡¨çš„åˆ›å§‹äººæ˜¯ä»€ä¹ˆæ—¶å€™å‡ºç”Ÿçš„ï¼Ÿ",
        "answer": """
æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯çš„ã€‚
åç»­é—®é¢˜ï¼šå…‹é›·æ ¼æ–¯åˆ—è¡¨çš„åˆ›å§‹äººæ˜¯è°ï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šå…‹é›·æ ¼æ–¯åˆ—è¡¨çš„åˆ›å§‹äººæ˜¯å…‹é›·æ ¼Â·çº½é©¬å…‹ã€‚
åç»­é—®é¢˜ï¼šå…‹é›·æ ¼Â·çº½é©¬å…‹æ˜¯ä»€ä¹ˆæ—¶å€™å‡ºç”Ÿçš„ï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šå…‹é›·æ ¼Â·çº½é©¬å…‹äº1952å¹´12æœˆ6æ—¥å‡ºç”Ÿã€‚
æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼š1952å¹´12æœˆ6æ—¥
""",
    },
    {
        "question": "ä¹”æ²»Â·åç››é¡¿çš„å¤–ç¥–çˆ¶æ˜¯è°ï¼Ÿ",
        "answer": """
æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯çš„ã€‚
åç»­é—®é¢˜ï¼šä¹”æ²»Â·åç››é¡¿çš„æ¯äº²æ˜¯è°ï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šä¹”æ²»Â·åç››é¡¿çš„æ¯äº²æ˜¯ç›ä¸½Â·æ³¢å°”Â·åç››é¡¿ã€‚
åç»­é—®é¢˜ï¼šç›ä¸½Â·æ³¢å°”Â·åç››é¡¿çš„çˆ¶äº²æ˜¯è°ï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šç›ä¸½Â·æ³¢å°”Â·åç››é¡¿çš„çˆ¶äº²æ˜¯çº¦ç‘Ÿå¤«Â·æ³¢å°”ã€‚
æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šçº¦ç‘Ÿå¤«Â·æ³¢å°”
""",
    },
    {
        "question": "ã€Šå¤§ç™½é²¨ã€‹å’Œã€Šçš‡å®¶èµŒåœºã€‹çš„å¯¼æ¼”éƒ½æ¥è‡ªåŒä¸€ä¸ªå›½å®¶å—ï¼Ÿ",
        "answer": """
æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯çš„ã€‚
åç»­é—®é¢˜ï¼šã€Šå¤§ç™½é²¨ã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šã€Šå¤§ç™½é²¨ã€‹çš„å¯¼æ¼”æ˜¯å²è’‚æ–‡Â·æ–¯çš®å°”ä¼¯æ ¼ã€‚
åç»­é—®é¢˜ï¼šå²è’‚æ–‡Â·æ–¯çš®å°”ä¼¯æ ¼æ¥è‡ªå“ªä¸ªå›½å®¶ï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šç¾å›½ã€‚
åç»­é—®é¢˜ï¼šã€Šçš‡å®¶èµŒåœºã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šã€Šçš‡å®¶èµŒåœºã€‹çš„å¯¼æ¼”æ˜¯é©¬ä¸Â·åè´å°”ã€‚
åç»­é—®é¢˜ï¼šé©¬ä¸Â·åè´å°”æ¥è‡ªå“ªä¸ªå›½å®¶ï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šæ–°è¥¿å…°ã€‚
æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šä¸æ˜¯
""",
    },
]
```

æµ‹è¯•ï¼š

```python
print(example_prompt.invoke(examples[0]).to_string())
```

```
é—®é¢˜ï¼šè°æ´»å¾—æ›´é•¿ï¼Œç©†ç½•é»˜å¾·Â·é˜¿é‡Œè¿˜æ˜¯è‰¾ä¼¦Â·å›¾çµï¼Ÿ

æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯çš„ã€‚
åç»­é—®é¢˜ï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œå»ä¸–æ—¶å¤šå¤§å¹´çºªï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œå»ä¸–æ—¶74å²ã€‚
åç»­é—®é¢˜ï¼šè‰¾ä¼¦Â·å›¾çµå»ä¸–æ—¶å¤šå¤§å¹´çºªï¼Ÿ
ä¸­é—´ç­”æ¡ˆï¼šè‰¾ä¼¦Â·å›¾çµå»ä¸–æ—¶41å²ã€‚
æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œ
```

ç¬¬3æ­¥ï¼šå°†ç¤ºä¾‹å’Œæ ¼å¼åŒ–ç¨‹åºä¼ é€’ç»™FewShotPromptTemplate

å½“æ ¼å¼åŒ–æ­¤FewShotPromptTemplate æ—¶ï¼Œå®ƒä½¿ç”¨example_promptæ ¼å¼åŒ–ä¼ é€’çš„ç¤ºä¾‹ï¼Œç„¶åå°†å®ƒä»¬æ·»åŠ åˆ°`suffix`ä¹‹å‰çš„æœ€ç»ˆæç¤ºä¸­ï¼š

```python
from langchain_core.prompts import FewShotPromptTemplate

prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    suffix="é—®é¢˜ï¼š{input}",
    input_variables=["input"],
)

print(prompt.invoke({"input": "ä¹”æ²»Â·åç››é¡¿çš„çˆ¶äº²æ˜¯è°ï¼Ÿ"}).to_string())
# æˆ–è€…
# print(prompt.format(input="ç›ä¸½Â·é²å°”Â·åç››é¡¿çš„çˆ¶äº²æ˜¯è°ï¼Ÿ"))
```

> é—®é¢˜:è°æ´»å¾—æ›´é•¿ï¼Œç©†ç½•é»˜å¾·Â·é˜¿é‡Œè¿˜æ˜¯è‰¾ä¼¦Â·å›¾çµï¼Ÿ
>
> æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯çš„ã€‚
> åç»­é—®é¢˜ï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œå»ä¸–æ—¶å¤šå¤§å¹´çºªï¼Ÿ
> ä¸­é—´ç­”æ¡ˆï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œå»ä¸–æ—¶74å²ã€‚
> åç»­é—®é¢˜ï¼šè‰¾ä¼¦Â·å›¾çµå»ä¸–æ—¶å¤šå¤§å¹´çºªï¼Ÿ
> ä¸­é—´ç­”æ¡ˆï¼šè‰¾ä¼¦Â·å›¾çµå»ä¸–æ—¶41å²ã€‚
> æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œ
>
>
> é—®é¢˜:å…‹é›·æ ¼æ–¯åˆ—è¡¨çš„åˆ›å§‹äººæ˜¯ä»€ä¹ˆæ—¶å€™å‡ºç”Ÿçš„ï¼Ÿ
>
> æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯çš„ã€‚
> åç»­é—®é¢˜ï¼šå…‹é›·æ ¼æ–¯åˆ—è¡¨çš„åˆ›å§‹äººæ˜¯è°ï¼Ÿ
> ä¸­é—´ç­”æ¡ˆï¼šå…‹é›·æ ¼æ–¯åˆ—è¡¨çš„åˆ›å§‹äººæ˜¯å…‹é›·æ ¼Â·çº½é©¬å…‹ã€‚
> åç»­é—®é¢˜ï¼šå…‹é›·æ ¼Â·çº½é©¬å…‹æ˜¯ä»€ä¹ˆæ—¶å€™å‡ºç”Ÿçš„ï¼Ÿ
> ä¸­é—´ç­”æ¡ˆï¼šå…‹é›·æ ¼Â·çº½é©¬å…‹äº1952å¹´12æœˆ6æ—¥å‡ºç”Ÿã€‚
> æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼š1952å¹´12æœˆ6æ—¥
>
>
> é—®é¢˜:ä¹”æ²»Â·åç››é¡¿çš„å¤–ç¥–çˆ¶æ˜¯è°ï¼Ÿ
>
> æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯çš„ã€‚
> åç»­é—®é¢˜ï¼šä¹”æ²»Â·åç››é¡¿çš„æ¯äº²æ˜¯è°ï¼Ÿ
> ä¸­é—´ç­”æ¡ˆï¼šä¹”æ²»Â·åç››é¡¿çš„æ¯äº²æ˜¯ç›ä¸½Â·æ³¢å°”Â·åç››é¡¿ã€‚
> åç»­é—®é¢˜ï¼šç›ä¸½Â·æ³¢å°”Â·åç››é¡¿çš„çˆ¶äº²æ˜¯è°ï¼Ÿ
> ä¸­é—´ç­”æ¡ˆï¼šç›ä¸½Â·æ³¢å°”Â·åç››é¡¿çš„çˆ¶äº²æ˜¯çº¦ç‘Ÿå¤«Â·æ³¢å°”ã€‚
> æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šçº¦ç‘Ÿå¤«Â·æ³¢å°”
>
>
> é—®é¢˜:ã€Šå¤§ç™½é²¨ã€‹å’Œã€Šçš‡å®¶èµŒåœºã€‹çš„å¯¼æ¼”éƒ½æ¥è‡ªåŒä¸€ä¸ªå›½å®¶å—ï¼Ÿ
>
> æ˜¯å¦éœ€è¦åç»­é—®é¢˜ï¼šæ˜¯çš„ã€‚
> åç»­é—®é¢˜ï¼šã€Šå¤§ç™½é²¨ã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ
> ä¸­é—´ç­”æ¡ˆï¼šã€Šå¤§ç™½é²¨ã€‹çš„å¯¼æ¼”æ˜¯å²è’‚æ–‡Â·æ–¯çš®å°”ä¼¯æ ¼ã€‚
> åç»­é—®é¢˜ï¼šå²è’‚æ–‡Â·æ–¯çš®å°”ä¼¯æ ¼æ¥è‡ªå“ªä¸ªå›½å®¶ï¼Ÿ
> ä¸­é—´ç­”æ¡ˆï¼šç¾å›½ã€‚
> åç»­é—®é¢˜ï¼šã€Šçš‡å®¶èµŒåœºã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ
> ä¸­é—´ç­”æ¡ˆï¼šã€Šçš‡å®¶èµŒåœºã€‹çš„å¯¼æ¼”æ˜¯é©¬ä¸Â·åè´å°”ã€‚
> åç»­é—®é¢˜ï¼šé©¬ä¸Â·åè´å°”æ¥è‡ªå“ªä¸ªå›½å®¶ï¼Ÿ
> ä¸­é—´ç­”æ¡ˆï¼šæ–°è¥¿å…°ã€‚
> æ‰€ä»¥æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼šä¸æ˜¯
>
>
> é—®é¢˜ï¼šä¹”æ²»Â·åç››é¡¿çš„çˆ¶äº²æ˜¯è°ï¼Ÿ

é€šè¿‡å‘æ¨¡å‹æä¾›è¿™æ ·çš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å¯¼æ¨¡å‹åšå‡ºæ›´å¥½çš„å›åº”ã€‚

**ä¸¾ä¾‹2ï¼š**

```python
from langchain.prompts import PromptTemplate
from langchain.prompts.few_shot import FewShotPromptTemplate

import os
import dotenv
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

chat_model = ChatOpenAI(model="gpt-4o-mini")

# ä¾‹å­
examples = [
    {"input": "åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·", "output": "åŒ—äº¬å¸‚"},
    {"input": "å—äº¬ä¸‹é›¨å—", "output": "å—äº¬å¸‚"},
    {"input": "æ­¦æ±‰çƒ­å—", "output": "æ­¦æ±‰å¸‚"}
]

# ä¾‹å­æ‹¼è£…çš„æ ¼å¼
example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}"
)

# Promptæ¨¡æ¿
prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    suffix="Input: {input}\nOutput:",  # è¦æ”¾åœ¨ç¤ºä¾‹åé¢çš„æç¤ºæ¨¡æ¿å­—ç¬¦ä¸²ã€‚
    input_variables=["input"]  # ä¼ å…¥çš„å˜é‡
)

prompt = prompt.format(input="é•¿æ²™å¤šå°‘åº¦")

print("===Prompt===")
print(prompt)

print("===Response===")
response = chat_model.invoke(prompt)
print(response.content)
```

> ```
> ===Prompt===
> Input: åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·
> Output: åŒ—äº¬å¸‚
> 
> Input: å—äº¬ä¸‹é›¨å—
> Output: å—äº¬å¸‚
> 
> Input: æ­¦æ±‰çƒ­å—
> Output: æ­¦æ±‰å¸‚
> 
> Input: é•¿æ²™å¤šå°‘åº¦
> Output:
> ===Response===
> é•¿æ²™å¸‚
> ```



**ä¸¾ä¾‹3ï¼š**

```python
#########1ã€åˆ›å»ºæç¤ºæ¨¡æ¿#############
from langchain.prompts import PromptTemplate

# åˆ›å»ºæç¤ºæ¨¡æ¿ï¼Œé…ç½®ä¸€ä¸ªæç¤ºæ¨¡æ¿ï¼Œå°†ä¸€ä¸ªç¤ºä¾‹æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
prompt_template = "ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶,ç®—å¼ï¼š {input} å€¼ï¼š {output} ä½¿ç”¨ï¼š {description} "

# è¿™æ˜¯ä¸€ä¸ªæç¤ºæ¨¡æ¿ï¼Œç”¨äºè®¾ç½®æ¯ä¸ªç¤ºä¾‹çš„æ ¼å¼
prompt_sample = PromptTemplate.from_template(prompt_template)

# print(prompt_sample.format_prompt(input="2+2", output="4", description="åŠ æ³•è¿ç®—"))


########2ã€åˆ›å»ºä¸€ä¸ªFewShotPromptTemplateå¯¹è±¡###############
from langchain.prompts.few_shot import FewShotPromptTemplate

examples = [
    {"input": "2+2", "output": "4", "description": "åŠ æ³•è¿ç®—"},
    {"input": "5-2", "output": "3", "description": "å‡æ³•è¿ç®—"},
]

prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=prompt_sample,
    suffix="ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶,ç®—å¼: {input}  å€¼: {output}",
    input_variables=["input", "output"]
)
#print(prompt.format(input="2*5", output="10"))  # ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶,ç®—å¼: 2*5  å€¼: 10

########3ã€åˆå§‹åŒ–å¤§æ¨¡å‹ï¼Œç„¶åè°ƒç”¨####################
import os
import dotenv
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

chat_model = ChatOpenAI(model="gpt-4o-mini")

result = chat_model.invoke(prompt.format(input="2*5", output="10"))
print(result.content)  # ä½¿ç”¨: ä¹˜æ³•è¿ç®—
```

> ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶ï¼Œç®—å¼ï¼š2 * 5 å€¼ï¼š10 ä½¿ç”¨ï¼šä¹˜æ³•è¿ç®—
>
> å¦‚æœä½ è¿˜æœ‰å…¶ä»–çš„æ•°å­¦ç®—å¼éœ€è¦å¤„ç†æˆ–è§£æï¼Œè¯·å‘Šè¯‰æˆ‘ï¼



#### 3.4.3 FewShotChatMessagePromptTemplateçš„ä½¿ç”¨

é™¤äº†FewShotPromptTemplateä¹‹å¤–ï¼ŒFewShotChatMessagePromptTemplateæ˜¯ä¸“é—¨ä¸º **èŠå¤©å¯¹è¯åœºæ™¯** è®¾è®¡çš„å°‘æ ·æœ¬ï¼ˆfew-shotï¼‰æç¤ºæ¨¡æ¿ï¼Œå®ƒç»§æ‰¿è‡ª `FewShotPromptTemplate`ï¼Œä½†é’ˆå¯¹èŠå¤©æ¶ˆæ¯çš„æ ¼å¼è¿›è¡Œäº†ä¼˜åŒ–ã€‚



**ç‰¹ç‚¹**ï¼š

- è‡ªåŠ¨å°†ç¤ºä¾‹æ ¼å¼åŒ–ä¸ºèŠå¤©æ¶ˆæ¯ï¼ˆ`HumanMessage`/`AIMessage` ç­‰ï¼‰

- è¾“å‡ºç»“æ„åŒ–èŠå¤©æ¶ˆæ¯ï¼ˆ`List[BaseMessage]`ï¼‰

- ä¿ç•™å¯¹è¯è½®æ¬¡ç»“æ„

ä¸¾ä¾‹1ï¼šåŸºæœ¬ç»“æ„

```python
# 1.å¯¼å…¥ç›¸å…³åŒ…
from langchain.prompts import (
    FewShotChatMessagePromptTemplate,
    ChatPromptTemplate
)

# 2.ç¤ºä¾‹æ¶ˆæ¯æ ¼å¼
examples = [
    {"input": "1+1ç­‰äºå‡ ï¼Ÿ", "output": "1+1ç­‰äº2"},
    {"input": "æ³•å›½çš„é¦–éƒ½æ˜¯ï¼Ÿ", "output": "å·´é»"}
]

# 3.å®šä¹‰ç¤ºä¾‹çš„æ¶ˆæ¯æ ¼å¼æç¤ºè¯æ¨¡ç‰ˆ
msg_example_prompt = ChatPromptTemplate.from_messages([
    ("human", "{input}"),
    ("ai", "{output}"),
])

# 4.å®šä¹‰FewShotChatMessagePromptTemplateå¯¹è±¡
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=msg_example_prompt,
    examples=examples
)
# 5.è¾“å‡ºæ ¼å¼åŒ–åçš„æ¶ˆæ¯
print(few_shot_prompt.format())
```

```
Human: 1+1ç­‰äºå‡ ï¼Ÿ
AI: 1+1ç­‰äº2
Human: æ³•å›½çš„é¦–éƒ½æ˜¯ï¼Ÿ
AI: å·´é»
```



ä¸¾ä¾‹2ï¼šå°†åŸå§‹è¾“å…¥å’Œè¢«é€‰ä¸­çš„ç¤ºä¾‹ç»„ä¸€èµ·åŠ å…¥æç¤ºè¯æ¨¡ç‰ˆä¸­ã€‚

```python
# 1.å¯¼å…¥ç›¸å…³åŒ…
from langchain_core.prompts import (FewShotChatMessagePromptTemplate, ChatPromptTemplate)

# 2.å®šä¹‰ç¤ºä¾‹ç»„
examples = [
    {"input": "2+2", "output": "4"},
    {"input": "2+3", "output": "5"},
]

# 3.å®šä¹‰ç¤ºä¾‹çš„æ¶ˆæ¯æ ¼å¼æç¤ºè¯æ¨¡ç‰ˆ
example_prompt = ChatPromptTemplate.from_messages([('human', 'What is {input}?'), ('ai', '{output}')])

# 4.å®šä¹‰FewShotChatMessagePromptTemplateå¯¹è±¡
few_shot_prompt = FewShotChatMessagePromptTemplate(
    examples=examples,  # ç¤ºä¾‹ç»„
    example_prompt=example_prompt,  # ç¤ºä¾‹æç¤ºè¯è¯æ¨¡ç‰ˆ
)
# 5.è¾“å‡ºå®Œæ•´æç¤ºè¯çš„æ¶ˆæ¯æ¨¡ç‰ˆ
final_prompt = ChatPromptTemplate.from_messages(
    [
        ('system', 'You are a helpful AI Assistant'),
        few_shot_prompt,
        ('human', '{input}'),
    ]
)
# 6.æ ¼å¼åŒ–å®Œæ•´æ¶ˆæ¯
#final_prompt.format(input="What is 4+4?")
# æˆ–è€…
final_prompt.format_messages(input="What is 4+4?")
```

> [SystemMessage(content='You are a helpful AI Assistant', additional_kwargs={}, response_metadata={}),
> HumanMessage(content='What is 2+2?', additional_kwargs={}, response_metadata={}),
> AIMessage(content='4', additional_kwargs={}, response_metadata={}),
> HumanMessage(content='What is 2+3?', additional_kwargs={}, response_metadata={}),
> AIMessage(content='5', additional_kwargs={}, response_metadata={}),
> HumanMessage(content='What is 4+4?', additional_kwargs={}, response_metadata={})]

ä¸¾ä¾‹3ï¼šç»“åˆå¤§æ¨¡å‹

```python
import os
import dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage


dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

chat_model = ChatOpenAI(model="gpt-4o-mini")

messages = [SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ•°å­¦æ¨ç†çš„ä¸“å®¶"),
            HumanMessage(
                content="è‰¾ç±³éœ€è¦4åˆ†é’Ÿæ‰èƒ½çˆ¬åˆ°æ»‘æ¢¯é¡¶éƒ¨ï¼Œå¥¹èŠ±äº†1åˆ†é’Ÿæ‰æ»‘ä¸‹æ¥ï¼Œæ°´æ»‘æ¢¯å°†åœ¨15åˆ†é’Ÿåå…³é—­ï¼Œè¯·é—®åœ¨å…³é—­ä¹‹å‰å¥¹èƒ½æ»‘å¤šå°‘æ¬¡ï¼Ÿ"), ]

resonse = chat_model.invoke(messages)
print(resonse.content)
```

> è‰¾ç±³çˆ¬åˆ°æ»‘æ¢¯é¡¶éƒ¨éœ€è¦4åˆ†é’Ÿï¼Œç„¶åæ»‘ä¸‹æ¥éœ€è¦1åˆ†é’Ÿã€‚å› æ­¤ï¼Œå®Œæˆä¸€æ¬¡æ¥å›çš„æ€»æ—¶é—´ä¸ºï¼š
>
> 4åˆ†é’Ÿï¼ˆçˆ¬ä¸Šï¼‰ + 1åˆ†é’Ÿï¼ˆæ»‘ä¸‹ï¼‰ = 5åˆ†é’Ÿã€‚
>
> æ°´æ»‘æ¢¯å°†åœ¨15åˆ†é’Ÿåå…³é—­ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è®¡ç®—åœ¨è¿™æ®µæ—¶é—´å†…è‰¾ç±³èƒ½å¤Ÿå®Œæˆçš„æ¬¡æ•°ï¼š
>
> 15åˆ†é’Ÿ Ã· 5åˆ†é’Ÿ/æ¬¡ = 3æ¬¡ã€‚
>
> å› æ­¤ï¼Œè‰¾ç±³åœ¨æ°´æ»‘æ¢¯å…³é—­ä¹‹å‰èƒ½å¤Ÿæ»‘3æ¬¡ã€‚

> å®é™…ä¸Šï¼Œåœ¨ä½¿ç”¨ gpt-4o æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå³ä½¿ä¸é‡‡ç”¨Few-Shotæç¤ºï¼Œæ¨¡å‹ä¹Ÿèƒ½ä»¥å¾ˆé«˜çš„æ¦‚ç‡æ­£ç¡®å›ç­”é—®é¢˜ï¼Œè¿™å½’åŠŸäºæ¨¡å‹æœ¬èº«å·²ç»éå¸¸å¼ºå¤§çš„èƒ½åŠ›ã€‚

ç»§ç»­ï¼š

```python
from langchain.prompts import (
    ChatPromptTemplate,
    FewShotChatMessagePromptTemplate,
)

examples = [
    {"input": "ç½—æ°æœ‰äº”ä¸ªç½‘çƒï¼Œä»–åˆä¹°äº†ä¸¤ç›’ç½‘çƒï¼Œæ¯ç›’æœ‰3ä¸ªç½‘çƒï¼Œè¯·é—®ä»–ç°åœ¨æ€»å…±æœ‰å¤šå°‘ä¸ªç½‘çƒï¼Ÿ",
     "output": "ç½—æ°ä¸€å¼€å§‹æœ‰äº”ä¸ªç½‘çƒï¼Œåˆè´­ä¹°äº†ä¸¤ç›’ç½‘çƒï¼Œæ¯ç›’3ä¸ªï¼Œå…±è´­ä¹°äº†6ä¸ªç½‘çƒï¼Œå› æ­¤ç°åœ¨æ€»å…±ç”±5+6=11ä¸ªç½‘çƒã€‚å› æ­¤ç­”æ¡ˆæ˜¯11ã€‚"},

    {"input": "é£Ÿå ‚æ€»å…±æœ‰23ä¸ªè‹¹æœï¼Œå¦‚æœä»–ä»¬ç”¨æ‰20ä¸ªè‹¹æœï¼Œç„¶ååˆä¹°äº†6ä¸ªè‹¹æœï¼Œè¯·é—®ç°åœ¨é£Ÿå ‚æ€»å…±æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ",
     "output": "é£Ÿå ‚æœ€åˆæœ‰23ä¸ªè‹¹æœï¼Œç”¨æ‰20ä¸ªï¼Œç„¶ååˆä¹°äº†6ä¸ªï¼Œæ€»å…±æœ‰23-20+6=9ä¸ªè‹¹æœï¼Œç­”æ¡ˆæ˜¯9ã€‚"},

    {"input": "æ‚è€è€…å¯ä»¥æ‚è€16ä¸ªçƒã€‚å…¶ä¸­ä¸€åŠçš„çƒæ˜¯é«˜å°”å¤«çƒï¼Œå…¶ä¸­ä¸€åŠçš„é«˜å°”å¤«çƒæ˜¯è“è‰²çš„ã€‚è¯·é—®æ€»å…±æœ‰å¤šå°‘ä¸ªè“è‰²é«˜å°”å¤«çƒï¼Ÿ",
     "output": "æ€»å…±æœ‰16ä¸ªçƒï¼Œå…¶ä¸­ä¸€åŠæ˜¯é«˜å°”å¤«çƒï¼Œä¹Ÿå°±æ˜¯8ä¸ªï¼Œå…¶ä¸­ä¸€åŠæ˜¯è“è‰²çš„ï¼Œä¹Ÿå°±æ˜¯4ä¸ªï¼Œç­”æ¡ˆæ˜¯4ä¸ªã€‚"},
]

example_prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "{input}"),
        ("ai", "{output}"),
    ]
)

few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)

print(few_shot_prompt.format())
```

> Human: ç½—æ°æœ‰äº”ä¸ªç½‘çƒï¼Œä»–åˆä¹°äº†ä¸¤ç›’ç½‘çƒï¼Œæ¯ç›’æœ‰3ä¸ªç½‘çƒï¼Œè¯·é—®ä»–ç°åœ¨æ€»å…±æœ‰å¤šå°‘ä¸ªç½‘çƒï¼Ÿ
> AI: ç½—æ°ä¸€å¼€å§‹æœ‰äº”ä¸ªç½‘çƒï¼Œåˆè´­ä¹°äº†ä¸¤ç›’ç½‘çƒï¼Œæ¯ç›’3ä¸ªï¼Œå…±è´­ä¹°äº†6ä¸ªç½‘çƒï¼Œå› æ­¤ç°åœ¨æ€»å…±ç”±5+6=11ä¸ªç½‘çƒã€‚å› æ­¤ç­”æ¡ˆæ˜¯11ã€‚
> Human: é£Ÿå ‚æ€»å…±æœ‰23ä¸ªè‹¹æœï¼Œå¦‚æœä»–ä»¬ç”¨æ‰20ä¸ªè‹¹æœï¼Œç„¶ååˆä¹°äº†6ä¸ªè‹¹æœï¼Œè¯·é—®ç°åœ¨é£Ÿå ‚æ€»å…±æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ
> AI: é£Ÿå ‚æœ€åˆæœ‰23ä¸ªè‹¹æœï¼Œç”¨æ‰20ä¸ªï¼Œç„¶ååˆä¹°äº†6ä¸ªï¼Œæ€»å…±æœ‰23-20+6=9ä¸ªè‹¹æœï¼Œç­”æ¡ˆæ˜¯9ã€‚
> Human: æ‚è€è€…å¯ä»¥æ‚è€16ä¸ªçƒã€‚å…¶ä¸­ä¸€åŠçš„çƒæ˜¯é«˜å°”å¤«çƒï¼Œå…¶ä¸­ä¸€åŠçš„é«˜å°”å¤«çƒæ˜¯è“è‰²çš„ã€‚è¯·é—®æ€»å…±æœ‰å¤šå°‘ä¸ªè“è‰²é«˜å°”å¤«çƒï¼Ÿ
> AI: æ€»å…±æœ‰16ä¸ªçƒï¼Œå…¶ä¸­ä¸€åŠæ˜¯é«˜å°”å¤«çƒï¼Œä¹Ÿå°±æ˜¯8ä¸ªï¼Œå…¶ä¸­ä¸€åŠæ˜¯è“è‰²çš„ï¼Œä¹Ÿå°±æ˜¯4ä¸ªï¼Œç­”æ¡ˆæ˜¯4ä¸ªã€‚

è¿›ä¸€æ­¥ï¼š

``` python
final_prompt = ChatPromptTemplate.from_messages(
    [
        few_shot_prompt,
        ("human", "{input}"),
    ]
)

print(final_prompt)
```

> input_variables=['input'] input_types={} partial_variables={} messages=[FewShotChatMessagePromptTemplate(examples=[{'input': 'ç½—æ°æœ‰äº”ä¸ªç½‘çƒï¼Œä»–åˆä¹°äº†ä¸¤ç›’ç½‘çƒï¼Œæ¯ç›’æœ‰3ä¸ªç½‘çƒï¼Œè¯·é—®ä»–ç°åœ¨æ€»å…±æœ‰å¤šå°‘ä¸ªç½‘çƒï¼Ÿ', 'output': 'ç½—æ°ä¸€å¼€å§‹æœ‰äº”ä¸ªç½‘çƒï¼Œåˆè´­ä¹°äº†ä¸¤ç›’ç½‘çƒï¼Œæ¯ç›’3ä¸ªï¼Œå…±è´­ä¹°äº†6ä¸ªç½‘çƒï¼Œå› æ­¤ç°åœ¨æ€»å…±ç”±5+6=11ä¸ªç½‘çƒã€‚å› æ­¤ç­”æ¡ˆæ˜¯11ã€‚'}, {'input': 'é£Ÿå ‚æ€»å…±æœ‰23ä¸ªè‹¹æœï¼Œå¦‚æœä»–ä»¬ç”¨æ‰20ä¸ªè‹¹æœï¼Œç„¶ååˆä¹°äº†6ä¸ªè‹¹æœï¼Œè¯·é—®ç°åœ¨é£Ÿå ‚æ€»å…±æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ', 'output': 'é£Ÿå ‚æœ€åˆæœ‰23ä¸ªè‹¹æœï¼Œç”¨æ‰20ä¸ªï¼Œç„¶ååˆä¹°äº†6ä¸ªï¼Œæ€»å…±æœ‰23-20+6=9ä¸ªè‹¹æœï¼Œç­”æ¡ˆæ˜¯9ã€‚'}, {'input': 'æ‚è€è€…å¯ä»¥æ‚è€16ä¸ªçƒã€‚å…¶ä¸­ä¸€åŠçš„çƒæ˜¯é«˜å°”å¤«çƒï¼Œå…¶ä¸­ä¸€åŠçš„é«˜å°”å¤«çƒæ˜¯è“è‰²çš„ã€‚è¯·é—®æ€»å…±æœ‰å¤šå°‘ä¸ªè“è‰²é«˜å°”å¤«çƒï¼Ÿ', 'output': 'æ€»å…±æœ‰16ä¸ªçƒï¼Œå…¶ä¸­ä¸€åŠæ˜¯é«˜å°”å¤«çƒï¼Œä¹Ÿå°±æ˜¯8ä¸ªï¼Œå…¶ä¸­ä¸€åŠæ˜¯è“è‰²çš„ï¼Œä¹Ÿå°±æ˜¯4ä¸ªï¼Œç­”æ¡ˆæ˜¯4ä¸ªã€‚'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]

è¿›è€Œï¼š

``` python
response = chat_model.invoke(final_prompt.invoke({"input": "è‰¾ç±³éœ€è¦4åˆ†é’Ÿæ‰èƒ½çˆ¬åˆ°æ»‘æ¢¯é¡¶éƒ¨ï¼Œå¥¹èŠ±äº†1åˆ†é’Ÿæ‰æ»‘ä¸‹æ¥ï¼Œæ°´æ»‘æ¢¯å°†åœ¨15åˆ†é’Ÿåå…³é—­ï¼Œè¯·é—®åœ¨å…³é—­ä¹‹å‰å¥¹èƒ½æ»‘å¤šå°‘æ¬¡ï¼Ÿ"}))
print(response.content)
```

> è‰¾ç±³çˆ¬åˆ°æ»‘æ¢¯é¡¶éƒ¨éœ€è¦4åˆ†é’Ÿï¼Œæ»‘ä¸‹æ¥éœ€è¦1åˆ†é’Ÿï¼Œå› æ­¤å¥¹æ¯æ¬¡æ»‘æ¢¯çš„æ€»æ—¶é—´æ˜¯ï¼š
>
> 4åˆ†é’Ÿï¼ˆçˆ¬ä¸Šå»ï¼‰ + 1åˆ†é’Ÿï¼ˆæ»‘ä¸‹æ¥ï¼‰ = 5åˆ†é’Ÿã€‚
>
> æ°´æ»‘æ¢¯å°†åœ¨15åˆ†é’Ÿåå…³é—­ï¼Œå› æ­¤å¥¹åœ¨å…³é—­ä¹‹å‰å¯ä»¥æ»‘çš„æ¬¡æ•°ä¸ºï¼š
>
> 15åˆ†é’Ÿ Ã· 5åˆ†é’Ÿ/æ¬¡ = 3æ¬¡ã€‚
>
> æ‰€ä»¥ï¼Œè‰¾ç±³åœ¨æ°´æ»‘æ¢¯å…³é—­ä¹‹å‰èƒ½æ»‘3æ¬¡ã€‚

ä¸¾ä¾‹4ï¼š

```python
# 1.å¯¼å…¥ç›¸å…³åŒ…
from langchain_core.prompts import (FewShotChatMessagePromptTemplate, ChatPromptTemplate)

# 2.å®šä¹‰ç¤ºä¾‹ç»„
examples = [
    {"input": "2ğŸ¦œ2", "output": "4"},
    {"input": "2ğŸ¦œ3", "output": "8"},
]

# 3.å®šä¹‰ç¤ºä¾‹çš„æ¶ˆæ¯æ ¼å¼æç¤ºè¯æ¨¡ç‰ˆ
example_prompt = ChatPromptTemplate.from_messages([('human', '{input} æ˜¯å¤šå°‘?'), ('ai', '{output}')])

# 4.å®šä¹‰FewShotChatMessagePromptTemplateå¯¹è±¡
few_shot_prompt = FewShotChatMessagePromptTemplate(
    examples=examples,  # ç¤ºä¾‹ç»„
    example_prompt=example_prompt,  # ç¤ºä¾‹æç¤ºè¯è¯æ¨¡ç‰ˆ
)
# 5.è¾“å‡ºå®Œæ•´æç¤ºè¯çš„æ¶ˆæ¯æ¨¡ç‰ˆ
final_prompt = ChatPromptTemplate.from_messages(
    [
        ('system', 'ä½ æ˜¯ä¸€ä¸ªæ•°å­¦å¥‡æ‰'),
        few_shot_prompt,
        ('human', '{input}'),
    ]
)

#6.æä¾›å¤§æ¨¡å‹
import os
import dotenv
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

chat_model = ChatOpenAI(model="gpt-4o-mini",
                        temperature=0.4)

chat_model.invoke(final_prompt.invoke(input="2ğŸ¦œ4")).content
```

> '2ğŸ¦œ4 ç­‰äº 16ã€‚'



#### 3.4.4 Example selectors(ç¤ºä¾‹é€‰æ‹©å™¨)

å‰é¢FewShotPromptTemplateçš„ç‰¹ç‚¹æ˜¯ï¼Œæ— è®ºè¾“å…¥ä»€ä¹ˆé—®é¢˜ï¼Œéƒ½ä¼šåŒ…å«å…¨éƒ¨ç¤ºä¾‹ã€‚åœ¨å®é™…å¼€å‘ä¸­æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰é€‰æ‹©å™¨æ¥é€‰æ‹©ä¾‹å­ã€‚

**ä½¿ç”¨çš„å¥½å¤„**ï¼š

- `åŠ¨æ€ç¤ºä¾‹ç­›é€‰`ï¼šæ ¹æ®å½“å‰è¾“å…¥ï¼Œä»å¤§é‡å€™é€‰ç¤ºä¾‹ä¸­é€‰å–æœ€ç›¸å…³çš„å­é›†
- `ä¸Šä¸‹æ–‡ä¼˜åŒ–`ï¼šé¿å…ç›²ç›®ä¼ é€’æ‰€æœ‰ç¤ºä¾‹ï¼Œå‡å°‘ token æ¶ˆè€—å¹¶æå‡æ•ˆæœ
- `çµæ´»é€‚é…`ï¼šæ”¯æŒå¤šç§é€‰æ‹©ç­–ç•¥ï¼ˆç›¸ä¼¼åº¦ã€å¤šæ ·æ€§ã€è‡ªå®šä¹‰è§„åˆ™ç­‰ï¼‰

##### ç±»å‹1ï¼šè¯­ä¹‰ç›¸ä¼¼ç¤ºä¾‹é€‰æ‹©å™¨

**æ¦‚å¿µ**ï¼š`SemanticSimilarityExampleSelector`æ˜¯ä¸€ç§åŸºäº**è¯­ä¹‰ç›¸ä¼¼åº¦**åŠ¨æ€ç­›é€‰ç¤ºä¾‹çš„ç»„ä»¶ï¼Œä¸“ä¸º Few-Shot Learningï¼ˆå°æ ·æœ¬å­¦ä¹ ï¼‰åœºæ™¯è®¾è®¡ã€‚

**è®¾è®¡ç†å¿µ**ï¼šä½¿ç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ OpenAI Embeddingsï¼‰å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œé€šè¿‡**ä½™å¼¦ç›¸ä¼¼åº¦**ç­‰åº¦é‡æ–¹å¼è¯„ä¼°è¯­ä¹‰ç›¸å…³æ€§ï¼Œç›´æ¥é€‰æ‹©ä¸è¾“å…¥é—®é¢˜æœ€ç›¸ä¼¼çš„ `k` ä¸ªç¤ºä¾‹ï¼Œä¿è¯ç»“æœé«˜åº¦ç›¸å…³ã€‚

> - ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯é€šè¿‡è®¡ç®—ä¸¤ä¸ªå‘é‡çš„å¤¹è§’ä½™å¼¦å€¼æ¥è¡¡é‡å®ƒä»¬çš„ç›¸ä¼¼æ€§ã€‚å®ƒçš„å€¼èŒƒå›´åœ¨-1åˆ°1ä¹‹é—´ï¼šå½“ä¸¤ä¸ªå‘é‡æ–¹å‘ç›¸åŒæ—¶å€¼ä¸º1ï¼›å¤¹è§’ä¸º90Â°æ—¶å€¼ä¸º0ï¼›æ–¹å‘å®Œå…¨ç›¸åæ—¶ä¸º-1ã€‚
>
> - æ•°å­¦è¡¨è¾¾å¼ï¼šä½™å¼¦ç›¸ä¼¼åº¦ = (AÂ·B) / (||A|| * ||B||)ã€‚å…¶ä¸­AÂ·Bæ˜¯ç‚¹ç§¯ï¼Œ||A||å’Œ||B||æ˜¯å‘é‡çš„æ¨¡ï¼ˆé•¿åº¦ï¼‰ã€‚

ä¸¾ä¾‹1ï¼š

```bash
pip install chromadb
```

```python
# 1.å¯¼å…¥ç›¸å…³åŒ…
from langchain_community.vectorstores import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
import os
import dotenv
from langchain_openai import OpenAIEmbeddings

dotenv.load_dotenv()

# 2.å®šä¹‰åµŒå…¥æ¨¡å‹
os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

embeddings_model = OpenAIEmbeddings(
    model="text-embedding-ada-002"
)

# 3.å®šä¹‰ç¤ºä¾‹ç»„
examples = [
    {
        "question": "è°æ´»å¾—æ›´ä¹…ï¼Œç©†ç½•é»˜å¾·Â·é˜¿é‡Œè¿˜æ˜¯è‰¾ä¼¦Â·å›¾çµ?",
        "answer": """
        æ¥ä¸‹æ¥è¿˜éœ€è¦é—®ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ
        è¿½é—®ï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œå»ä¸–æ—¶å¤šå¤§å¹´çºªï¼Ÿ
        ä¸­é—´ç­”æ¡ˆï¼šç©†ç½•é»˜å¾·Â·é˜¿é‡Œå»ä¸–æ—¶äº«å¹´74å²ã€‚
        """,
    },
    {
        "question": "craigslistçš„åˆ›å§‹äººæ˜¯ä»€ä¹ˆæ—¶å€™å‡ºç”Ÿçš„ï¼Ÿ",
        "answer": """
        æ¥ä¸‹æ¥è¿˜éœ€è¦é—®ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ
        è¿½é—®ï¼šè°æ˜¯craigslistçš„åˆ›å§‹äººï¼Ÿ
        ä¸­çº§ç­”æ¡ˆï¼šCraigslistæ˜¯ç”±å…‹é›·æ ¼Â·çº½é©¬å…‹åˆ›ç«‹çš„ã€‚
        """,
    },
    {
        "question": "è°æ˜¯ä¹”æ²»Â·åç››é¡¿çš„å¤–ç¥–çˆ¶ï¼Ÿ",
        "answer": """
        æ¥ä¸‹æ¥è¿˜éœ€è¦é—®ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ
        è¿½é—®ï¼šè°æ˜¯ä¹”æ²»Â·åç››é¡¿çš„æ¯äº²ï¼Ÿ
        ä¸­é—´ç­”æ¡ˆï¼šä¹”æ²»Â·åç››é¡¿çš„æ¯äº²æ˜¯ç›ä¸½Â·é²å°”Â·åç››é¡¿ã€‚
        """,
    },
    {
        "question": "ã€Šå¤§ç™½é²¨ã€‹å’Œã€Šçš‡å®¶èµŒåœºã€‹çš„å¯¼æ¼”éƒ½æ¥è‡ªåŒä¸€ä¸ªå›½å®¶å—ï¼Ÿ",
        "answer": """
        æ¥ä¸‹æ¥è¿˜éœ€è¦é—®ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ
        è¿½é—®ï¼šã€Šå¤§ç™½é²¨ã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ
        ä¸­çº§ç­”æ¡ˆï¼šã€Šå¤§ç™½é²¨ã€‹çš„å¯¼æ¼”æ˜¯å²è’‚æ–‡Â·æ–¯çš®å°”ä¼¯æ ¼ã€‚
        """,
    },
]

# 4.å®šä¹‰ç¤ºä¾‹é€‰æ‹©å™¨
example_selector = SemanticSimilarityExampleSelector.from_examples(
    # è¿™æ˜¯å¯ä¾›é€‰æ‹©çš„ç¤ºä¾‹åˆ—è¡¨
    examples,
    # è¿™æ˜¯ç”¨äºç”ŸæˆåµŒå…¥çš„åµŒå…¥ç±»ï¼Œç”¨äºè¡¡é‡è¯­ä¹‰ç›¸ä¼¼æ€§
    embeddings_model,
    # è¿™æ˜¯ç”¨äºå­˜å‚¨åµŒå…¥å¹¶è¿›è¡Œç›¸ä¼¼æ€§æœç´¢çš„ VectorStore ç±»
    Chroma,
    # è¿™æ˜¯è¦ç”Ÿæˆçš„ç¤ºä¾‹æ•°é‡
    k=1,
)

# é€‰æ‹©ä¸è¾“å…¥æœ€ç›¸ä¼¼çš„ç¤ºä¾‹
question = "ç›ä¸½Â·é²å°”Â·åç››é¡¿çš„çˆ¶äº²æ˜¯è°?"
selected_examples = example_selector.select_examples({"question": question})
print(f"ä¸è¾“å…¥æœ€ç›¸ä¼¼çš„ç¤ºä¾‹ï¼š{selected_examples}")

# for example in selected_examples:
#     print("\n")
#     for k, v in example.items():
#         print(f"{k}: {v}")
```

> question: è°æ˜¯ä¹”æ²»Â·åç››é¡¿çš„å¤–ç¥–çˆ¶ï¼Ÿ
>answer: 
> æ¥ä¸‹æ¥è¿˜éœ€è¦é—®ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ
> è¿½é—®ï¼šè°æ˜¯ä¹”æ²»Â·åç››é¡¿çš„æ¯äº²ï¼Ÿ
> ä¸­é—´ç­”æ¡ˆï¼šä¹”æ²»Â·åç››é¡¿çš„æ¯äº²æ˜¯ç›ä¸½Â·é²å°”Â·åç››é¡¿

ä¸¾ä¾‹2ï¼šç»“åˆ FewShotPromptTemplate ä½¿ç”¨

è¿™é‡Œä½¿ç”¨FAISSï¼Œéœ€å®‰è£…ï¼š

```bash
pip install faiss-cpu
#æˆ–
conda install faiss-cpu
```

```python
# 1.å¯¼å…¥ç›¸å…³åŒ…
from langchain_community.vectorstores import FAISS
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_openai import OpenAIEmbeddings

# 2.å®šä¹‰ç¤ºä¾‹æç¤ºè¯æ¨¡ç‰ˆ
example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

# 3.åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æç¤ºè¯æ¨¡ç‰ˆ
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

# 4.å®šä¹‰åµŒå…¥æ¨¡å‹
embeddings = OpenAIEmbeddings(
    model="text-embedding-ada-002"
)

# 5.åˆ›å»ºè¯­ä¹‰ç›¸ä¼¼æ€§ç¤ºä¾‹é€‰æ‹©å™¨
example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,
    embeddings,
    FAISS,
    k=2,
)
#æˆ–è€…
#example_selector = SemanticSimilarityExampleSelector(
#    examples,
#    embeddings,
#    FAISS,
#    k=2
#)

# 6.å®šä¹‰å°æ ·æœ¬æç¤ºè¯æ¨¡ç‰ˆ
similar_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="ç»™å‡ºæ¯ä¸ªè¯ç»„çš„åä¹‰è¯",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)
print(similar_prompt.format(adjective="worried"))
```

> ç»™å‡ºæ¯ä¸ªè¯ç»„çš„åä¹‰è¯
>
> Input: happy
> Output: sad
>
> Input: sunny
> Output: gloomy
>
> Input: worried
> Output:



##### ç±»å‹2ï¼šé•¿åº¦é€‰æ‹©å™¨ Select by length

**æ¦‚å¿µ**ï¼š`LengthBasedExampleSelector` æ˜¯ä¸€ç§åŸºäº**æ–‡æœ¬é•¿åº¦**åŠ¨æ€é€‰æ‹©ç¤ºä¾‹çš„ç»„ä»¶ï¼Œä¸»è¦ç”¨äº Few-Shot Learning åœºæ™¯ä¸­ä¼˜åŒ–ç¤ºä¾‹é€‰æ‹©ã€‚å…¶æ ¸å¿ƒä½œç”¨æ˜¯**æ ¹æ®è¾“å…¥æ–‡æœ¬çš„é•¿åº¦ï¼Œä»å€™é€‰ç¤ºä¾‹ä¸­ç­›é€‰å‡ºé•¿åº¦æœ€åŒ¹é…çš„ç¤ºä¾‹**ï¼Œä»è€Œæå‡å¤§æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ•ˆæœå¹¶åˆç†æ§åˆ¶ token æ¶ˆè€—ã€‚

**æ€æƒ³ï¼š**LengthBasedExampleSelector` çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šæ ¹æ®è¾“å…¥æç¤ºè¯çš„é•¿åº¦ï¼ŒåŠ¨æ€é€‰æ‹©åˆé€‚æ•°é‡çš„ç¤ºä¾‹ï¼Œç¡®ä¿æœ€ç»ˆæ‹¼æ¥çš„æç¤ºè¯ä¸è¶…è¿‡è®¾å®šçš„ `max_lengthã€‚

**ç‰¹ç‚¹**ï¼š

- **é•¿åº¦åŒ¹é…**ï¼šé€‰æ‹©ä¸å½“å‰è¾“å…¥æ–‡æœ¬é•¿åº¦ç›¸è¿‘çš„ç¤ºä¾‹ï¼Œå¢å¼ºæ¨¡å‹å¯¹æ–‡æœ¬ç»“æ„çš„ç†è§£
- **Token æ§åˆ¶**ï¼šé¿å…é€‰æ‹©è¿‡é•¿çš„ç¤ºä¾‹å¯¼è‡´ token è¶…é™
- **æ•ˆç‡ä¼˜åŒ–**ï¼šæ¯”è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—æ›´è½»é‡ï¼Œé€‚åˆå¯¹å“åº”é€Ÿåº¦è¦æ±‚é«˜çš„åœºæ™¯



ä¸¾ä¾‹ï¼š

```python
# 1.å¯¼å…¥ç›¸å…³åŒ…
from langchain_core.example_selectors import LengthBasedExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

# 2.åˆ›é€ åä¹‰è¯çš„æ¨¡æ‹Ÿä»»åŠ¡çš„ä¾‹å­
examples = [
    {"input": "happy", "output": "sad"},  # å¿«ä¹çš„ -> æ‚²ä¼¤çš„
    {"input": "tall", "output": "short"},  # é«˜çš„ -> çŸ®çš„
    {"input": "energetic", "output": "lethargic"},  # ç²¾åŠ›å……æ²›çš„ -> æ˜ç¡çš„
    {"input": "sunny", "output": "gloomy"},  # é˜³å…‰æ˜åªšçš„ -> é˜´æ²‰çš„
    {"input": "windy", "output": "calm"},  # å¤šé£çš„ -> å¹³é™çš„
]

# 3.åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æç¤ºè¯æ¨¡ç‰ˆ
example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

# 4.åˆ›å»ºä¸€ä¸ªç¤ºä¾‹é€‰æ‹©å™¨ï¼Œé€‰æ‹©é•¿åº¦å°äº25çš„ä¾‹å­
example_selector = LengthBasedExampleSelector(
    # æä¾›äº†å¯ä¾›é€‰æ‹©çš„ç¤ºä¾‹.
    examples=examples,
    # æä¾›ç¤ºä¾‹æç¤ºè¯æ¨¡ç‰ˆï¼ˆæ ¼å¼åŒ–æç¤ºè¯ï¼‰
    example_prompt=example_prompt,
    # æ ¼å¼ç¤ºä¾‹çš„æœ€å¤§é•¿åº¦ï¼ˆæ‰€æœ‰ç¤ºä¾‹é•¿åº¦+è¾“å…¥çš„æ€»é•¿åº¦é™åˆ¶ï¼‰
    max_length=25,  #å¯ä»¥å°è¯•æ”¹å°
    # ç”¨äºè·å–å­—ç¬¦ä¸²é•¿åº¦çš„å‡½æ•°
    # ä»¥ç¡®å®šè¦åŒ…å«å“ªäº›ç¤ºä¾‹ã€‚å®ƒè¢«æ³¨é‡Šæ‰æ˜¯å› ä¸º
    # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œåˆ™ä½œä¸ºé»˜è®¤å€¼æä¾›ã€‚
    # get_text_length: Callable[[str], int] = lambda x: len(re.split("\n| ", x))
)

# å®šä¹‰å°æ ·æœ¬æç¤ºè¯æ¨¡ç‰ˆ
dynamic_prompt = FewShotPromptTemplate(
    # We provide an ExampleSelector instead of examples.
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="ç»™å‡ºæ¯ä¸ªè¾“å…¥çš„åä¹‰è¯",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)

#ä¸€ä¸ªè¾“å…¥å¾ˆå°çš„ä¾‹å­ï¼Œå®ƒä¼šé€‰æ‹©æ‰€æœ‰çš„ä¾‹å­
print(dynamic_prompt.format(adjective="big"))
```

> ç»™å‡ºæ¯ä¸ªè¾“å…¥çš„åä¹‰è¯
>
> Input: happy
> Output: sad
>
> Input: tall
> Output: short
>
> Input: energetic
> Output: lethargic
>
> Input: sunny
> Output: gloomy
>
> Input: windy
> Output: calm
>
> Input: big
> Output:

ä»£ç ä¸­çš„ max_length=25 å¯ä»¥å°è¯•æ”¹å°ã€‚

ä¸€ä¸ªå…·æœ‰é•¿è¾“å…¥çš„ç¤ºä¾‹ï¼Œå› æ­¤å®ƒåªé€‰æ‹©ä¸€ä¸ªç¤ºä¾‹

```python
long_string = "big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else"

print(dynamic_prompt.format(adjective=long_string))
```

> ç»™å‡ºæ¯ä¸ªè¾“å…¥çš„åä¹‰è¯
>
> Input: happy
> Output: sad
>
> Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else
> Output:

å‘ç¤ºä¾‹é€‰æ‹©å™¨ä¸­æ·»åŠ ç¤ºä¾‹ï¼š

```python
new_example = {"input": "big", "output": "small"}
dynamic_prompt.example_selector.add_example(new_example)
print(dynamic_prompt.format(adjective="enthusiastic"))
```

> ç»™å‡ºæ¯ä¸ªè¾“å…¥çš„åä¹‰è¯
>
> Input: happy
> Output: sad
>
> Input: tall
> Output: short
>
> Input: energetic
> Output: lethargic
>
> Input: sunny
> Output: gloomy
>
> Input: windy
> Output: calm
>
> Input: big
> Output: small
>
> Input: enthusiastic
> Output:

##### ç±»å‹3ï¼šæœ€å¤§è¾¹é™…ç›¸å…³ç¤ºä¾‹é€‰æ‹©å™¨MMR

**æ¦‚å¿µ**ï¼šMaxMarginalRelevanceExampleSelectoræ ¹æ®ä¸è¾“å…¥æœ€ç›¸ä¼¼çš„ç¤ºä¾‹ç»„åˆæ¥é€‰æ‹©ç¤ºä¾‹ï¼ŒåŒæ—¶è¿˜ä¼˜åŒ–äº†å¤šæ ·æ€§ã€‚å®ƒé€šè¿‡å¯»æ‰¾ä¸è¾“å…¥å…·æœ‰æœ€å¤§ä½™å¼¦ç›¸ä¼¼åº¦çš„åµŒå…¥çš„ä¾‹å­æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œç„¶åè¿­ä»£åœ°æ·»åŠ å®ƒä»¬ï¼ŒåŒæ—¶æƒ©ç½šå®ƒä»¬ä¸å·²ç»é€‰æ‹©çš„ä¾‹å­çš„æ¥è¿‘ç¨‹åº¦ã€‚



**ç‰¹ç‚¹**ï¼š

- **åŒé‡ä¼˜åŒ–ç›®æ ‡**

  - ç›¸å…³æ€§ä¼˜å…ˆï¼šä¼˜å…ˆé€‰æ‹©ä¸è¾“å…¥é—®é¢˜è¯­ä¹‰ç›¸ä¼¼çš„ç¤ºä¾‹
  - å¤šæ ·æ€§ä¿éšœï¼šé€šè¿‡æƒ©ç½šæœºåˆ¶é¿å…è¿”å›åŒè´¨åŒ–çš„å†…å®¹
- **é˜²å†—ä½™ã€å¤šè§’åº¦è¦†ç›–**
- **é€‚ç”¨å¼€æ”¾åŸŸé—®ç­”ã€å¤šè§’åº¦åˆ†æä»»åŠ¡ç­‰åœºæ™¯**



**åŸç†**ï¼š

![image-20250430182819041](images/image-20250430182819041.png)



**ç¤ºä¾‹æ¨æ¼”**

è¾“å…¥**æŸ¥è¯¢** ="happy"ï¼ˆå‘é‡ = `[1.0, 0.0, 0.0]`ï¼‰ å€™é€‰ç¤ºä¾‹ï¼š

| å€™é€‰ç¤ºä¾‹            | å‘é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰  |
| :------------------ | :-------------- |
| `happy â†’ sad`       | [0.9, 0.1, 0.0] |
| `tall â†’ short`      | [0.2, 0.8, 0.0] |
| `energetic â†’ tired` | [0.1, 0.1, 0.8] |
| `sunny â†’ cloudy`    | [0.3, 0.6, 0.1] |
| `windy â†’ calm`      | [0.0, 0.7, 0.3] |


**ç›®æ ‡**ï¼šé€‰æ‹© 2 ä¸ªç¤ºä¾‹ï¼ŒåŒæ—¶é¿å…å†—ä½™ã€‚



**é€‰æ‹©è¿‡ç¨‹**ï¼š

- **Step 1: è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰ç¤ºä¾‹çš„ç›¸ä¼¼åº¦**

  ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®— `query` å’Œæ¯ä¸ªç¤ºä¾‹çš„ç›¸ä¼¼åº¦

  | å€™é€‰ç¤ºä¾‹            | ç›¸ä¼¼åº¦ï¼ˆsimï¼‰  |
  | :------------------ | :------------- |
  | `happy â†’ sad`       | 0.95ï¼ˆæœ€ç›¸å…³ï¼‰ |
  | `tall â†’ short`      | 0.2            |
  | `energetic â†’ tired` | 0.1            |
  | `sunny â†’ cloudy`    | 0.3            |
  | `windy â†’ calm`      | 0.0            |

  **é€‰æ‹©ç¬¬ 1 ä¸ªç¤ºä¾‹**ï¼š`happy â†’ sad`ï¼ˆç›¸ä¼¼åº¦æœ€é«˜ï¼‰

  

- **Step 2: é€‰æ‹©ç¬¬ 2 ä¸ªç¤ºä¾‹ï¼ˆè€ƒè™‘å¤šæ ·æ€§ï¼‰**

  è®¡ç®—æ¯ä¸ªå€™é€‰ç¤ºä¾‹ä¸å·²é€‰ç¤ºä¾‹çš„ç›¸ä¼¼åº¦

  | å€™é€‰ç¤ºä¾‹             | MMR åˆ†æ•°   |
  | :------------------- | :--------- |
  | `tall â†’ short`       | **-0.025** |
  | ``energetic â†’ tired` | **-0.035** |
  | sunny â†’ gloomy       | **0.0**    |
  | `windy â†’ calm`       | **-0.05**  |

**é€‰æ‹© MMR åˆ†æ•°æœ€é«˜çš„ç¤ºä¾‹**ï¼š`sunny â†’ gloomy`ï¼ˆåˆ†æ•° = 0.0ï¼‰



- **Step 3 æœ€ç»ˆé€‰æ‹©ç»“æœ**
  1. `happy â†’ sad`ï¼ˆæœ€ç›¸å…³ï¼‰ã€‚
  2. `sunny â†’ gloomy`ï¼ˆä¸å·²é€‰ç¤ºä¾‹å·®å¼‚æœ€å¤§ï¼‰ã€‚

```python
pip install faiss-cpu
#æˆ–
#conda install faiss-cpu
```

ä¸¾ä¾‹ï¼š

```python
#  1.å¯¼å…¥ç›¸å…³åŒ…
from langchain_community.vectorstores import FAISS
from langchain_core.example_selectors import MaxMarginalRelevanceExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_openai import OpenAIEmbeddings

# 2.åˆ›å»ºä¸€ä¸ªç¤ºä¾‹ç»„
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

# 3.å®šä¹‰ç¤ºä¾‹æç¤ºè¯æ¨¡ç‰ˆ
example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

# 4.å®šä¹‰åµŒå…¥æ¨¡å‹
embeddings = OpenAIEmbeddings(
    model="text-embedding-ada-002"
)

# 5.åˆ›å»ºMMRç¤ºä¾‹é€‰æ‹©å™¨
example_selector = MaxMarginalRelevanceExampleSelector.from_examples(
    # ç¤ºä¾‹æç¤ºè¯.
    examples,
    # åµŒå…¥æ¨¡å‹.
    embeddings,
    # å‘é‡æ•°æ®åº“ï¼ˆå­˜å‚¨åµŒå…¥å¹¶å¯¹å…¶è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼‰.
    FAISS,
    # ç”Ÿæˆç¤ºä¾‹çš„æ•°é‡.
    k=2,
)

# 6.å®šä¹‰å°æ ·æœ¬æç¤ºè¯æ¨¡ç‰ˆ
mmr_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="ç»™å‡ºæ¯ä¸ªè¾“å…¥çš„åä¹‰è¯",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)

# è¾“å…¥çš„æ˜¯ä¸€ç§æ„Ÿè§‰ï¼Œæ‰€ä»¥åº”è¯¥é€‰æ‹©å¿«ä¹/æ‚²ä¼¤çš„ä¾‹å­ä½œä¸ºç¬¬ä¸€ä¸ª
print(mmr_prompt.format(adjective="Cheerful"))
```

> ç»™å‡ºæ¯ä¸ªè¾“å…¥çš„åä¹‰è¯
>
> Input: sunny
> Output: gloomy
>
> Input: happy
> Output: sad
>
> Input: Cheerful
> Output:



### 3.6 å…·ä½“ä½¿ç”¨ï¼šPipelinePromptTemplate(äº†è§£)

ç”¨äºå°†å¤šä¸ªæç¤ºæ¨¡æ¿**æŒ‰é¡ºåºç»„åˆæˆå¤„ç†ç®¡é“**ï¼Œå®ç°åˆ†é˜¶æ®µã€æ¨¡å—åŒ–çš„æç¤ºæ„å»ºã€‚å®ƒçš„æ ¸å¿ƒä½œç”¨ç±»ä¼¼äºè½¯ä»¶å¼€å‘ä¸­çš„`ç®¡é“æ¨¡å¼`ï¼ˆPipeline Patternï¼‰ï¼Œé€šè¿‡ä¸²è”å¤šä¸ªæç¤ºå¤„ç†æ­¥éª¤ï¼Œå®ç°å¤æ‚çš„æç¤ºç”Ÿæˆé€»è¾‘ã€‚

**ç‰¹ç‚¹**ï¼š

- å°†å¤æ‚æç¤ºæ‹†è§£ä¸ºå¤šä¸ªå¤„ç†é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µä½¿ç”¨ç‹¬ç«‹çš„æç¤ºæ¨¡æ¿
- å‰ä¸€ä¸ªæ¨¡æ¿çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªæ¨¡æ¿çš„è¾“å…¥å˜é‡
- ä½¿ç”¨åœºæ™¯ï¼šè§£å†³å•ä¸€è¶…å¤§æç¤ºæ¨¡æ¿éš¾ä»¥ç»´æŠ¤çš„é—®é¢˜

**è¯´æ˜ï¼š**PipelinePromptTemplateåœ¨langchain 0.3.22ç‰ˆæœ¬ä¸­è¢«æ ‡è®°ä¸ºè¿‡æ—¶ï¼Œåœ¨ langchain-core==1.0 ä¹‹å‰ä¸ä¼šåˆ é™¤å®ƒã€‚

https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.pipeline.PipelinePromptTemplate.html

**ä¸¾ä¾‹ï¼š**

```python
from langchain_core.prompts.pipeline import PipelinePromptTemplate
from langchain_core.prompts.prompt import PromptTemplate


# é˜¶æ®µ1ï¼šé—®é¢˜åˆ†æ
analysis_template = PromptTemplate.from_template("""
åˆ†æè¿™ä¸ªé—®é¢˜ï¼š{question}
å…³é”®è¦ç´ ï¼š
""")

# é˜¶æ®µ2ï¼šçŸ¥è¯†æ£€ç´¢
retrieval_template = PromptTemplate.from_template("""
åŸºäºä»¥ä¸‹è¦ç´ æœç´¢èµ„æ–™ï¼š
{analysis_result}
æœç´¢å…³é”®è¯ï¼š
""")

# é˜¶æ®µ3ï¼šç”Ÿæˆæœ€ç»ˆå›ç­”
answer_template = PromptTemplate.from_template("""
ç»¼åˆä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š
{retrieval_result}
æœ€ç»ˆç­”æ¡ˆï¼š
""")

# æ„å»ºç®¡é“
pipeline = PipelinePromptTemplate(
    final_prompt=answer_template,
    pipeline_prompts=[
        ("analysis_result", analysis_template),
        ("retrieval_result", retrieval_template)
    ]
)

print(pipeline.format(question="é‡å­è®¡ç®—çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"))
```

> ç»¼åˆä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š
>
> åŸºäºä»¥ä¸‹è¦ç´ æœç´¢èµ„æ–™ï¼š
>
> åˆ†æè¿™ä¸ªé—®é¢˜ï¼šé‡å­è®¡ç®—çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ
> å…³é”®è¦ç´ ï¼š
>
> æœç´¢å…³é”®è¯ï¼š
>
> æœ€ç»ˆç­”æ¡ˆï¼š

ä¸Šè¿°ä»£ç æ‰§è¡Œæ—¶ï¼Œæç¤ºPipelinePromptTemplateå·²è¿‡æ—¶ï¼Œä»£ç æ›´æ–°å¦‚ä¸‹ï¼š

```python
from langchain_core.prompts.prompt import PromptTemplate

# é˜¶æ®µ1ï¼šé—®é¢˜åˆ†æ
analysis_template = PromptTemplate.from_template("""
åˆ†æè¿™ä¸ªé—®é¢˜ï¼š{question}
å…³é”®è¦ç´ ï¼š
""")

# é˜¶æ®µ2ï¼šçŸ¥è¯†æ£€ç´¢
retrieval_template = PromptTemplate.from_template("""
åŸºäºä»¥ä¸‹è¦ç´ æœç´¢èµ„æ–™ï¼š
{analysis_result}
æœç´¢å…³é”®è¯ï¼š
""")

# é˜¶æ®µ3ï¼šç”Ÿæˆæœ€ç»ˆå›ç­”
answer_template = PromptTemplate.from_template("""
ç»¼åˆä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š
{retrieval_result}
æœ€ç»ˆç­”æ¡ˆï¼š
""")

# é€æ­¥æ‰§è¡Œç®¡é“æç¤º
pipeline_prompts = [
    ("analysis_result", analysis_template),
    ("retrieval_result", retrieval_template)
]


my_input = {"question": "é‡å­è®¡ç®—çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"}

# print(pipeline_prompts)

# [('analysis_result', PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\nåˆ†æè¿™ä¸ªé—®é¢˜ï¼š{question}\nå…³é”®è¦ç´ ï¼š\n')), ('retrieval_result', PromptTemplate(input_variables=['analysis_result'], input_types={}, partial_variables={}, template='\nåŸºäºä»¥ä¸‹è¦ç´ æœç´¢èµ„æ–™ï¼š\n{analysis_result}\næœç´¢å…³é”®è¯ï¼š\n'))]

for name, prompt in pipeline_prompts:
    # è°ƒç”¨å½“å‰æç¤ºæ¨¡æ¿å¹¶è·å–å­—ç¬¦ä¸²ç»“æœ
    result = prompt.invoke(my_input).to_string()
    # å°†ç»“æœæ·»åŠ åˆ°è¾“å…¥å­—å…¸ä¸­ä¾›ä¸‹ä¸€æ­¥ä½¿ç”¨
    my_input[name] = result

# ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
my_output = answer_template.invoke(my_input).to_string()
print(my_output)
```



### 3.7 å…·ä½“ä½¿ç”¨ï¼šè‡ªå®šä¹‰æç¤ºè¯æ¨¡ç‰ˆ(äº†è§£)

åœ¨åˆ›å»ºpromptæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŒ‰ç…§è‡ªå·±çš„éœ€æ±‚å»åˆ›å»ºè‡ªå®šä¹‰çš„æç¤ºæ¨¡ç‰ˆã€‚

**æ­¥éª¤**ï¼š

- è‡ªå®šä¹‰ç±»ç»§æ‰¿æç¤ºè¯åŸºç±»æ¨¡ç‰ˆBasePromptTemplate
- é‡å†™formatã€format_promptã€from_templateæ–¹æ³•

ä¸¾ä¾‹ï¼š

```python
# 1.å¯¼å…¥ç›¸å…³åŒ…
from typing import List, Dict, Any
from langchain.prompts import BasePromptTemplate
from langchain.prompts import PromptTemplate
from langchain.schema import PromptValue

#  2.è‡ªå®šä¹‰æç¤ºè¯æ¨¡ç‰ˆ
class SimpleCustomPrompt(BasePromptTemplate):
    """ç®€å•è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿"""
    template: str
    
    def __init__(self, template: str, **kwargs):
        # ä½¿ç”¨PromptTemplateè§£æè¾“å…¥å˜é‡
        prompt = PromptTemplate.from_template(template)
    
        super().__init__(
            input_variables=prompt.input_variables,
            template=template,
            **kwargs
        )
    
    def format(self, **kwargs: Any) -> str:
        """æ ¼å¼åŒ–æç¤ºè¯"""
        # print("kwargs:", kwargs)
        # print("self.template:", self.template)
    
        return self.template.format(**kwargs)
    
    def format_prompt(self, **kwargs: Any) -> PromptValue:
        """å®ç°æŠ½è±¡æ–¹æ³•"""
        return PromptValue(text=self.format(**kwargs))

    @classmethod
    def from_template(cls, template: str, **kwargs) -> "SimpleCustomPrompt":
        """ä»æ¨¡æ¿åˆ›å»ºå®ä¾‹"""
        return cls(template=template, **kwargs)

# 3.ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯æ¨¡ç‰ˆ
custom_prompt = SimpleCustomPrompt.from_template(
    template="è¯·å›ç­”å…³äº{subject}çš„é—®é¢˜ï¼š{question}"
)

#  4.æ ¼å¼åŒ–æç¤ºè¯
formatted = custom_prompt.format(
    subject="äººå·¥æ™ºèƒ½", 
    question="ä»€ä¹ˆæ˜¯LLMï¼Ÿ"
)

print(formatted)
```

> è¯·å›ç­”å…³äºäººå·¥æ™ºèƒ½çš„é—®é¢˜ï¼šä»€ä¹ˆæ˜¯LLMï¼Ÿ
>



### 3.8 ä»æ–‡æ¡£ä¸­åŠ è½½Prompt

ä¸€æ–¹é¢ï¼Œå°†æƒ³è¦è®¾å®špromptæ‰€æ”¯æŒçš„æ ¼å¼ä¿å­˜ä¸ºJSONæˆ–è€…YAMLæ ¼å¼æ–‡ä»¶ã€‚

å¦ä¸€æ–¹é¢ï¼Œé€šè¿‡è¯»å–æŒ‡å®šè·¯å¾„çš„æ ¼å¼åŒ–æ–‡ä»¶ï¼Œè·å–ç›¸åº”çš„promptã€‚

ç›®çš„ä¸ä½¿ç”¨åœºæ™¯ï¼š

- ä¸ºäº†ä¾¿äºå…±äº«ã€å­˜å‚¨å’ŒåŠ å¼ºå¯¹promptçš„ç‰ˆæœ¬æ§åˆ¶ã€‚
- å½“æˆ‘ä»¬çš„promptæ¨¡æ¿æ•°æ®è¾ƒå¤§æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¤–éƒ¨å¯¼å…¥çš„æ–¹å¼è¿›è¡Œç®¡ç†å’Œç»´æŠ¤ã€‚



#### 3.8.1 yamlæ ¼å¼æç¤ºè¯

assetä¸‹åˆ›å»ºyamlæ–‡ä»¶ï¼šprompt.yaml

```json
_type:
    "prompt"
input_variables:
    ["name","what"]
template:
    "è¯·ç»™{name}è®²ä¸€ä¸ªå…³äº{what}çš„æ•…äº‹"
```

ä»£ç ï¼š

```python
from langchain_core.prompts import load_prompt
from dotenv import load_dotenv

load_dotenv()

prompt = load_prompt("asset/prompt.yaml", encoding="utf-8")
# print(prompt)
print(prompt.format(name="å¹´è½»äºº", what="æ»‘ç¨½"))
```

> è¯·ç»™å¹´è½»äººè®²ä¸€ä¸ªå…³äºæ»‘ç¨½çš„ç¬‘è¯

####  3.8.2 jsonæ ¼å¼æç¤ºè¯

assetä¸‹åˆ›å»ºjsonæ–‡ä»¶ï¼šsimple_prompt.json

```python
{
  "_type": "prompt",
  "input_variables": ["name", "what"],
  "template": "è¯·{name}è®²ä¸€ä¸ª{what}çš„æ•…äº‹ã€‚"
}
```

ä»£ç ï¼š

```python
from langchain_core.prompts import load_prompt
from dotenv import load_dotenv

load_dotenv()

prompt = load_prompt("asset/simple_prompt.json",encoding="utf-8")
print(prompt.format(name="å¼ ä¸‰",what="æç¬‘çš„"))
```

> è¯·å¼ ä¸‰è®²ä¸€ä¸ªæç¬‘çš„çš„æ•…äº‹ã€‚
>



## 4ã€Model I/Oä¹‹Output Parsers

> åœ¨äººç±»è¯­è¨€äº¤äº’ä¸­ï¼Œä¸åŒçš„è¯­è¨€è¡¨è¾¾æ–¹å¼é€šå¸¸ä¸ä¼šé€ æˆç†è§£ä¸Šçš„éšœç¢ã€‚ä½†åœ¨åº”ç”¨å¼€å‘ä¸­ï¼Œå¤§æ¨¡å‹çš„è¾“å‡ºå¯èƒ½æ˜¯ä¸‹ä¸€æ­¥é€»è¾‘å¤„ç†çš„å…³é”®è¾“å…¥ã€‚å› æ­¤ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`è§„èŒƒåŒ–è¾“å‡º`æ˜¯å¿…é¡»è¦åšçš„ä»»åŠ¡ï¼Œä»¥ç¡®ä¿åº”ç”¨èƒ½å¤Ÿé¡ºåˆ©è¿›è¡Œåç»­çš„é€»è¾‘å¤„ç†ã€‚

è¯­è¨€æ¨¡å‹è¿”å›çš„å†…å®¹é€šå¸¸éƒ½æ˜¯å­—ç¬¦ä¸²çš„æ ¼å¼ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰ï¼Œä½†åœ¨å®é™…AIåº”ç”¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¾€å¾€å¸Œæœ›modelå¯ä»¥è¿”å›**æ›´ç›´è§‚ã€æ›´æ ¼å¼åŒ–çš„å†…å®¹**ï¼ŒLangChainæä¾›çš„è¾“å‡ºè§£æå™¨å°±æ´¾ä¸Šç”¨åœºäº†ã€‚

è¾“å‡ºè§£æå™¨ï¼ˆOutput Parserï¼‰è´Ÿè´£è·å– LLM çš„è¾“å‡ºå¹¶å°†å…¶è½¬æ¢ä¸ºæ›´åˆé€‚çš„æ ¼å¼ã€‚**è¿™åœ¨åº”ç”¨å¼€å‘ä¸­åŠå…¶é‡è¦**ã€‚

### 4.1 è¾“å‡ºè§£æå™¨çš„åˆ†ç±»

LangChainæœ‰è®¸å¤šä¸åŒç±»å‹çš„è¾“å‡ºè§£æå™¨

- `StrOutputParser`ï¼šå­—ç¬¦ä¸²è§£æå™¨
- `JsonOutputParser`ï¼šJSONè§£æå™¨ï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆç‰¹å®šJSONå¯¹è±¡æ ¼å¼
- `DatetimeOutputParser`ï¼šæ—¥æœŸæ—¶é—´è§£æå™¨ï¼Œå¯ç”¨äºå°† LLM è¾“å‡ºè§£æä¸ºæ—¥æœŸæ—¶é—´æ ¼å¼
- `CommaSeparatedListOutputParser`ï¼šCSVè§£æå™¨ï¼Œæ¨¡å‹çš„è¾“å‡ºä»¥é€—å·åˆ†éš”ï¼Œä»¥åˆ—è¡¨å½¢å¼è¿”å›è¾“å‡º
- `XMLOutputParser`ï¼šXMLè§£æå™¨ï¼Œå…è®¸ä»¥æµè¡Œçš„XMLæ ¼å¼ä»LLMè·å–ç»“æœ

é™¤äº†ä¸Šè¿°å¸¸ç”¨çš„è¾“å‡ºè§£æå™¨ä¹‹å¤–ï¼Œè¿˜æœ‰ï¼š

- `EnumOutputParser`ï¼šæšä¸¾è§£æå™¨ï¼Œå°†LLMçš„è¾“å‡ºï¼Œè§£æä¸ºé¢„å®šä¹‰çš„æšä¸¾å€¼
- `StructuredOutputParser`ï¼šå°†**éç»“æ„åŒ–æ–‡æœ¬**è½¬æ¢ä¸ºé¢„å®šä¹‰æ ¼å¼çš„**ç»“æ„åŒ–æ•°æ®**ï¼ˆå¦‚å­—å…¸ï¼‰
- `OutputFixingParser`ï¼šè¾“å‡ºä¿®å¤è§£æå™¨ï¼Œç”¨äºè‡ªåŠ¨ä¿®å¤æ ¼å¼é”™è¯¯çš„è§£æå™¨ï¼Œæ¯”å¦‚å°†è¿”å›çš„ä¸ç¬¦åˆé¢„æœŸæ ¼å¼çš„è¾“å‡ºï¼Œå°è¯•ä¿®æ­£ä¸ºæ­£ç¡®çš„ç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚ JSONï¼‰
- `PydanticOutputParser`ï¼šå°†è¾“å‡ºè‡ªåŠ¨è§£æä¸º Pydantic æ¨¡å‹å®ä¾‹çš„ç»„ä»¶
- `RetryOutputParser`ï¼šé‡è¯•è§£æå™¨ï¼Œå½“ä¸»è§£æå™¨ï¼ˆå¦‚ PydanticOutputParser æˆ– JSONOutputParserï¼‰å› æ ¼å¼é”™è¯¯æ— æ³•è§£æ LLM çš„è¾“å‡ºæ—¶ï¼Œé€šè¿‡è°ƒç”¨å¦ä¸€ä¸ª LLM è‡ªåŠ¨ä¿®æ­£é”™è¯¯ï¼Œå¹¶é‡æ–°å°è¯•è§£æ

### 4.2 å…·ä½“è§£æå™¨çš„ä½¿ç”¨

#### â‘  å­—ç¬¦ä¸²è§£æå™¨ StrOutputParser

StrOutputParser ç®€å•åœ°å°†`ä»»ä½•è¾“å…¥`è½¬æ¢ä¸º`å­—ç¬¦ä¸²`ã€‚å®ƒæ˜¯ä¸€ä¸ªç®€å•çš„è§£æå™¨ï¼Œä»ç»“æœä¸­æå–contentå­—æ®µ

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser

import os
import dotenv
from langchain_openai import ChatOpenAI

dotenv.load_dotenv()

os.environ['OPENAI_API_KEY'] = os.getenv("OPENAI_API_KEY1")
os.environ['OPENAI_BASE_URL'] = os.getenv("OPENAI_BASE_URL")

chat_model = ChatOpenAI(model="gpt-4o-mini")

messages = [
    SystemMessage(content="å°†ä»¥ä¸‹å†…å®¹ä»è‹±è¯­ç¿»è¯‘æˆä¸­æ–‡"),
    HumanMessage(content="It's a nice day today"),
]

result = chat_model.invoke(messages)
print(type(result))
print(result)

parser = StrOutputParser()
#ä½¿ç”¨parserå¤„ç†modelè¿”å›çš„ç»“æœ
response = parser.invoke(result)
print(type(response))
print(response)
```

> <class 'langchain_core.messages.ai.AIMessage'>
>
> content='ä»Šå¤©æ˜¯ä¸ªå¥½å¤©ã€‚' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 25, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-BpPd126GlvwFI3TpL2EMaInxruhk0', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--690e05f2-39ad-4ff7-98fd-ef3ad00e6133-0' usage_metadata={'input_tokens': 25, 'output_tokens': 7, 'total_tokens': 32, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}
>
> <class 'str'>
>
> ä»Šå¤©æ˜¯ä¸ªå¥½å¤©ã€‚



#### â‘¡ JSONè§£æå™¨ JsonOutputParser

JSON Output Parserï¼Œå³JSON è§£æå™¨ï¼Œæ˜¯ä¸€ç§ç”¨äºå°†å¤§æ¨¡å‹çš„`è‡ªç”±æ–‡æœ¬è¾“å‡º`è½¬æ¢ä¸º`ç»“æ„åŒ–JSONæ•°æ®`çš„å·¥å…·ã€‚

**é€‚åˆåœºæ™¯ï¼š**ç‰¹åˆ«é€‚ç”¨äºéœ€è¦ä¸¥æ ¼ç»“æ„åŒ–è¾“å‡ºçš„åœºæ™¯ï¼Œæ¯”å¦‚ API è°ƒç”¨ã€æ•°æ®å­˜å‚¨æˆ–ä¸‹æ¸¸ä»»åŠ¡å¤„ç†ã€‚


**ç‰¹ç‚¹ï¼š**
- **æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼**
- **æ•°æ®éªŒè¯**
  ç»“åˆ Pydantic æ¨¡å‹ï¼Œè‡ªåŠ¨éªŒè¯å­—æ®µç±»å‹å’Œå†…å®¹ï¼ˆå¦‚å­—ç¬¦ä¸²ã€æ•°å­—ã€åµŒå¥—å¯¹è±¡ç­‰ï¼‰
- **ä¸æç¤ºæ¨¡æ¿é›†æˆ**
- `get_format_instructions()`ï¼š ç”Ÿæˆæ ¼å¼è¯´æ˜ï¼ŒæŒ‡å¯¼å¦‚ä½•æ ¼å¼åŒ–è¯­è¨€æ¨¡å‹è¾“å‡ºçš„å­—ç¬¦ä¸²ï¼ŒJsonOutputParserè°ƒç”¨æ­¤æ–¹æ³•å¯ä»¥æŒ‡å¯¼æ¨¡å‹è¾“å‡º JSON ç»“æ„

ä¸¾ä¾‹1ï¼š


```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate

chat_model = ChatOpenAI(model="gpt-4o-mini")

chat_prompt_template = ChatPromptTemplate.from_messages([
    ("system","ä½ æ˜¯ä¸€ä¸ªé è°±çš„{role}"),
    ("human","{question}")
])

parser = JsonOutputParser()

# æ–¹å¼1ï¼š
result = chat_model.invoke(chat_prompt_template.format_messages(role="äººå·¥æ™ºèƒ½ä¸“å®¶",question="äººå·¥æ™ºèƒ½ç”¨è‹±æ–‡æ€ä¹ˆè¯´ï¼Ÿé—®é¢˜ç”¨qè¡¨ç¤ºï¼Œç­”æ¡ˆç”¨aè¡¨ç¤ºï¼Œè¿”å›ä¸€ä¸ªJSONæ ¼å¼"))
print(result)
print(type(result))

parser.invoke(result)

# æ–¹å¼2ï¼š
# chain = chat_prompt_template | chat_model | parser
# chain.invoke({"role":"äººå·¥æ™ºèƒ½ä¸“å®¶","question" : "äººå·¥æ™ºèƒ½ç”¨è‹±æ–‡æ€ä¹ˆè¯´ï¼Ÿé—®é¢˜ç”¨qè¡¨ç¤ºï¼Œç­”æ¡ˆç”¨aè¡¨ç¤ºï¼Œè¿”å›ä¸€ä¸ªJSONæ ¼å¼"})
```

> content='```json\n{\n  "q": "äººå·¥æ™ºèƒ½ç”¨è‹±æ–‡æ€ä¹ˆè¯´ï¼Ÿ",\n  "a": "Artificial Intelligence"\n}\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 40, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-ByBTRPL6LKkHJgsjbRwVawe1L92jJ', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--aadc94df-3608-4b1b-adf6-4a53ef8b640c-0' usage_metadata={'input_tokens': 40, 'output_tokens': 28, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}
>
> <class 'langchain_core.messages.ai.AIMessage'>
>
> {'q': 'äººå·¥æ™ºèƒ½ç”¨è‹±æ–‡æ€ä¹ˆè¯´ï¼Ÿ', 'a': 'Artificial Intelligence'}

ä¸¾ä¾‹2ï¼šè‡ªå·±æŒ‡å®šJSONæ ¼å¼

```python
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini")

# æ‰‹åŠ¨å®šä¹‰JSONæ ¼å¼æŒ‡ä»¤
format_instructions = """{
    "joke": "str  #å®Œæ•´çš„ç¬‘è¯å†…å®¹",
    "explanation": "str  #ç¬‘è¯çš„æ–‡åŒ–èƒŒæ™¯è§£é‡Š"
}"""

# æç¤ºæ¨¡æ¿
joke_query = "å‘Šè¯‰æˆ‘ä¸€ä¸ªç¬‘è¯ã€‚"

prompt_template = PromptTemplate(
    template="å›ç­”ç”¨æˆ·çš„æŸ¥è¯¢ã€‚\næ ¼å¼è¦æ±‚ï¼š{format_instructions}\nï¼Œé—®é¢˜æ˜¯ï¼š{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": format_instructions},
)

# è¾“å‡ºè§£æå™¨
parser = JsonOutputParser()

chain = prompt_template | chat_model | parser
resp = chain.invoke({"query": joke_query})
print(resp)
print(type(resp))
```

> {'joke': 'ä¸ºä»€ä¹ˆæ•°å­¦ä¹¦æ€»æ˜¯å¾ˆå¿§ä¼¤ï¼Ÿå› ä¸ºå®ƒæœ‰å¤ªå¤šçš„é—®é¢˜ï¼', 'explanation': 'è¿™ä¸ªç¬‘è¯åˆ©ç”¨äº†â€˜é—®é¢˜â€™ä¸€è¯çš„åŒå…³å«ä¹‰ã€‚åœ¨æ•°å­¦ä¸­ï¼Œé—®é¢˜é€šå¸¸æŒ‡ä»£éœ€è¦è§£å†³çš„æ•°å­¦é¢˜ï¼Œè€Œåœ¨ç”Ÿæ´»ä¸­ï¼Œé—®é¢˜ä¹Ÿå¯ä»¥æŒ‡çƒ¦æ¼æˆ–å›°æ‰°ã€‚é€šè¿‡å°†è¿™ä¸¤ç§æ„ä¹‰ç»“åˆï¼Œåˆ›é€ å‡ºä¸€ç§å¹½é»˜æ„Ÿï¼Œåæ˜ å‡ºæ•°å­¦ä¹¦çš„å¤æ‚æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚'}
> <class 'dict'>

ä¸¾ä¾‹3ï¼šä½¿ç”¨æŒ‡å®šçš„JSONæ ¼å¼

```python
# å¼•å…¥ä¾èµ–åŒ…
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate

# åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini")

joke_query = "å‘Šè¯‰æˆ‘ä¸€ä¸ªç¬‘è¯ã€‚"

# å®šä¹‰Jsonè§£æå™¨
parser = JsonOutputParser()

# å®šä¹‰æç¤ºè¯æ¨¡ç‰ˆ
# æ³¨æ„ï¼Œæç¤ºè¯æ¨¡æ¿ä¸­éœ€è¦éƒ¨åˆ†æ ¼å¼åŒ–è§£æå™¨çš„æ ¼å¼è¦æ±‚format_instructions
prompt = PromptTemplate(
    template="å›ç­”ç”¨æˆ·çš„æŸ¥è¯¢.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

# 5.ä½¿ç”¨LCELè¯­æ³•ç»„åˆä¸€ä¸ªç®€å•çš„é“¾
chain = prompt | chat_model | parser
# 6.æ‰§è¡Œé“¾
output = chain.invoke({"query": "ç»™æˆ‘è®²ä¸€ä¸ªç¬‘è¯"})
print(output)
```

> {'joke': 'ä¸ºä»€ä¹ˆæµ·æ´‹æ€»æ˜¯å’¸çš„ï¼Ÿå› ä¸ºå®ƒæœ‰å¤ªå¤šçš„"æµ·"æ¹¿çš„äº‹æƒ…å‘ç”Ÿï¼'}
>



#### â‘¢ æ—¥æœŸè§£æå™¨ DatetimeOutputParser

åˆ©ç”¨æ­¤è§£æå™¨å¯ä»¥ç›´æ¥å°†LLMè¾“å‡ºè§£æä¸ºæ—¥æœŸæ—¶é—´æ ¼å¼ã€‚

- **get_format_instructions()**ï¼š è·å–æ—¥æœŸè§£æçš„æ ¼å¼åŒ–æŒ‡ä»¤ï¼ŒæŒ‡ä»¤ä¸ºï¼š"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'ã€‚
  - ä¸¾ä¾‹ï¼š1206-08-16T17:39:06.176399Z

ä¸¾ä¾‹1ï¼š

```python
from langchain.output_parsers import DatetimeOutputParser

output_parser = DatetimeOutputParser()

format_instructions = output_parser.get_format_instructions()
print(format_instructions)
```

> Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.
>
> Examples: 1563-09-27T04:28:14.640366Z, 1786-06-24T23:46:01.984421Z, 1079-05-27T08:43:24.266403Z
>
> Return ONLY this string, no other words!

ä¸¾ä¾‹2ï¼š

```python
from langchain.output_parsers import DatetimeOutputParser

output_parser = DatetimeOutputParser()
# print(output_parser.get_format_instructions())

str_time = '1547-06-23T21:24:07.078384Z'
date_time = output_parser.parse(str_time)
print(type(str_time))
print(type(date_time))
print(date_time)
```

> <class 'str'>
> <class 'datetime.datetime'>
> 1547-06-23 21:24:07.078384

ä¸¾ä¾‹3ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain.prompts.chat import HumanMessagePromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain.output_parsers import DatetimeOutputParser

chat_model = ChatOpenAI(model="gpt-4o-mini")


chat_prompt = ChatPromptTemplate.from_messages([
    ("system","{format_instructions}"),
    ("human", "{request}")
])

output_parser = DatetimeOutputParser()

# æ–¹å¼1ï¼š
# model_request = chat_prompt.format_messages(
#     request="ä¸­åäººæ°‘å…±å’Œå›½æ˜¯ä»€ä¹ˆæ—¶å€™æˆç«‹çš„",
#     format_instructions=output_parser.get_format_instructions()
# )

# response = chat_model.invoke(model_request)
# result = output_parser.invoke(response)
# print(result)
# print(type(result))

# æ–¹å¼2ï¼š
chain = chat_prompt | chat_model | output_parser
resp = chain.invoke({"request":"ä¸­åäººæ°‘å…±å’Œå›½æ˜¯ä»€ä¹ˆæ—¶å€™æˆç«‹çš„",
                     "format_instructions":output_parser.get_format_instructions()})
print(resp)
print(type(resp))

```

> 1949-10-01 00:00:00
> <class 'datetime.datetime'>



#### â‘£ åˆ—è¡¨è§£æå™¨ CommaSeparatedListOutputParser 

åˆ—è¡¨è§£æå™¨ï¼šåˆ©ç”¨æ­¤è§£æå™¨å¯ä»¥å°†æ¨¡å‹çš„æ–‡æœ¬å“åº”è½¬æ¢ä¸ºä¸€ä¸ªç”¨é€—å·åˆ†éš”çš„åˆ—è¡¨ï¼ˆList[str]ï¼‰ã€‚

ä¸¾ä¾‹1ï¼š

```python
from langchain_core.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()

# è¿”å›ä¸€äº›æŒ‡ä»¤æˆ–æ¨¡æ¿ï¼Œè¿™äº›æŒ‡ä»¤å‘Šè¯‰ç³»ç»Ÿå¦‚ä½•è§£ææˆ–æ ¼å¼åŒ–è¾“å‡ºæ•°æ®
format_instructions = output_parser.get_format_instructions()
print(format_instructions)

messages = "å¤§è±¡,çŒ©çŒ©,ç‹®å­"
result = output_parser.parse(messages)
print(result)
print(type(result))
```

> Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`
> ['å¤§è±¡', 'çŒ©çŒ©', 'ç‹®å­']
> <class 'list'>

ä¸¾ä¾‹2ï¼š


```python
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.output_parsers import CommaSeparatedListOutputParser

# åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini")

# åˆ›å»ºè§£æå™¨
output_parser = CommaSeparatedListOutputParser()

# åˆ›å»ºLangChainæç¤ºæ¨¡æ¿
chat_prompt = PromptTemplate.from_template(
    "ç”Ÿæˆ5ä¸ªå…³äº{text}çš„åˆ—è¡¨.\n\n{format_instructions}",
    partial_variables={
    "format_instructions": output_parser.get_format_instructions()
    })

# æç¤ºæ¨¡æ¿ä¸è¾“å‡ºè§£æå™¨ä¼ é€’è¾“å‡º
# chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())

# å°†æç¤ºå’Œæ¨¡å‹åˆå¹¶ä»¥è¿›è¡Œè°ƒç”¨
chain = chat_prompt | chat_model | output_parser
res = chain.invoke({"text": "ç”µå½±"})
print(res)
print(type(res))
```

> ['ç»å…¸ç”µå½±', 'ç°ä»£ç”µå½±', 'åŠ¨ä½œç”µå½±', 'çˆ±æƒ…ç”µå½±', 'ç§‘å¹»ç”µå½±']
> <class 'list'>

ä¸¾ä¾‹3ï¼š

```python
from langchain.prompts.chat import HumanMessagePromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.output_parsers import CommaSeparatedListOutputParser

# åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini")

output_parser = CommaSeparatedListOutputParser()

chat_prompt = ChatPromptTemplate.from_messages([
    ("human", "{request}\n{format_instructions}")
    # HumanMessagePromptTemplate.from_template("{request}\n{format_instructions}"),
])

model_request = chat_prompt.format_messages(
    request="ç»™æˆ‘5ä¸ªå¿ƒæƒ…",
    format_instructions=output_parser.get_format_instructions()
)

#æ–¹å¼1ï¼š
# result = chat_model.invoke(model_request)
#
# resp = output_parser.parse(result.content)
# print(resp)
# print(type(resp))
#
# result = chat_model.invoke(model_request)

# æ–¹å¼2ï¼š
# result = chat_model.invoke(model_request)
# resp = output_parser.invoke(result)
# print(resp)
# print(type(resp))

# æ–¹å¼3ï¼š
chain = chat_prompt | chat_model | output_parser
resp = chain.invoke({"request": "ç»™æˆ‘5ä¸ªå¿ƒæƒ…", "format_instructions": output_parser.get_format_instructions()})
print(resp)
print(type(resp))
```

> ['å¿«ä¹', 'å¿§ä¼¤', 'æ„¤æ€’', 'å…´å¥‹', 'å®é™']
> <class 'list'>



#### â‘¤ XMLè§£æå™¨ XMLOutputParser





**ç‰¹ç‚¹**ï¼š

- **è§£æ XML**ï¼šå¼ºåˆ¶ LLM è¿”å› XML æ ¼å¼çš„å“åº”ï¼Œå¹¶æå–ç»“æ„åŒ–æ•°æ®ã€‚
- **ä¸ LLM ç»“åˆ**ï¼šåœ¨ `PromptTemplate` ä¸­æŒ‡å®š XML æ ¼å¼è¦æ±‚ï¼Œè®©æ¨¡å‹è¿”å› `<tag>content</tag>` å½¢å¼çš„æ•°æ®ã€‚
- **æ•°æ®æ ‡å‡†åŒ–**ï¼šå°†æ¨¡å‹çš„è‡ªç”±æ–‡æœ¬è¾“å‡ºè½¬æ¢ä¸ºå¯ç¼–ç¨‹å¤„ç†çš„ XML æ•°æ®ã€‚



ä¸¾ä¾‹1ï¼š

```python
from langchain_core.output_parsers import XMLOutputParser

output_parser = XMLOutputParser()
# è¿”å›ä¸€äº›æŒ‡ä»¤æˆ–æ¨¡æ¿ï¼Œè¿™äº›æŒ‡ä»¤å‘Šè¯‰ç³»ç»Ÿå¦‚ä½•è§£ææˆ–æ ¼å¼åŒ–è¾“å‡ºæ•°æ®
format_instructions = output_parser.get_format_instructions()
print(format_instructions)
```

> ```
> The output should be formatted as a XML file.
> 1. Output should conform to the tags below.
> 2. If tags are not given, make them on your own.
> 3. Remember to always open and close all the tags.
> 
> As an example, for the tags ["foo", "bar", "baz"]:
> 1. String "<foo>
>    <bar>
>       <baz></baz>
>    </bar>
> </foo>" is a well-formatted instance of the schema.
> 2. String "<foo>
>    <bar>
>    </foo>" is a badly-formatted instance.
> 3. String "<foo>
>    <tag>
>    </tag>
> </foo>" is a badly-formatted instance.
> 
> Here are the output tags:
> â€‹```
> None
> â€‹```
> ```

ä¸¾ä¾‹2ï¼šé€šè¿‡å¤§æ¨¡å‹çš„èƒ½åŠ›ï¼Œè¿”å›xmlæ ¼å¼æ•°æ®

```python
# åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini")

# æµ‹è¯•æ¨¡å‹çš„xmlè§£ææ•ˆæœ
actor_query = "ç”Ÿæˆæ±¤å§†Â·æ±‰å…‹æ–¯çš„ç®€çŸ­ç”µå½±è®°å½•"
output = chat_model.invoke(f"""{actor_query}è¯·å°†å½±ç‰‡é™„åœ¨<movie></movie>æ ‡ç­¾ä¸­"""
)
print(type(output))  # <class 'langchain_core.messages.ai.AIMessage'>
print(output.content)
```

> ```
> <class 'langchain_core.messages.ai.AIMessage'>
> ä»¥ä¸‹æ˜¯æ±¤å§†Â·æ±‰å…‹æ–¯çš„ä¸€äº›è‘—åç”µå½±è®°å½•ï¼Œä½¿ç”¨äº†`<movie></movie>`æ ‡ç­¾ï¼š
> 
> <movie>
>   <title>å¤§ç™½é²¨</title>
>   <year>1975</year>
>   <description>æ±¤å§†Â·æ±‰å…‹æ–¯å¹¶æœªå‚æ¼”è¯¥ç‰‡ï¼Œä½†å®ƒä»£è¡¨äº†ä»–æ‰€å´‡æ‹œçš„æ—©æœŸå†’é™©å’Œææ€–ç”µå½±ã€‚</description>
> </movie>
> 
> <movie>
>   <title>è´¹åŸæ•…äº‹</title>
>   <year>1993</year>
>   <description>æ±‰å…‹æ–¯åœ¨ç‰‡ä¸­é¥°æ¼”ä¸€åå› ç¢äºè‰¾æ»‹ç—…è€Œé­å—æ­§è§†çš„å¾‹å¸ˆï¼Œå±•ç°äº†å¼ºå¤§çš„è¡¨æ¼”èƒ½åŠ›ã€‚</description>
> </movie>
> 
> <movie>
>   <title>æ‹¯æ•‘å¤§å…µç‘æ©</title>
>   <year>1998</year>
>   <description>æ±¤å§†Â·æ±‰å…‹æ–¯åœ¨è¿™éƒ¨äºŒæˆ˜ç”µå½±ä¸­é¥°æ¼”ä¸€ä½å†›å®˜ï¼Œé¢†å¯¼å°é˜Ÿå¯»æ‰¾å¤±è¸ªå£«å…µï¼Œè¡¨ç°å‡ºå‹‡æ°”ä¸äººæ€§ã€‚</description>
> </movie>
> 
> <movie>
>   <title>é˜¿ç”˜æ­£ä¼ </title>
>   <year>1994</year>
>   <description>æ±‰å…‹æ–¯é¥°æ¼”æ™ºåŠ›æœ‰æ‰€ç¼ºé™·çš„é˜¿ç”˜ï¼Œå‡­å€Ÿå…¶çº¯çœŸå’Œæ¯…åŠ›èµ°è¿‡äº†åŠ¨è¡çš„å†å²ï¼Œæˆä¸ºç»å…¸è§’è‰²ã€‚</description>
> </movie>
> 
> <movie>
>   <title>äº‘å›¾</title>
>   <year>2012</year>
>   <description>è¿™éƒ¨ä½œå“å±•ç¤ºäº†ä¸åŒå†å²æ—¶æœŸçš„äººç‰©ï¼Œæ±‰å…‹æ–¯åœ¨å…¶ä¸­æ‰®æ¼”å¤šä¸ªè§’è‰²ï¼Œä½“ç°äº†äººä¸æ—¶é—´çš„å…³ç³»ã€‚</description>
> </movie>
> 
> <movie>
>   <title>å¤§åœ°æƒŠé›·</title>
>   <year>2000</year>
>   <description>æ±‰å…‹æ–¯åœ¨è¿™éƒ¨å½±ç‰‡ä¸­æ‹…ä»»åˆ¶ç‰‡äººå’Œä¸»æ¼”ï¼Œè®²è¿°äº†å…³äºå¸Œæœ›ä¸é‡ç”Ÿçš„æ„Ÿäººæ•…äº‹ã€‚</description>
> </movie>
> 
> <movie>
>   <title>è¥¿çº¿æ— æˆ˜äº‹</title>
>   <year>2022</year>
>   <description>è™½ç„¶æ±‰å…‹æ–¯å¹¶æ²¡æœ‰åœ¨ç‰‡ä¸­å‡ºæ¼”ï¼Œä½†ä½œä¸ºåˆ¶ç‰‡äººï¼Œä»–å¯¹åæˆ˜ä¸»é¢˜çš„ä¼ æ’­ä½œå‡ºè´¡çŒ®ã€‚</description>
> </movie>
> 
> è¿™äº›ç”µå½±å±•ç¤ºäº†æ±¤å§†Â·æ±‰å…‹æ–¯åœ¨ä¸åŒæ—¶æœŸæ‰€åšå‡ºçš„å¤šæ ·åŒ–å’Œæ·±åˆ»çš„è‰ºæœ¯è´¡çŒ®ã€‚
> ```

ä¸¾ä¾‹3ï¼š

XMLOutputParser ä¸ä¼šç›´æ¥å°†æ¨¡å‹çš„è¾“å‡ºä¿æŒä¸ºåŸå§‹XMLå­—ç¬¦ä¸²ï¼Œè€Œæ˜¯ä¼šè§£æXMLå¹¶è½¬æ¢æˆ`Pythonå­—å…¸`ï¼ˆæˆ–ç±»ä¼¼ç»“æ„åŒ–çš„æ•°æ®ï¼‰ã€‚ç›®çš„æ˜¯ä¸ºäº†æ–¹ä¾¿ç¨‹åºåç»­å¤„ç†æ•°æ®ï¼Œè€Œä¸æ˜¯å•çº¯ä¿ç•™XMLæ ¼å¼ã€‚

```python
# 1.å¯¼å…¥ç›¸å…³åŒ…
from langchain_core.output_parsers import XMLOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# 2. åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
chat_model = ChatOpenAI(model="gpt-4o-mini")

# 3.æµ‹è¯•æ¨¡å‹çš„xmlè§£ææ•ˆæœ
actor_query = "ç”Ÿæˆæ±¤å§†Â·æ±‰å…‹æ–¯çš„ç®€çŸ­ç”µå½±è®°å½•,ä½¿ç”¨ä¸­æ–‡å›å¤"

# 4.å®šä¹‰XMLOutputParserå¯¹è±¡
parser = XMLOutputParser()

# 5.å®šä¹‰æç¤ºè¯æ¨¡ç‰ˆå¯¹è±¡
# prompt = PromptTemplate(
#    template="{query}\n{format_instructions}",
#    input_variables=["query","format_instructions"],
#    partial_variables={"format_instructions": parser.get_format_instructions()},
#)

prompt_template = PromptTemplate.from_template("{query}\n{format_instructions}")

prompt_template1 = prompt_template.partial(format_instructions=parser.get_format_instructions())

response = chat_model.invoke(prompt_template1.format(query=actor_query))
print(response.content)
```

> ```xml
> <æ±¤å§†æ±‰å…‹æ–¯ç”µå½±è®°å½•>
>    <ç”µå½±>
>       <åç§°>é˜¿ç”˜æ­£ä¼ </åç§°>
>       <å¹´ä»½>1994</å¹´ä»½>
>       <è§’è‰²>ç¦é‡Œæ–¯ç‰¹Â·ç”˜</è§’è‰²>
>       <ç±»å‹>å‰§æƒ…/å–œå‰§</ç±»å‹>
>    </ç”µå½±>
>    <ç”µå½±>
>       <åç§°>æ‹¯æ•‘å¤§å…µç‘æ©</åç§°>
>       <å¹´ä»½>1998</å¹´ä»½>
>       <è§’è‰²>ç±³å‹’ä¸Šå°‰</è§’è‰²>
>       <ç±»å‹>æˆ˜äº‰/å‰§æƒ…</ç±»å‹>
>    </ç”µå½±>
>    <ç”µå½±>
>       <åç§°>æ²‰é»˜çš„ç¾”ç¾Š</åç§°>
>       <å¹´ä»½>2001</å¹´ä»½>
>       <è§’è‰²>æŸ¥å…‹Â·è¯ºå…°</è§’è‰²>
>       <ç±»å‹>å†’é™©/å‰§æƒ…</ç±»å‹>
>    </ç”µå½±>
>    <ç”µå½±>
>       <åç§°>è§’æ–—å£«</åç§°>
>       <å¹´ä»½>2000</å¹´ä»½>
>       <è§’è‰²>ç½—é©¬æŒ‡æŒ¥å®˜</è§’è‰²>
>       <ç±»å‹>å†å²/å‰§æƒ…</ç±»å‹>
>    </ç”µå½±>
>    <ç”µå½±>
>       <åç§°>å¤§å…µç‘æ©</åç§°>
>       <å¹´ä»½>1998</å¹´ä»½>
>       <è§’è‰²>ç±³å¥‡Â·å¸ƒæœ—</è§’è‰²>
>       <ç±»å‹>æˆ˜äº‰/åŠ¨ä½œ</ç±»å‹>
>    </ç”µå½±>
> </æ±¤å§†æ±‰å…‹æ–¯ç”µå½±è®°å½•>
> ```

ç»§ç»­ï¼š

```python
# æ–¹å¼1
response = chat_model.invoke(prompt_template1.format(query=actor_query))
result = parser.invoke(response)
print(result)
print(type(result))

# æ–¹å¼2
# chain = prompt_template1 | chat_model | parser
# result = chain.invoke({"query":actor_query})
# print(result)
# print(type(result))
```

> {'ç”µå½±è®°å½•': [{'æ¼”å‘˜': [{'åå­—': 'æ±¤å§†Â·æ±‰å…‹æ–¯'}, {'ä»£è¡¨ä½œå“': [{'ç”µå½±': [{'æ ‡é¢˜': 'é˜¿ç”˜æ­£ä¼ '}, {'å¹´ä»½': '1994'}, {'ç®€ä»‹': 'è®²è¿°äº†ä¸€ä¸ªæ™ºåŠ›ç®€å•å´æ‹¥æœ‰ä¼ å¥‡äººç”Ÿçš„ç”·å­é˜¿ç”˜çš„æ•…äº‹ã€‚'}]}, {'ç”µå½±': [{'æ ‡é¢˜': 'æ‹¯æ•‘å¤§å…µç‘æ©'}, {'å¹´ä»½': '1998'}, {'ç®€ä»‹': 'ä¸€æ”¯ç¾å›½å°é˜Ÿåœ¨è¯ºæ›¼åº•ç™»é™†åï¼Œæ·±å…¥æ•Œåï¼Œå¯»æ‰¾å¹¶æ‹¯æ•‘è¢«å›°çš„å£«å…µç‘æ©çš„æ•…äº‹ã€‚'}]}, {'ç”µå½±': [{'æ ‡é¢˜': 'ç»¿è‰²é‡Œå°å±‹'}, {'å¹´ä»½': '1999'}, {'ç®€ä»‹': 'æ”¹ç¼–è‡ªæ–¯è’‚èŠ¬Â·é‡‘çš„å°è¯´ï¼Œè®²è¿°äº†ä¸€ä½ç‹±è­¦ä¸ä¸€åæ­»åˆ‘çŠ¯ä¹‹é—´çš„å¥‡å¦™å…³ç³»ã€‚'}]}, {'ç”µå½±': [{'æ ‡é¢˜': 'ç©å…·æ€»åŠ¨å‘˜'}, {'å¹´ä»½': '1995'}, {'ç®€ä»‹': 'è®²è¿°äº†ç©å…·åœ¨ä¸»äººä¸åœ¨æ—¶çš„å†’é™©æ•…äº‹ï¼Œæ˜¯é¦–éƒ¨å…¨ç”µè„‘åŠ¨ç”»ç”µå½±ã€‚'}]}]}]}]}
> <class 'dict'>

ä¸¾ä¾‹3ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import XMLOutputParser


model = ChatOpenAI(model="gpt-4o-mini")

actor_query = "ç”Ÿæˆå‘¨æ˜Ÿé©°çš„ç®€åŒ–ç”µå½±ä½œå“åˆ—è¡¨ï¼ŒæŒ‰ç…§æœ€æ–°çš„æ—¶é—´é™åºï¼Œå¿…è¦æ—¶ä½¿ç”¨ä¸­æ–‡"
# è®¾ç½®è§£æå™¨ + å°†æŒ‡ä»¤æ³¨å…¥æç¤ºæ¨¡æ¿ã€‚
parser = XMLOutputParser()
prompt = PromptTemplate(
    template="å›ç­”ç”¨æˆ·çš„æŸ¥è¯¢ã€‚\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

output = model.invoke(prompt.invoke({"query": actor_query}))

# chain = prompt | model
# output = chain.invoke({"query": actor_query})
print(output.content)
```

> ```xml
> <å‘¨æ˜Ÿé©°ç”µå½±ä½œå“>
>    <ç”µå½±>
>       <æ ‡é¢˜>ç¾äººé±¼</æ ‡é¢˜>
>       <å¹´ä»½>2016</å¹´ä»½>
>    </ç”µå½±>
>    <ç”µå½±>
>       <æ ‡é¢˜>è¥¿æ¸¸é™é­”ç¯‡</æ ‡é¢˜>
>       <å¹´ä»½>2013</å¹´ä»½>
>    </ç”µå½±>
>    <ç”µå½±>
>       <æ ‡é¢˜>å¤§è¯è¥¿æ¸¸ä¹‹å¤§åœ£å¨¶äº²</æ ‡é¢˜>
>       <å¹´ä»½>1995</å¹´ä»½>
>    </ç”µå½±>
>    <ç”µå½±>
>       <æ ‡é¢˜>èµŒåœ£</æ ‡é¢˜>
>       <å¹´ä»½>1990</å¹´ä»½>
>    </ç”µå½±>
>    <ç”µå½±>
>       <æ ‡é¢˜>é€ƒå­¦å¨é¾™</æ ‡é¢˜>
>       <å¹´ä»½>1991</å¹´ä»½>
>    </ç”µå½±>
> </å‘¨æ˜Ÿé©°ç”µå½±ä½œå“>
> ```



## 5ã€LangChainè°ƒç”¨ç§æœ‰æ¨¡å‹

### 5.1 Ollamaçš„ä»‹ç»

Ollamaæ˜¯åœ¨Githubä¸Šçš„ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œå…¶é¡¹ç›®å®šä½æ˜¯ï¼š**ä¸€ä¸ªæœ¬åœ°è¿è¡Œå¤§æ¨¡å‹çš„é›†æˆæ¡†æ¶**ã€‚ç›®å‰ä¸»è¦é’ˆå¯¹ä¸»æµçš„LlaMAæ¶æ„çš„å¼€æºå¤§æ¨¡å‹è®¾è®¡ï¼Œå¯ä»¥å®ç°å¦‚ Qwenã€Deepseek ç­‰ä¸»æµå¤§æ¨¡å‹çš„ä¸‹è½½ã€å¯åŠ¨å’Œæœ¬åœ°è¿è¡Œçš„è‡ªåŠ¨åŒ–éƒ¨ç½²åŠæ¨ç†æµç¨‹ã€‚

ç›®å‰ä½œä¸ºä¸€ä¸ªéå¸¸çƒ­é—¨çš„å¤§æ¨¡å‹æ‰˜ç®¡å¹³å°ï¼Œå·²è¢«åŒ…æ‹¬LangChainã€Taskweaverç­‰åœ¨å†…çš„å¤šä¸ªçƒ­é—¨é¡¹ç›®é«˜åº¦é›†æˆã€‚

Ollamaå®˜æ–¹åœ°å€ï¼šhttps://ollama.com

Ollama Githubå¼€æºåœ°å€ï¼šhttps://github.com/ollama/ollama

### 5.2 Ollamaçš„ä¸‹è½½-å®‰è£…

Ollamaé¡¹ç›®æ”¯æŒè·¨å¹³å°éƒ¨ç½²ï¼Œç›®å‰å·²å…¼å®¹Macã€Linuxå’ŒWindowsæ“ä½œç³»ç»Ÿã€‚ç‰¹åˆ«åœ°å¯¹äºWindowsç”¨æˆ·æä¾›äº†éå¸¸ç›´è§‚çš„é¢„è§ˆç‰ˆã€‚

![image-20250617140911121](images/image-20250617140911121.png)

æ— è®ºä½¿ç”¨å“ªä¸ªæ“ä½œç³»ç»Ÿï¼ŒOllamaé¡¹ç›®çš„å®‰è£…è¿‡ç¨‹éƒ½è®¾è®¡å¾—éå¸¸ç®€å•ã€‚

è®¿é—® https://ollama.com/download ä¸‹è½½å¯¹åº”ç³»ç»Ÿçš„å®‰è£…æ–‡ä»¶ã€‚

- Windows ç³»ç»Ÿæ‰§è¡Œ `.exe` æ–‡ä»¶å®‰è£…

- Linux ç³»ç»Ÿæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š

  ```
  curl -fsSL https://ollama.com/install.sh | sh
  ```

  - è¿™è¡Œå‘½ä»¤çš„ç›®çš„æ˜¯ä»`https://ollama.com/` ç½‘ç«™è¯»å– `install.sh`è„šæœ¬ï¼Œå¹¶ç«‹å³é€šè¿‡ `sh` æ‰§è¡Œè¯¥è„šæœ¬ï¼Œåœ¨å®‰è£…è¿‡ç¨‹ä¸­ä¼šåŒ…å«ä»¥ä¸‹å‡ ä¸ªä¸»è¦çš„æ“ä½œï¼š
    - æ£€æŸ¥å½“å‰æœåŠ¡å™¨çš„åŸºç¡€ç¯å¢ƒï¼Œå¦‚ç³»ç»Ÿç‰ˆæœ¬ç­‰ï¼›
    - ä¸‹è½½Ollamaçš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼›
    - é…ç½®ç³»ç»ŸæœåŠ¡ï¼ŒåŒ…æ‹¬åˆ›å»ºç”¨æˆ·å’Œç”¨æˆ·ç»„ï¼Œæ·»åŠ Ollamaçš„é…ç½®ä¿¡æ¯ï¼›
    - å¯åŠ¨OllamaæœåŠ¡ï¼›

### 5.3 æ¨¡å‹çš„ä¸‹è½½-å®‰è£…

è®¿é—® https://ollama.com/search å¯ä»¥æŸ¥çœ‹ Ollama æ”¯æŒçš„æ¨¡å‹ã€‚ä½¿ç”¨å‘½ä»¤è¡Œå¯ä»¥ä¸‹è½½å¹¶è¿è¡Œæ¨¡å‹ï¼Œä¾‹å¦‚è¿è¡Œ `deepseek-r1:7b` æ¨¡å‹ï¼š

```bash
ollama run deepseek-r1:7b
```



### 5.4 è°ƒç”¨ç§æœ‰æ¨¡å‹

ä¸¾ä¾‹1ï¼š

``` python
from langchain_community.chat_models import ChatOllama

ollama_llm = ChatOllama(model="deepseek-r1:7b")
```

``` python
from langchain_core.messages import HumanMessage

messages = [
    HumanMessage(content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
]

chat_model_response = ollama_llm.invoke(messages)

print(chat_model_response.content)
```

> <think>
> æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰å…¬å¸å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹DeepSeek-R1ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚
> </think>
>
> æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰å…¬å¸å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹DeepSeek-R1ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚

è‹¥ Ollama ä¸åœ¨æœ¬åœ°é»˜è®¤ç«¯å£è¿è¡Œï¼Œéœ€æŒ‡å®š `base_url`ï¼Œå³ï¼š

```python
ollama_llm = ChatOllama(
    model="deepseek-r1:7b",
    base_url="http://your-ip:port"  # è‡ªå®šä¹‰åœ°å€
)
```

``` python
print(chat_model_response.content)
```

> æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰å…¬å¸å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹DeepSeek-R1ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚

ä¸¾ä¾‹2ï¼š

``` python
from langchain.prompts.chat import ChatPromptTemplate
from langchain_community.chat_models import ChatOllama

# æ„å»ºæ¨¡ç‰ˆ
template = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥å°†{input_language}ç¿»è¯‘æˆ{output_language}ã€‚"
human_template = "{text}"

# ç”Ÿæˆå¯¹è¯å½¢å¼çš„èŠå¤©ä¿¡æ¯æ ¼å¼
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])

# æ ¼å¼åŒ–å˜é‡è¾“å…¥
messages = chat_prompt.format_messages(input_language="ä¸­æ–‡", output_language="è‹±è¯­", text="æˆ‘çˆ±ç¼–ç¨‹")

# å®ä¾‹åŒ–Ollamaå¯åŠ¨çš„æ¨¡å‹
ollama_llm = ChatOllama(model="deepseek-r1:7b")

# æ‰§è¡Œæ¨ç†
result = ollama_llm.invoke(messages)

print(result.content)
```

> <think>
> å¥½ï¼Œç”¨æˆ·è®©æˆ‘æŠŠâ€œæˆ‘çˆ±ç¼–ç¨‹â€ç¿»è¯‘æˆè‹±æ–‡ã€‚é¦–å…ˆï¼Œâ€œæˆ‘â€ç¿»è¯‘æˆâ€œIâ€ï¼Œæ²¡é—®é¢˜ã€‚â€œçˆ±â€æ˜¯â€œloveâ€ï¼Œå¸¸ç”¨åœ¨è¡¨è¾¾æƒ…æ„Ÿä¸Šã€‚â€œç¼–ç¨‹â€æ¯”è¾ƒåˆé€‚çš„æ˜¯â€œprogrammingâ€ï¼Œè¿™ä¸ªè¯å¾ˆå¸¸è§ï¼Œç”¨æ¥æŒ‡ä»£è®¡ç®—æœºç¼–ç¨‹ã€‚
>
> æ‰€ä»¥ç»„åˆèµ·æ¥å°±æ˜¯â€œI love programmingâ€ã€‚å¬èµ·æ¥æŒºè‡ªç„¶çš„ï¼Œç¬¦åˆè‹±è¯­çš„ä¹ æƒ¯ç”¨æ³•ã€‚ç”¨æˆ·å¯èƒ½æ˜¯åœ¨å­¦ä¹ ç¼–ç¨‹æˆ–è€…åˆ†äº«è‡ªå·±çš„å…´è¶£ï¼Œæ‰€ä»¥ç¿»è¯‘å¾—ç®€æ´æ˜äº†å°±å¯ä»¥äº†ã€‚
> </think>
>
> I love programming.











[https://lmarena.ai/leaderboard]: 